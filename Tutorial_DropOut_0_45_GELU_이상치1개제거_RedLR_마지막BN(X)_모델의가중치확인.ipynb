{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial_DropOut_0.45_GELU_이상치1개제거_RedLR_마지막BN(X)_모델의가중치확인.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoonJaeHoon/Dacon_Behavioral_Data_Analysis/blob/master/Tutorial_DropOut_0_45_GELU_%EC%9D%B4%EC%83%81%EC%B9%981%EA%B0%9C%EC%A0%9C%EA%B1%B0_RedLR_%EB%A7%88%EC%A7%80%EB%A7%89BN(X)_%EB%AA%A8%EB%8D%B8%EC%9D%98%EA%B0%80%EC%A4%91%EC%B9%98%ED%99%95%EC%9D%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkIsyTZNAJ2P",
        "colab_type": "code",
        "outputId": "fa21ee2c-c271-4c3f-f08c-1bf0c48c4f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoP1zL0pARNV",
        "colab_type": "code",
        "outputId": "2ddabc6b-3878-4da7-a79c-cc5c0df1b51a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report #for model evaluation\n",
        "from sklearn.metrics import confusion_matrix #for model evaluation\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit #for data splitting\n",
        "np.random.seed(123) #ensure reproducibility\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('seaborn') # matplotlib 도 종류가 다양하기 때문에 seaborn 스타일로 지정한 거임.\n",
        "sns.set(font_scale=1) # (기본으로) 폰트 크기 2.5로 지정 미리 해놓는거임, 2.5면 꽤 크게 나옴\n",
        "\n",
        "#ignore warnings\n",
        "import warnings\n",
        "#warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd \n",
        "\n",
        "from numpy.random import seed\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "\n",
        "#train = pd.read_csv('drive/My Drive/데이콘_천체유형/train.csv', index_col=0)\n",
        "train = pd.read_csv('drive/My Drive/데이콘_천체유형/이상치_최대한_적게_train.csv', index_col=None)\n",
        "test = pd.read_csv('drive/My Drive/데이콘_천체유형/test.csv', index_col=0)\n",
        "sample_submission = pd.read_csv('drive/My Drive/데이콘_천체유형/sample_submission.csv', index_col=0)\n",
        "\n",
        "#######################################################################################################\n",
        "## petroMag_g 이상치 하나 더 없애기\n",
        "name = 'petroMag_g'\n",
        "drop_index = np.argmax(np.array(train[name]))\n",
        "train = train.drop(index=drop_index,axis=0)\n",
        "######################################################################################################\n",
        "\n",
        "print('csv 파일 (train, test, sample)을 불러왔습니다')\n",
        "print('train shape : {0}'.format(train.shape))\n",
        "print('test shape : {0}'.format(test.shape))\n",
        "print('='*50)\n",
        "\n",
        "train_type_num = train['type_num']\n",
        "needscaling_train = train.drop(['type','fiberID','type_num'],axis=1)\n",
        "needscaling_test = test.drop(['fiberID'], axis=1)\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(needscaling_train)\n",
        "scaled_train = pd.DataFrame(sc.transform(needscaling_train),\n",
        "                      columns=needscaling_train.columns,\n",
        "                      index = needscaling_train.index)\n",
        "\n",
        "scaled_test = pd.DataFrame(sc.transform(needscaling_test),\n",
        "                      columns=needscaling_test.columns,\n",
        "                      index = needscaling_test.index)\n",
        "\n",
        "scaled_train['type_num'] = train_type_num\n",
        "train_x = scaled_train.drop(['type_num'],axis=1)\n",
        "train_y = scaled_train['type_num']\n",
        "test_x = scaled_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "csv 파일 (train, test, sample)을 불러왔습니다\n",
            "train shape : (199898, 23)\n",
            "test shape : (10009, 21)\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4h9A4H3E_XB",
        "colab_type": "text"
      },
      "source": [
        "## petroMag_g 의 maximum 값 하나는 없애도 됨. ( 합리적으로)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKEZBrFkCwzC",
        "colab_type": "code",
        "outputId": "849981fa-bc18-4741-89ce-5d856547cd7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# setting outlier symbol, title, xlabel\n",
        "name = 'petroMag_g'\n",
        "plt.boxplot(train[name], sym=\"bo\")\n",
        "plt.title('Box plot of {0}'.format(name))\n",
        "plt.xticks([1], [name])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# setting outlier symbol, title, xlabel\n",
        "name = 'petroMag_g'\n",
        "plt.boxplot(test[name], sym=\"bo\")\n",
        "plt.title('Box plot of {0}'.format(name))\n",
        "plt.xticks([1], [name])\n",
        "plt.show()\n",
        "\n",
        "Q1 = train.describe().loc['25%',name]\n",
        "Q3 = train.describe().loc['75%',name]\n",
        "minimum = train.describe().loc['min',name]\n",
        "maximum = train.describe().loc['max',name]\n",
        "IQR = Q3-Q1\n",
        "\n",
        "print(maximum - Q3 + (1.5*IQR))\n",
        "print(Q1 - (1.5*IQR) -  minimum)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEPCAYAAABY9lNGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSapkQEiSKv1/86kb8I35pz8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEPCAYAAAC6Kkg/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nb+/ar53IAgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "253.7626148220352\n",
            "475.60843595216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in4ouFUuFFLy",
        "colab_type": "text"
      },
      "source": [
        "## 딥러닝 모델 구축 시작 (Elu)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e37totm8FI28",
        "colab_type": "code",
        "outputId": "13d2e057-547f-4252-e223-bd1609d03a1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#random seeds for stochastic parts of neural network \n",
        "np.random.seed(123)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(123)\n",
        "\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import  Activation,  Lambda, Flatten, LeakyReLU, ELU, Dense\n",
        "from keras.layers import Input, Concatenate, Reshape, Dropout, BatchNormalization\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.optimizers import Adam, SGD,RMSprop\n",
        "from keras import  backend as K\n",
        "from keras import metrics\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
        "#from swa.keras import SWA # swa optimizer - https://pypi.org/project/keras-swa/\n",
        "import tensorflow as tf\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "\n",
        "################ Gelu #############\n",
        "\n",
        "class Gelu(Activation):\n",
        "    def __init__(self, activation, **kwargs):\n",
        "        super(Gelu, self).__init__(activation, **kwargs)\n",
        "        self.__name__='gelu'\n",
        "        \n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
        "\n",
        "get_custom_objects().update({'gelu': Activation(gelu)})\n",
        "\n",
        "################# tanh 활용 ( 범위 늘림 ) ##############\n",
        "def custom_activation(x):\n",
        "  return (K.tanh(x) * 145) + 155\n",
        "\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "get_custom_objects().update({'custom_activation': Activation(custom_activation)})\n",
        "\n",
        "def create_nn_model():\n",
        "    inp = Input(shape=(20,))\n",
        "    x = Dense(2373)(inp)\n",
        "    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.45)(x)\n",
        "    \n",
        "    x = Dense(2355)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.45)(x)\n",
        "\n",
        "    x = Dense(1197)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.4)(x)\n",
        "\n",
        "    x = Dense(1187)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.35)(x)\n",
        "\n",
        "    x = Dense(612)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.35)(x)\n",
        "\n",
        "    x = Dense(602)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.3)(x)\n",
        "\n",
        "\n",
        "    out = Dense(19, activation='softmax')(x) #scalar_coupling_constant    \n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    return model\n",
        "\n",
        "nn_model = create_nn_model(  )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IVJlTVTKYT-",
        "colab_type": "code",
        "outputId": "674ff673-cd77-4255-c0f7-27989ffd4bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "csv_folder = '0218_이상치한개더제거_GELU_RedLR_DO_0.45'\n",
        "rate = ''\n",
        "\n",
        "import os\n",
        "SAVEMODEL_NEWFOLDER0 = 'drive/My Drive/데이콘_천체유형/ModelCheck/' + csv_folder\n",
        "SAVEMODEL_NEWFOLDER1 = 'drive/My Drive/데이콘_천체유형/기존모델저장/' + csv_folder\n",
        "if not os.path.exists(SAVEMODEL_NEWFOLDER1):\n",
        "  os.mkdir(SAVEMODEL_NEWFOLDER1)\n",
        "  print('모델폴더를 새로 생성했습니다.')\n",
        "SUBMISSION_NEWFOLDER = 'drive/My Drive/데이콘_천체유형/파일제출/' + csv_folder\n",
        "if not os.path.exists(SUBMISSION_NEWFOLDER):\n",
        "  os.mkdir(SUBMISSION_NEWFOLDER)\n",
        "  print('제출폴더를 새로 생성했습니다.')\n",
        "\n",
        "################## StratifiedShuffleSplit 를 이용해서 층화분할 #############\n",
        "X_array = train_x.values\n",
        "y_array = train_y.values\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=2, test_size=1/13, random_state=123)\n",
        "index1, index2 = sss.split(X_array, y_array)\n",
        "train_index = index1[0].tolist()\n",
        "val_index = index1[1].tolist()\n",
        "train_input = X_array[train_index]\n",
        "cv_input = X_array[val_index]\n",
        "train_target  = y_array[train_index]\n",
        "cv_target = y_array[val_index]\n",
        "\n",
        "\n",
        "\n",
        "#train_index, val_index = train_test_split(np.arange(len(train_y)),random_state=42, test_size=1/13)\n",
        "#train_input=train_x.iloc[train_index].values\n",
        "#train_target0=train_y[train_index]\n",
        "#train_target=train_target0.values\n",
        "#cv_input=train_x.iloc[val_index].values\n",
        "#cv_target0=train_y[val_index]\n",
        "#cv_target = cv_target0.values\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "모델폴더를 새로 생성했습니다.\n",
            "제출폴더를 새로 생성했습니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFJe8RqARKMq",
        "colab_type": "text"
      },
      "source": [
        "# Reduce LR on Plateau\n",
        "\n",
        "\n",
        "\n",
        "> patience를 지정해놓고 metric(일반적으로 val_loss)를 보면서 줄어들지 않을 때에 (손실감소곡선이 너무 평탄할때) 일정상수를 initial_learning_rate에 곱해 local minima를 탈출하는 방법\n",
        "\n",
        "# 스케쥴러\n",
        "\n",
        "\n",
        "\n",
        "> 스케쥴러도 위의 Reduce LR과 굉장히 유사함. 하지만 스케쥴러는 위와 달리  (Linear, Cosine) 등 여러가지 종류의 스케쥴러로 **patience를 지정해서 쓰는게 아니라** 일정 패턴을 따라 큰 값에서 작은 값을 왔다갔다하며 그 사이에서 학습되도록 함.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oy650Oa7Fv9G"
      },
      "source": [
        "## initial_rate = 1e-4 (과적합 심하고 Gradient Exploding 문제도 있음)\n",
        "\n",
        "### 1. lr 줄여서 local minima 문제 없애고\n",
        "### 2. Drop Out 비율 더 높게 해주고\n",
        "### 3. Gradient Exploding 문제 해결을 하고 싶다면... clip value 지정해줘야 할 듯 (얘는 아직 적용 안됨)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "abfcd94b-9996-4d8f-ae44-b91f1051a92f",
        "id": "tDPK5E_dFv9R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epoch =  800\n",
        "pat =  80\n",
        "red_pat =  25\n",
        "batchsize = 256*4\n",
        "initial_rate = 1e-4\n",
        "factor = 1/np.sqrt(10) # red_patience 만큼 기다리다가, 학습률을 *factor 배로 줄여버림 \n",
        "minimumlr = 1e-7\n",
        "\n",
        "import os\n",
        "MODEL_SAVE_FOLDER_PATH0 = SAVEMODEL_NEWFOLDER0 +  '/initial_rate=%s/' % initial_rate ## checkpoint\n",
        "MODEL_SAVE_FOLDER_PATH1 = SAVEMODEL_NEWFOLDER1 +  '/initial_rate=%s/' % initial_rate ## 기존모델저장\n",
        "if not os.path.exists(MODEL_SAVE_FOLDER_PATH1):\n",
        "  os.mkdir(MODEL_SAVE_FOLDER_PATH1)\n",
        "check_path = MODEL_SAVE_FOLDER_PATH0 + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
        "json_path = MODEL_SAVE_FOLDER_PATH1 + \"model1.json\"\n",
        "weight_path = MODEL_SAVE_FOLDER_PATH1 +\"model1.h5\"\n",
        "\n",
        "### model early stopping : 더이상 성능 개선이 되지 않으면 멈춤\n",
        "es = EarlyStopping(monitor= 'val_loss', patience = pat, verbose = 1, mode='min',\n",
        "                    restore_best_weights = True\n",
        "                   ) \n",
        "\n",
        "### model check point\n",
        "mc = ModelCheckpoint(filepath=check_path, monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "## ReduceLR on Plateau : val_loss가 안 줄어들 때 lr을 작게 할 수 있음 (local minima 대처방법)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor = factor,   # patience 만큼 기다리다가 0.1이면 학습률을 0.1배로 줄여버림 \n",
        "                        patience = red_pat, mode = 'min', verbose = 1,\n",
        "                        min_lr = minimumlr\n",
        "                        )\n",
        "\n",
        "from keras import optimizers\n",
        "optimizer = optimizers.Adam(\n",
        "    lr=initial_rate,\n",
        ")\n",
        "\n",
        "## compile model\n",
        "CCE = metrics.sparse_categorical_crossentropy\n",
        "\n",
        "nn_model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              #metrics=['accuracy']\n",
        "              #metrics=[metrics.sparse_categorical_accuracy]\n",
        "              metrics=[CCE]\n",
        "              )\n",
        "\n",
        "## fitting model\n",
        "hist = nn_model.fit(  train_input, train_target,validation_data=[cv_input, cv_target],\n",
        "                    batch_size=batchsize,\n",
        "                    epochs=epoch,\n",
        "                    callbacks = [es \n",
        "                                 #,mc\n",
        "                                 ,rlr\n",
        "                                ] )\n",
        "\n",
        "## save model\n",
        "model_json = nn_model.to_json()\n",
        "with open(json_path, \"w\") as json_file : \n",
        "    json_file.write(model_json)\n",
        "## model weight save\n",
        "nn_model.save_weights(weight_path)\n",
        "print(\"모델저장완료\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"Loss와 ACC에 대한 Plot을 그립니다\")\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "loss_ax.legend(loc='lower left')\n",
        "\n",
        "#acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
        "#acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        "#acc_ax.set_ylabel('accuracy')\n",
        "#acc_ax.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## evaluate model\n",
        "print('기존nn모델의 train loss를 출력합니다')\n",
        "loss_and_metric = nn_model.evaluate(train_input, train_target, batch_size=batchsize, verbose=0)\n",
        "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
        "## evaluate model\n",
        "print('기존nn모델의 valid loss를 출력합니다')\n",
        "loss_and_metric = nn_model.evaluate(cv_input, cv_target, batch_size=batchsize, verbose=0)\n",
        "print(\"valid, loss and metric: {}\".format(loss_and_metric))\n",
        "## model weight save ## 기존 모델의 가중치 저장\n",
        "#nn_model.save_weights(weight_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 184521 samples, validate on 15377 samples\n",
            "Epoch 1/800\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "184521/184521 [==============================] - 19s 103us/step - loss: 1.0129 - sparse_categorical_crossentropy: 1.0129 - val_loss: 0.6649 - val_sparse_categorical_crossentropy: 0.6649\n",
            "Epoch 2/800\n",
            "184521/184521 [==============================] - 8s 41us/step - loss: 0.6256 - sparse_categorical_crossentropy: 0.6256 - val_loss: 0.5605 - val_sparse_categorical_crossentropy: 0.5605\n",
            "Epoch 3/800\n",
            "184521/184521 [==============================] - 8s 42us/step - loss: 0.5680 - sparse_categorical_crossentropy: 0.5680 - val_loss: 0.5271 - val_sparse_categorical_crossentropy: 0.5271\n",
            "Epoch 4/800\n",
            "184521/184521 [==============================] - 8s 42us/step - loss: 0.5389 - sparse_categorical_crossentropy: 0.5389 - val_loss: 0.4997 - val_sparse_categorical_crossentropy: 0.4997\n",
            "Epoch 5/800\n",
            "184521/184521 [==============================] - 8s 42us/step - loss: 0.5218 - sparse_categorical_crossentropy: 0.5218 - val_loss: 0.5024 - val_sparse_categorical_crossentropy: 0.5024\n",
            "Epoch 6/800\n",
            "184521/184521 [==============================] - 8s 43us/step - loss: 0.5058 - sparse_categorical_crossentropy: 0.5058 - val_loss: 0.4792 - val_sparse_categorical_crossentropy: 0.4792\n",
            "Epoch 7/800\n",
            "184521/184521 [==============================] - 8s 43us/step - loss: 0.4954 - sparse_categorical_crossentropy: 0.4954 - val_loss: 0.4593 - val_sparse_categorical_crossentropy: 0.4593\n",
            "Epoch 8/800\n",
            "184521/184521 [==============================] - 8s 43us/step - loss: 0.4877 - sparse_categorical_crossentropy: 0.4877 - val_loss: 0.4600 - val_sparse_categorical_crossentropy: 0.4600\n",
            "Epoch 9/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4817 - sparse_categorical_crossentropy: 0.4817 - val_loss: 0.4470 - val_sparse_categorical_crossentropy: 0.4470\n",
            "Epoch 10/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4734 - sparse_categorical_crossentropy: 0.4734 - val_loss: 0.4412 - val_sparse_categorical_crossentropy: 0.4412\n",
            "Epoch 11/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4670 - sparse_categorical_crossentropy: 0.4670 - val_loss: 0.4351 - val_sparse_categorical_crossentropy: 0.4351\n",
            "Epoch 12/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4632 - sparse_categorical_crossentropy: 0.4632 - val_loss: 0.4423 - val_sparse_categorical_crossentropy: 0.4423\n",
            "Epoch 13/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4573 - sparse_categorical_crossentropy: 0.4573 - val_loss: 0.4289 - val_sparse_categorical_crossentropy: 0.4289\n",
            "Epoch 14/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4550 - sparse_categorical_crossentropy: 0.4550 - val_loss: 0.4257 - val_sparse_categorical_crossentropy: 0.4257\n",
            "Epoch 15/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4523 - sparse_categorical_crossentropy: 0.4523 - val_loss: 0.4279 - val_sparse_categorical_crossentropy: 0.4279\n",
            "Epoch 16/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4485 - sparse_categorical_crossentropy: 0.4485 - val_loss: 0.4276 - val_sparse_categorical_crossentropy: 0.4276\n",
            "Epoch 17/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4458 - sparse_categorical_crossentropy: 0.4458 - val_loss: 0.4163 - val_sparse_categorical_crossentropy: 0.4163\n",
            "Epoch 18/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4413 - sparse_categorical_crossentropy: 0.4413 - val_loss: 0.4200 - val_sparse_categorical_crossentropy: 0.4200\n",
            "Epoch 19/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4387 - sparse_categorical_crossentropy: 0.4387 - val_loss: 0.4143 - val_sparse_categorical_crossentropy: 0.4143\n",
            "Epoch 20/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4354 - sparse_categorical_crossentropy: 0.4354 - val_loss: 0.4086 - val_sparse_categorical_crossentropy: 0.4086\n",
            "Epoch 21/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4355 - sparse_categorical_crossentropy: 0.4355 - val_loss: 0.4102 - val_sparse_categorical_crossentropy: 0.4102\n",
            "Epoch 22/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4331 - sparse_categorical_crossentropy: 0.4331 - val_loss: 0.4120 - val_sparse_categorical_crossentropy: 0.4120\n",
            "Epoch 23/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4315 - sparse_categorical_crossentropy: 0.4315 - val_loss: 0.4014 - val_sparse_categorical_crossentropy: 0.4014\n",
            "Epoch 24/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4290 - sparse_categorical_crossentropy: 0.4290 - val_loss: 0.4015 - val_sparse_categorical_crossentropy: 0.4015\n",
            "Epoch 25/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4271 - sparse_categorical_crossentropy: 0.4271 - val_loss: 0.4003 - val_sparse_categorical_crossentropy: 0.4003\n",
            "Epoch 26/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4256 - sparse_categorical_crossentropy: 0.4256 - val_loss: 0.3991 - val_sparse_categorical_crossentropy: 0.3991\n",
            "Epoch 27/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4230 - sparse_categorical_crossentropy: 0.4230 - val_loss: 0.4010 - val_sparse_categorical_crossentropy: 0.4010\n",
            "Epoch 28/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4240 - sparse_categorical_crossentropy: 0.4240 - val_loss: 0.3964 - val_sparse_categorical_crossentropy: 0.3964\n",
            "Epoch 29/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4206 - sparse_categorical_crossentropy: 0.4206 - val_loss: 0.4061 - val_sparse_categorical_crossentropy: 0.4061\n",
            "Epoch 30/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4189 - sparse_categorical_crossentropy: 0.4189 - val_loss: 0.3959 - val_sparse_categorical_crossentropy: 0.3959\n",
            "Epoch 31/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4191 - sparse_categorical_crossentropy: 0.4191 - val_loss: 0.3939 - val_sparse_categorical_crossentropy: 0.3939\n",
            "Epoch 32/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4171 - sparse_categorical_crossentropy: 0.4171 - val_loss: 0.3991 - val_sparse_categorical_crossentropy: 0.3991\n",
            "Epoch 33/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4140 - sparse_categorical_crossentropy: 0.4140 - val_loss: 0.3927 - val_sparse_categorical_crossentropy: 0.3927\n",
            "Epoch 34/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4146 - sparse_categorical_crossentropy: 0.4146 - val_loss: 0.3923 - val_sparse_categorical_crossentropy: 0.3923\n",
            "Epoch 35/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4129 - sparse_categorical_crossentropy: 0.4129 - val_loss: 0.3916 - val_sparse_categorical_crossentropy: 0.3916\n",
            "Epoch 36/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4128 - sparse_categorical_crossentropy: 0.4128 - val_loss: 0.3942 - val_sparse_categorical_crossentropy: 0.3942\n",
            "Epoch 37/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4100 - sparse_categorical_crossentropy: 0.4100 - val_loss: 0.3971 - val_sparse_categorical_crossentropy: 0.3971\n",
            "Epoch 38/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4108 - sparse_categorical_crossentropy: 0.4108 - val_loss: 0.3920 - val_sparse_categorical_crossentropy: 0.3920\n",
            "Epoch 39/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4093 - sparse_categorical_crossentropy: 0.4093 - val_loss: 0.3968 - val_sparse_categorical_crossentropy: 0.3968\n",
            "Epoch 40/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4076 - sparse_categorical_crossentropy: 0.4076 - val_loss: 0.3923 - val_sparse_categorical_crossentropy: 0.3923\n",
            "Epoch 41/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4067 - sparse_categorical_crossentropy: 0.4067 - val_loss: 0.3885 - val_sparse_categorical_crossentropy: 0.3885\n",
            "Epoch 42/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4072 - sparse_categorical_crossentropy: 0.4072 - val_loss: 0.3867 - val_sparse_categorical_crossentropy: 0.3867\n",
            "Epoch 43/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4053 - sparse_categorical_crossentropy: 0.4053 - val_loss: 0.3889 - val_sparse_categorical_crossentropy: 0.3889\n",
            "Epoch 44/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4060 - sparse_categorical_crossentropy: 0.4060 - val_loss: 0.3853 - val_sparse_categorical_crossentropy: 0.3853\n",
            "Epoch 45/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4052 - sparse_categorical_crossentropy: 0.4052 - val_loss: 0.3860 - val_sparse_categorical_crossentropy: 0.3860\n",
            "Epoch 46/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4026 - sparse_categorical_crossentropy: 0.4026 - val_loss: 0.3827 - val_sparse_categorical_crossentropy: 0.3827\n",
            "Epoch 47/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4029 - sparse_categorical_crossentropy: 0.4029 - val_loss: 0.3838 - val_sparse_categorical_crossentropy: 0.3838\n",
            "Epoch 48/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4012 - sparse_categorical_crossentropy: 0.4012 - val_loss: 0.3933 - val_sparse_categorical_crossentropy: 0.3933\n",
            "Epoch 49/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4020 - sparse_categorical_crossentropy: 0.4020 - val_loss: 0.3854 - val_sparse_categorical_crossentropy: 0.3854\n",
            "Epoch 50/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4016 - sparse_categorical_crossentropy: 0.4016 - val_loss: 0.3892 - val_sparse_categorical_crossentropy: 0.3892\n",
            "Epoch 51/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4002 - sparse_categorical_crossentropy: 0.4002 - val_loss: 0.3943 - val_sparse_categorical_crossentropy: 0.3943\n",
            "Epoch 52/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3996 - sparse_categorical_crossentropy: 0.3996 - val_loss: 0.3839 - val_sparse_categorical_crossentropy: 0.3839\n",
            "Epoch 53/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3997 - sparse_categorical_crossentropy: 0.3997 - val_loss: 0.3837 - val_sparse_categorical_crossentropy: 0.3837\n",
            "Epoch 54/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3990 - sparse_categorical_crossentropy: 0.3990 - val_loss: 0.3977 - val_sparse_categorical_crossentropy: 0.3977\n",
            "Epoch 55/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3989 - sparse_categorical_crossentropy: 0.3989 - val_loss: 0.3823 - val_sparse_categorical_crossentropy: 0.3823\n",
            "Epoch 56/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3963 - sparse_categorical_crossentropy: 0.3963 - val_loss: 0.3832 - val_sparse_categorical_crossentropy: 0.3832\n",
            "Epoch 57/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3960 - sparse_categorical_crossentropy: 0.3960 - val_loss: 0.3802 - val_sparse_categorical_crossentropy: 0.3802\n",
            "Epoch 58/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3964 - sparse_categorical_crossentropy: 0.3964 - val_loss: 0.3782 - val_sparse_categorical_crossentropy: 0.3782\n",
            "Epoch 59/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3957 - sparse_categorical_crossentropy: 0.3957 - val_loss: 0.3778 - val_sparse_categorical_crossentropy: 0.3778\n",
            "Epoch 60/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3953 - sparse_categorical_crossentropy: 0.3953 - val_loss: 0.3906 - val_sparse_categorical_crossentropy: 0.3906\n",
            "Epoch 61/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3947 - sparse_categorical_crossentropy: 0.3947 - val_loss: 0.3867 - val_sparse_categorical_crossentropy: 0.3867\n",
            "Epoch 62/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3947 - sparse_categorical_crossentropy: 0.3947 - val_loss: 0.3915 - val_sparse_categorical_crossentropy: 0.3915\n",
            "Epoch 63/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3918 - sparse_categorical_crossentropy: 0.3918 - val_loss: 0.3825 - val_sparse_categorical_crossentropy: 0.3825\n",
            "Epoch 64/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3937 - sparse_categorical_crossentropy: 0.3937 - val_loss: 0.3771 - val_sparse_categorical_crossentropy: 0.3771\n",
            "Epoch 65/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3926 - sparse_categorical_crossentropy: 0.3926 - val_loss: 0.3768 - val_sparse_categorical_crossentropy: 0.3768\n",
            "Epoch 66/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3935 - sparse_categorical_crossentropy: 0.3935 - val_loss: 0.3752 - val_sparse_categorical_crossentropy: 0.3752\n",
            "Epoch 67/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3927 - sparse_categorical_crossentropy: 0.3927 - val_loss: 0.3777 - val_sparse_categorical_crossentropy: 0.3777\n",
            "Epoch 68/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3898 - sparse_categorical_crossentropy: 0.3898 - val_loss: 0.3739 - val_sparse_categorical_crossentropy: 0.3739\n",
            "Epoch 69/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3916 - sparse_categorical_crossentropy: 0.3916 - val_loss: 0.3787 - val_sparse_categorical_crossentropy: 0.3787\n",
            "Epoch 70/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3903 - sparse_categorical_crossentropy: 0.3903 - val_loss: 0.3757 - val_sparse_categorical_crossentropy: 0.3757\n",
            "Epoch 71/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3899 - sparse_categorical_crossentropy: 0.3899 - val_loss: 0.3886 - val_sparse_categorical_crossentropy: 0.3886\n",
            "Epoch 72/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3904 - sparse_categorical_crossentropy: 0.3904 - val_loss: 0.3796 - val_sparse_categorical_crossentropy: 0.3796\n",
            "Epoch 73/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3884 - sparse_categorical_crossentropy: 0.3884 - val_loss: 0.3762 - val_sparse_categorical_crossentropy: 0.3762\n",
            "Epoch 74/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3871 - sparse_categorical_crossentropy: 0.3871 - val_loss: 0.3784 - val_sparse_categorical_crossentropy: 0.3784\n",
            "Epoch 75/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3894 - sparse_categorical_crossentropy: 0.3894 - val_loss: 0.3749 - val_sparse_categorical_crossentropy: 0.3749\n",
            "Epoch 76/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3880 - sparse_categorical_crossentropy: 0.3880 - val_loss: 0.3779 - val_sparse_categorical_crossentropy: 0.3779\n",
            "Epoch 77/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3870 - sparse_categorical_crossentropy: 0.3870 - val_loss: 0.3788 - val_sparse_categorical_crossentropy: 0.3788\n",
            "Epoch 78/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3866 - sparse_categorical_crossentropy: 0.3866 - val_loss: 0.3815 - val_sparse_categorical_crossentropy: 0.3815\n",
            "Epoch 79/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3885 - sparse_categorical_crossentropy: 0.3885 - val_loss: 0.3801 - val_sparse_categorical_crossentropy: 0.3801\n",
            "Epoch 80/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3850 - sparse_categorical_crossentropy: 0.3850 - val_loss: 0.3809 - val_sparse_categorical_crossentropy: 0.3809\n",
            "Epoch 81/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3836 - sparse_categorical_crossentropy: 0.3836 - val_loss: 0.3746 - val_sparse_categorical_crossentropy: 0.3746\n",
            "Epoch 82/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3851 - sparse_categorical_crossentropy: 0.3851 - val_loss: 0.3749 - val_sparse_categorical_crossentropy: 0.3749\n",
            "Epoch 83/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3859 - sparse_categorical_crossentropy: 0.3859 - val_loss: 0.3755 - val_sparse_categorical_crossentropy: 0.3755\n",
            "Epoch 84/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3844 - sparse_categorical_crossentropy: 0.3844 - val_loss: 0.3748 - val_sparse_categorical_crossentropy: 0.3748\n",
            "Epoch 85/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3847 - sparse_categorical_crossentropy: 0.3847 - val_loss: 0.3754 - val_sparse_categorical_crossentropy: 0.3754\n",
            "Epoch 86/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3840 - sparse_categorical_crossentropy: 0.3840 - val_loss: 0.3850 - val_sparse_categorical_crossentropy: 0.3850\n",
            "Epoch 87/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3842 - sparse_categorical_crossentropy: 0.3842 - val_loss: 0.3730 - val_sparse_categorical_crossentropy: 0.3730\n",
            "Epoch 88/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3827 - sparse_categorical_crossentropy: 0.3827 - val_loss: 0.3732 - val_sparse_categorical_crossentropy: 0.3732\n",
            "Epoch 89/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3826 - sparse_categorical_crossentropy: 0.3826 - val_loss: 0.3754 - val_sparse_categorical_crossentropy: 0.3754\n",
            "Epoch 90/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3832 - sparse_categorical_crossentropy: 0.3832 - val_loss: 0.3788 - val_sparse_categorical_crossentropy: 0.3788\n",
            "Epoch 91/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3806 - sparse_categorical_crossentropy: 0.3806 - val_loss: 0.3726 - val_sparse_categorical_crossentropy: 0.3726\n",
            "Epoch 92/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3826 - sparse_categorical_crossentropy: 0.3826 - val_loss: 0.3732 - val_sparse_categorical_crossentropy: 0.3732\n",
            "Epoch 93/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3819 - sparse_categorical_crossentropy: 0.3819 - val_loss: 0.3735 - val_sparse_categorical_crossentropy: 0.3735\n",
            "Epoch 94/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3820 - sparse_categorical_crossentropy: 0.3820 - val_loss: 0.3804 - val_sparse_categorical_crossentropy: 0.3804\n",
            "Epoch 95/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3822 - sparse_categorical_crossentropy: 0.3822 - val_loss: 0.3745 - val_sparse_categorical_crossentropy: 0.3745\n",
            "Epoch 96/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3804 - sparse_categorical_crossentropy: 0.3804 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 97/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3832 - sparse_categorical_crossentropy: 0.3832 - val_loss: 0.3747 - val_sparse_categorical_crossentropy: 0.3747\n",
            "Epoch 98/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3808 - sparse_categorical_crossentropy: 0.3808 - val_loss: 0.3720 - val_sparse_categorical_crossentropy: 0.3720\n",
            "Epoch 99/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3809 - sparse_categorical_crossentropy: 0.3809 - val_loss: 0.3740 - val_sparse_categorical_crossentropy: 0.3740\n",
            "Epoch 100/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3794 - sparse_categorical_crossentropy: 0.3794 - val_loss: 0.3750 - val_sparse_categorical_crossentropy: 0.3750\n",
            "Epoch 101/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3795 - sparse_categorical_crossentropy: 0.3795 - val_loss: 0.3772 - val_sparse_categorical_crossentropy: 0.3772\n",
            "Epoch 102/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3798 - sparse_categorical_crossentropy: 0.3798 - val_loss: 0.3752 - val_sparse_categorical_crossentropy: 0.3752\n",
            "Epoch 103/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3792 - sparse_categorical_crossentropy: 0.3792 - val_loss: 0.3771 - val_sparse_categorical_crossentropy: 0.3771\n",
            "Epoch 104/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3787 - sparse_categorical_crossentropy: 0.3787 - val_loss: 0.3708 - val_sparse_categorical_crossentropy: 0.3708\n",
            "Epoch 105/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3769 - sparse_categorical_crossentropy: 0.3769 - val_loss: 0.3732 - val_sparse_categorical_crossentropy: 0.3732\n",
            "Epoch 106/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3782 - sparse_categorical_crossentropy: 0.3782 - val_loss: 0.3746 - val_sparse_categorical_crossentropy: 0.3746\n",
            "Epoch 107/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3780 - sparse_categorical_crossentropy: 0.3780 - val_loss: 0.3767 - val_sparse_categorical_crossentropy: 0.3767\n",
            "Epoch 108/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3774 - sparse_categorical_crossentropy: 0.3774 - val_loss: 0.3722 - val_sparse_categorical_crossentropy: 0.3722\n",
            "Epoch 109/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3767 - sparse_categorical_crossentropy: 0.3767 - val_loss: 0.3719 - val_sparse_categorical_crossentropy: 0.3719\n",
            "Epoch 110/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3772 - sparse_categorical_crossentropy: 0.3772 - val_loss: 0.3688 - val_sparse_categorical_crossentropy: 0.3688\n",
            "Epoch 111/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3771 - sparse_categorical_crossentropy: 0.3771 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 112/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3773 - sparse_categorical_crossentropy: 0.3773 - val_loss: 0.3737 - val_sparse_categorical_crossentropy: 0.3737\n",
            "Epoch 113/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3746 - sparse_categorical_crossentropy: 0.3746 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 114/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3756 - sparse_categorical_crossentropy: 0.3756 - val_loss: 0.3698 - val_sparse_categorical_crossentropy: 0.3698\n",
            "Epoch 115/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3755 - sparse_categorical_crossentropy: 0.3755 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "Epoch 116/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3759 - sparse_categorical_crossentropy: 0.3759 - val_loss: 0.3719 - val_sparse_categorical_crossentropy: 0.3719\n",
            "Epoch 117/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3757 - sparse_categorical_crossentropy: 0.3757 - val_loss: 0.3729 - val_sparse_categorical_crossentropy: 0.3729\n",
            "Epoch 118/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3746 - sparse_categorical_crossentropy: 0.3746 - val_loss: 0.3689 - val_sparse_categorical_crossentropy: 0.3689\n",
            "Epoch 119/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3749 - sparse_categorical_crossentropy: 0.3749 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "Epoch 120/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3737 - sparse_categorical_crossentropy: 0.3737 - val_loss: 0.3709 - val_sparse_categorical_crossentropy: 0.3709\n",
            "Epoch 121/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3744 - sparse_categorical_crossentropy: 0.3744 - val_loss: 0.3733 - val_sparse_categorical_crossentropy: 0.3733\n",
            "Epoch 122/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3742 - sparse_categorical_crossentropy: 0.3742 - val_loss: 0.3703 - val_sparse_categorical_crossentropy: 0.3703\n",
            "Epoch 123/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3752 - sparse_categorical_crossentropy: 0.3752 - val_loss: 0.3738 - val_sparse_categorical_crossentropy: 0.3738\n",
            "Epoch 124/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3751 - sparse_categorical_crossentropy: 0.3751 - val_loss: 0.3746 - val_sparse_categorical_crossentropy: 0.3746\n",
            "Epoch 125/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3732 - sparse_categorical_crossentropy: 0.3732 - val_loss: 0.3750 - val_sparse_categorical_crossentropy: 0.3750\n",
            "Epoch 126/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3735 - sparse_categorical_crossentropy: 0.3735 - val_loss: 0.3682 - val_sparse_categorical_crossentropy: 0.3682\n",
            "Epoch 127/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3735 - sparse_categorical_crossentropy: 0.3735 - val_loss: 0.3723 - val_sparse_categorical_crossentropy: 0.3723\n",
            "Epoch 128/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3738 - sparse_categorical_crossentropy: 0.3738 - val_loss: 0.3720 - val_sparse_categorical_crossentropy: 0.3720\n",
            "Epoch 129/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3724 - sparse_categorical_crossentropy: 0.3724 - val_loss: 0.3718 - val_sparse_categorical_crossentropy: 0.3718\n",
            "Epoch 130/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3740 - sparse_categorical_crossentropy: 0.3740 - val_loss: 0.3727 - val_sparse_categorical_crossentropy: 0.3727\n",
            "Epoch 131/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3713 - sparse_categorical_crossentropy: 0.3713 - val_loss: 0.3781 - val_sparse_categorical_crossentropy: 0.3781\n",
            "Epoch 132/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3724 - sparse_categorical_crossentropy: 0.3724 - val_loss: 0.3741 - val_sparse_categorical_crossentropy: 0.3741\n",
            "Epoch 133/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3716 - sparse_categorical_crossentropy: 0.3716 - val_loss: 0.3717 - val_sparse_categorical_crossentropy: 0.3717\n",
            "Epoch 134/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3731 - sparse_categorical_crossentropy: 0.3731 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 135/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3718 - sparse_categorical_crossentropy: 0.3718 - val_loss: 0.3717 - val_sparse_categorical_crossentropy: 0.3717\n",
            "Epoch 136/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3713 - sparse_categorical_crossentropy: 0.3713 - val_loss: 0.3704 - val_sparse_categorical_crossentropy: 0.3704\n",
            "Epoch 137/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3712 - sparse_categorical_crossentropy: 0.3712 - val_loss: 0.3756 - val_sparse_categorical_crossentropy: 0.3756\n",
            "Epoch 138/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3724 - sparse_categorical_crossentropy: 0.3724 - val_loss: 0.3759 - val_sparse_categorical_crossentropy: 0.3759\n",
            "Epoch 139/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3700 - sparse_categorical_crossentropy: 0.3700 - val_loss: 0.3742 - val_sparse_categorical_crossentropy: 0.3742\n",
            "Epoch 140/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3714 - sparse_categorical_crossentropy: 0.3714 - val_loss: 0.3676 - val_sparse_categorical_crossentropy: 0.3676\n",
            "Epoch 141/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3711 - sparse_categorical_crossentropy: 0.3711 - val_loss: 0.3674 - val_sparse_categorical_crossentropy: 0.3674\n",
            "Epoch 142/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3690 - sparse_categorical_crossentropy: 0.3690 - val_loss: 0.3708 - val_sparse_categorical_crossentropy: 0.3708\n",
            "Epoch 143/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3707 - sparse_categorical_crossentropy: 0.3707 - val_loss: 0.3666 - val_sparse_categorical_crossentropy: 0.3666\n",
            "Epoch 144/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3696 - sparse_categorical_crossentropy: 0.3696 - val_loss: 0.3705 - val_sparse_categorical_crossentropy: 0.3705\n",
            "Epoch 145/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3708 - sparse_categorical_crossentropy: 0.3708 - val_loss: 0.3760 - val_sparse_categorical_crossentropy: 0.3760\n",
            "Epoch 146/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3706 - sparse_categorical_crossentropy: 0.3706 - val_loss: 0.3689 - val_sparse_categorical_crossentropy: 0.3689\n",
            "Epoch 147/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3684 - sparse_categorical_crossentropy: 0.3684 - val_loss: 0.3738 - val_sparse_categorical_crossentropy: 0.3738\n",
            "Epoch 148/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3684 - sparse_categorical_crossentropy: 0.3684 - val_loss: 0.3675 - val_sparse_categorical_crossentropy: 0.3675\n",
            "Epoch 149/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3695 - sparse_categorical_crossentropy: 0.3695 - val_loss: 0.3705 - val_sparse_categorical_crossentropy: 0.3705\n",
            "Epoch 150/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3696 - sparse_categorical_crossentropy: 0.3696 - val_loss: 0.3688 - val_sparse_categorical_crossentropy: 0.3688\n",
            "Epoch 151/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3684 - sparse_categorical_crossentropy: 0.3684 - val_loss: 0.3729 - val_sparse_categorical_crossentropy: 0.3729\n",
            "Epoch 152/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3692 - sparse_categorical_crossentropy: 0.3692 - val_loss: 0.3693 - val_sparse_categorical_crossentropy: 0.3693\n",
            "Epoch 153/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3675 - sparse_categorical_crossentropy: 0.3675 - val_loss: 0.3705 - val_sparse_categorical_crossentropy: 0.3705\n",
            "Epoch 154/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3684 - sparse_categorical_crossentropy: 0.3684 - val_loss: 0.3668 - val_sparse_categorical_crossentropy: 0.3668\n",
            "Epoch 155/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3667 - sparse_categorical_crossentropy: 0.3667 - val_loss: 0.3682 - val_sparse_categorical_crossentropy: 0.3682\n",
            "Epoch 156/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3670 - sparse_categorical_crossentropy: 0.3670 - val_loss: 0.3697 - val_sparse_categorical_crossentropy: 0.3697\n",
            "Epoch 157/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3682 - sparse_categorical_crossentropy: 0.3682 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 158/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3675 - sparse_categorical_crossentropy: 0.3675 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 159/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3682 - sparse_categorical_crossentropy: 0.3682 - val_loss: 0.3697 - val_sparse_categorical_crossentropy: 0.3697\n",
            "Epoch 160/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3676 - sparse_categorical_crossentropy: 0.3676 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "Epoch 161/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3669 - sparse_categorical_crossentropy: 0.3669 - val_loss: 0.3673 - val_sparse_categorical_crossentropy: 0.3673\n",
            "Epoch 162/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3678 - sparse_categorical_crossentropy: 0.3678 - val_loss: 0.3689 - val_sparse_categorical_crossentropy: 0.3689\n",
            "Epoch 163/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3665 - sparse_categorical_crossentropy: 0.3665 - val_loss: 0.3693 - val_sparse_categorical_crossentropy: 0.3693\n",
            "Epoch 164/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3671 - sparse_categorical_crossentropy: 0.3671 - val_loss: 0.3794 - val_sparse_categorical_crossentropy: 0.3794\n",
            "Epoch 165/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3663 - sparse_categorical_crossentropy: 0.3663 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 166/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3659 - sparse_categorical_crossentropy: 0.3659 - val_loss: 0.3708 - val_sparse_categorical_crossentropy: 0.3708\n",
            "Epoch 167/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3649 - sparse_categorical_crossentropy: 0.3649 - val_loss: 0.3688 - val_sparse_categorical_crossentropy: 0.3688\n",
            "Epoch 168/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3654 - sparse_categorical_crossentropy: 0.3654 - val_loss: 0.3695 - val_sparse_categorical_crossentropy: 0.3695\n",
            "\n",
            "Epoch 00168: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.\n",
            "Epoch 169/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3607 - sparse_categorical_crossentropy: 0.3607 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 170/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3601 - sparse_categorical_crossentropy: 0.3601 - val_loss: 0.3666 - val_sparse_categorical_crossentropy: 0.3666\n",
            "Epoch 171/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3598 - sparse_categorical_crossentropy: 0.3598 - val_loss: 0.3648 - val_sparse_categorical_crossentropy: 0.3648\n",
            "Epoch 172/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3602 - sparse_categorical_crossentropy: 0.3602 - val_loss: 0.3668 - val_sparse_categorical_crossentropy: 0.3668\n",
            "Epoch 173/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3600 - sparse_categorical_crossentropy: 0.3600 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 174/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3573 - sparse_categorical_crossentropy: 0.3573 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 175/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3595 - sparse_categorical_crossentropy: 0.3595 - val_loss: 0.3668 - val_sparse_categorical_crossentropy: 0.3668\n",
            "Epoch 176/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3576 - sparse_categorical_crossentropy: 0.3576 - val_loss: 0.3653 - val_sparse_categorical_crossentropy: 0.3653\n",
            "Epoch 177/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3584 - sparse_categorical_crossentropy: 0.3584 - val_loss: 0.3653 - val_sparse_categorical_crossentropy: 0.3653\n",
            "Epoch 178/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3596 - sparse_categorical_crossentropy: 0.3596 - val_loss: 0.3646 - val_sparse_categorical_crossentropy: 0.3646\n",
            "Epoch 179/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3584 - sparse_categorical_crossentropy: 0.3584 - val_loss: 0.3664 - val_sparse_categorical_crossentropy: 0.3664\n",
            "Epoch 180/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3599 - sparse_categorical_crossentropy: 0.3599 - val_loss: 0.3671 - val_sparse_categorical_crossentropy: 0.3671\n",
            "Epoch 181/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3590 - sparse_categorical_crossentropy: 0.3590 - val_loss: 0.3701 - val_sparse_categorical_crossentropy: 0.3701\n",
            "Epoch 182/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3586 - sparse_categorical_crossentropy: 0.3586 - val_loss: 0.3662 - val_sparse_categorical_crossentropy: 0.3662\n",
            "Epoch 183/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3585 - sparse_categorical_crossentropy: 0.3585 - val_loss: 0.3662 - val_sparse_categorical_crossentropy: 0.3662\n",
            "Epoch 184/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3578 - sparse_categorical_crossentropy: 0.3578 - val_loss: 0.3668 - val_sparse_categorical_crossentropy: 0.3668\n",
            "Epoch 185/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3583 - sparse_categorical_crossentropy: 0.3583 - val_loss: 0.3664 - val_sparse_categorical_crossentropy: 0.3664\n",
            "Epoch 186/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3578 - sparse_categorical_crossentropy: 0.3578 - val_loss: 0.3653 - val_sparse_categorical_crossentropy: 0.3653\n",
            "Epoch 187/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3579 - sparse_categorical_crossentropy: 0.3579 - val_loss: 0.3662 - val_sparse_categorical_crossentropy: 0.3662\n",
            "Epoch 188/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3578 - sparse_categorical_crossentropy: 0.3578 - val_loss: 0.3651 - val_sparse_categorical_crossentropy: 0.3651\n",
            "Epoch 189/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3578 - sparse_categorical_crossentropy: 0.3578 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 190/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3569 - sparse_categorical_crossentropy: 0.3569 - val_loss: 0.3664 - val_sparse_categorical_crossentropy: 0.3664\n",
            "Epoch 191/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3575 - sparse_categorical_crossentropy: 0.3575 - val_loss: 0.3665 - val_sparse_categorical_crossentropy: 0.3665\n",
            "Epoch 192/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3577 - sparse_categorical_crossentropy: 0.3577 - val_loss: 0.3646 - val_sparse_categorical_crossentropy: 0.3646\n",
            "Epoch 193/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3571 - sparse_categorical_crossentropy: 0.3571 - val_loss: 0.3669 - val_sparse_categorical_crossentropy: 0.3669\n",
            "Epoch 194/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3580 - sparse_categorical_crossentropy: 0.3580 - val_loss: 0.3664 - val_sparse_categorical_crossentropy: 0.3664\n",
            "Epoch 195/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3563 - sparse_categorical_crossentropy: 0.3563 - val_loss: 0.3649 - val_sparse_categorical_crossentropy: 0.3649\n",
            "Epoch 196/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3563 - sparse_categorical_crossentropy: 0.3563 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 197/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3573 - sparse_categorical_crossentropy: 0.3573 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 198/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3570 - sparse_categorical_crossentropy: 0.3570 - val_loss: 0.3667 - val_sparse_categorical_crossentropy: 0.3667\n",
            "Epoch 199/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3564 - sparse_categorical_crossentropy: 0.3564 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 200/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3559 - sparse_categorical_crossentropy: 0.3559 - val_loss: 0.3676 - val_sparse_categorical_crossentropy: 0.3676\n",
            "Epoch 201/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3566 - sparse_categorical_crossentropy: 0.3566 - val_loss: 0.3665 - val_sparse_categorical_crossentropy: 0.3665\n",
            "Epoch 202/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3564 - sparse_categorical_crossentropy: 0.3564 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 203/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3577 - sparse_categorical_crossentropy: 0.3577 - val_loss: 0.3667 - val_sparse_categorical_crossentropy: 0.3667\n",
            "\n",
            "Epoch 00203: ReduceLROnPlateau reducing learning rate to 9.999999259090306e-06.\n",
            "Epoch 204/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3543 - sparse_categorical_crossentropy: 0.3543 - val_loss: 0.3666 - val_sparse_categorical_crossentropy: 0.3666\n",
            "Epoch 205/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3552 - sparse_categorical_crossentropy: 0.3552 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 206/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3549 - sparse_categorical_crossentropy: 0.3549 - val_loss: 0.3653 - val_sparse_categorical_crossentropy: 0.3653\n",
            "Epoch 207/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3548 - sparse_categorical_crossentropy: 0.3548 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 208/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3552 - sparse_categorical_crossentropy: 0.3552 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 209/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3540 - sparse_categorical_crossentropy: 0.3540 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 210/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3539 - sparse_categorical_crossentropy: 0.3539 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 211/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3529 - sparse_categorical_crossentropy: 0.3529 - val_loss: 0.3662 - val_sparse_categorical_crossentropy: 0.3662\n",
            "Epoch 212/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3546 - sparse_categorical_crossentropy: 0.3546 - val_loss: 0.3651 - val_sparse_categorical_crossentropy: 0.3651\n",
            "Epoch 213/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3544 - sparse_categorical_crossentropy: 0.3544 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 214/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3546 - sparse_categorical_crossentropy: 0.3546 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 215/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3546 - sparse_categorical_crossentropy: 0.3546 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 216/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3525 - sparse_categorical_crossentropy: 0.3525 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 217/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3535 - sparse_categorical_crossentropy: 0.3535 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 218/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3537 - sparse_categorical_crossentropy: 0.3537 - val_loss: 0.3653 - val_sparse_categorical_crossentropy: 0.3653\n",
            "Epoch 219/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3532 - sparse_categorical_crossentropy: 0.3532 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 220/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3541 - sparse_categorical_crossentropy: 0.3541 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 221/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3542 - sparse_categorical_crossentropy: 0.3542 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 222/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3533 - sparse_categorical_crossentropy: 0.3533 - val_loss: 0.3646 - val_sparse_categorical_crossentropy: 0.3646\n",
            "Epoch 223/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3540 - sparse_categorical_crossentropy: 0.3540 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 224/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3539 - sparse_categorical_crossentropy: 0.3539 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 225/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3545 - sparse_categorical_crossentropy: 0.3545 - val_loss: 0.3650 - val_sparse_categorical_crossentropy: 0.3650\n",
            "Epoch 226/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3538 - sparse_categorical_crossentropy: 0.3538 - val_loss: 0.3650 - val_sparse_categorical_crossentropy: 0.3650\n",
            "Epoch 227/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3530 - sparse_categorical_crossentropy: 0.3530 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 228/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3534 - sparse_categorical_crossentropy: 0.3534 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "\n",
            "Epoch 00228: ReduceLROnPlateau reducing learning rate to 3.162277292675049e-06.\n",
            "Epoch 229/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3528 - sparse_categorical_crossentropy: 0.3528 - val_loss: 0.3653 - val_sparse_categorical_crossentropy: 0.3653\n",
            "Epoch 230/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3534 - sparse_categorical_crossentropy: 0.3534 - val_loss: 0.3652 - val_sparse_categorical_crossentropy: 0.3652\n",
            "Epoch 231/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3526 - sparse_categorical_crossentropy: 0.3526 - val_loss: 0.3650 - val_sparse_categorical_crossentropy: 0.3650\n",
            "Epoch 232/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3525 - sparse_categorical_crossentropy: 0.3525 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 233/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3512 - sparse_categorical_crossentropy: 0.3512 - val_loss: 0.3647 - val_sparse_categorical_crossentropy: 0.3647\n",
            "Epoch 234/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3521 - sparse_categorical_crossentropy: 0.3521 - val_loss: 0.3646 - val_sparse_categorical_crossentropy: 0.3646\n",
            "Epoch 235/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3530 - sparse_categorical_crossentropy: 0.3530 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 236/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3533 - sparse_categorical_crossentropy: 0.3533 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 237/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3531 - sparse_categorical_crossentropy: 0.3531 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 238/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3518 - sparse_categorical_crossentropy: 0.3518 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 239/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3532 - sparse_categorical_crossentropy: 0.3532 - val_loss: 0.3651 - val_sparse_categorical_crossentropy: 0.3651\n",
            "Epoch 240/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3527 - sparse_categorical_crossentropy: 0.3527 - val_loss: 0.3651 - val_sparse_categorical_crossentropy: 0.3651\n",
            "Epoch 241/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3525 - sparse_categorical_crossentropy: 0.3525 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 242/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3521 - sparse_categorical_crossentropy: 0.3521 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 243/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3524 - sparse_categorical_crossentropy: 0.3524 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 244/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3529 - sparse_categorical_crossentropy: 0.3529 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 245/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3527 - sparse_categorical_crossentropy: 0.3527 - val_loss: 0.3651 - val_sparse_categorical_crossentropy: 0.3651\n",
            "Epoch 246/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3530 - sparse_categorical_crossentropy: 0.3530 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 247/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3530 - sparse_categorical_crossentropy: 0.3530 - val_loss: 0.3651 - val_sparse_categorical_crossentropy: 0.3651\n",
            "Epoch 248/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3524 - sparse_categorical_crossentropy: 0.3524 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 249/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3528 - sparse_categorical_crossentropy: 0.3528 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 250/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3529 - sparse_categorical_crossentropy: 0.3529 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 251/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3537 - sparse_categorical_crossentropy: 0.3537 - val_loss: 0.3653 - val_sparse_categorical_crossentropy: 0.3653\n",
            "Epoch 252/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3528 - sparse_categorical_crossentropy: 0.3528 - val_loss: 0.3652 - val_sparse_categorical_crossentropy: 0.3652\n",
            "Epoch 253/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3529 - sparse_categorical_crossentropy: 0.3529 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "\n",
            "Epoch 00253: ReduceLROnPlateau reducing learning rate to 9.999999115286567e-07.\n",
            "Epoch 254/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3519 - sparse_categorical_crossentropy: 0.3519 - val_loss: 0.3652 - val_sparse_categorical_crossentropy: 0.3652\n",
            "Epoch 255/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3524 - sparse_categorical_crossentropy: 0.3524 - val_loss: 0.3651 - val_sparse_categorical_crossentropy: 0.3651\n",
            "Epoch 256/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3519 - sparse_categorical_crossentropy: 0.3519 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 257/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3531 - sparse_categorical_crossentropy: 0.3531 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 258/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3527 - sparse_categorical_crossentropy: 0.3527 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00258: early stopping\n",
            "모델저장완료\n",
            "==================================================\n",
            "Loss와 ACC에 대한 Plot을 그립니다\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAETCAYAAABnSkJLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQgghcpoEKiGEEDnt/wOr0NJXMrQvsAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "기존nn모델의 train loss를 출력합니다\n",
            "train, loss and metric: [0.32823570037562466, 0.32823570037562466]\n",
            "기존nn모델의 valid loss를 출력합니다\n",
            "valid, loss and metric: [0.36459644830621496, 0.36459644830621496]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5oJjBcLFzzL",
        "colab_type": "code",
        "outputId": "52c6955a-9739-4660-86c9-5175c4b1472c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "savingpath_csv = 'drive/My Drive/데이콘_천체유형/파일제출/' + csv_folder + '/이상치_제거_initial_rate_%s.csv' % initial_rate\n",
        "\n",
        "print('final rate는 다음과 같습니다')\n",
        "print(hist.history['lr'][np.argmin(hist.history['val_loss'])])\n",
        "print('='*25)\n",
        "y_pred = nn_model.predict(test_x)\n",
        "submission = pd.DataFrame(data=y_pred, columns=sample_submission.columns, index=sample_submission.index)\n",
        "submission.to_csv(savingpath_csv, index=True)\n",
        "print('csv 저장완료')\n",
        "\n",
        "print('='*50)\n",
        "best_val_loss = np.min(hist.history['val_loss'])\n",
        "print(\"best_valid_loss: {}\".format(best_val_loss))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final rate는 다음과 같습니다\n",
            "3.1622774e-05\n",
            "=========================\n",
            "csv 저장완료\n",
            "==================================================\n",
            "best_valid_loss: 0.36459644830621496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7UR0QNdWEF9",
        "colab_type": "text"
      },
      "source": [
        "# 저장된 모델 로드해오기 & 평가해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1W33dlgNXSTo",
        "outputId": "9084d431-0d8c-4358-bfdd-77be9e884eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "initial_rate = 1e-2\n",
        "SAVEMODEL_NEWFOLDER1 = 'drive/My Drive/데이콘_천체유형/기존모델저장/' + csv_folder\n",
        "MODEL_SAVE_FOLDER_PATH1 = SAVEMODEL_NEWFOLDER1 +  '/initial_rate=%s/' % initial_rate ## 기존모델저장\n",
        "\n",
        "## model load\n",
        "json_path = MODEL_SAVE_FOLDER_PATH1 + \"model1.json\"\n",
        "weight_path = MODEL_SAVE_FOLDER_PATH1 +\"model1.h5\"\n",
        "\n",
        "from keras.models import model_from_json\n",
        "json_file = open(json_path, \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "## model weight load\n",
        "loaded_model.load_weights(weight_path)\n",
        "\n",
        "## model load and evaluation\n",
        "from keras import optimizers\n",
        "final_rate = hist.history['lr'][np.argmin(hist.history['val_loss'])]\n",
        "print(\"Loaded model from disk\")\n",
        "load_optimizer = optimizers.Adam(\n",
        "    lr=final_rate,\n",
        ")\n",
        "CCE = metrics.sparse_categorical_crossentropy\n",
        "loaded_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=load_optimizer\n",
        "                     ,metrics=[CCE]\n",
        "                     )\n",
        "batchsize=1024\n",
        "train_score = loaded_model.evaluate(train_input,train_target,verbose=0, batch_size=batchsize)\n",
        "print('Load한 Check모델의 train loss를 출력합니다')\n",
        "print(\"train, loss and metric: {}\".format(train_score))\n",
        "cv_score = loaded_model.evaluate(cv_input,cv_target,verbose=0, batch_size=batchsize)\n",
        "print('Load한 Check모델의 val loss를 출력합니다')\n",
        "print(\"valid, loss and metric: {}\".format(cv_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n",
            "Load한 Check모델의 train loss를 출력합니다\n",
            "train, loss and metric: [0.3077159990481829, 0.3077159990481829]\n",
            "Load한 Check모델의 val loss를 출력합니다\n",
            "valid, loss and metric: [0.3495782041837298, 0.3495782041837298]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mhSMJ_m3EiI",
        "colab_type": "text"
      },
      "source": [
        "## 모델 layer 자세히 보기 (가중치w, 편향값b 확인)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNI-3z8BzMAo",
        "colab_type": "code",
        "outputId": "7bbf4e55-17fd-40eb-9216-7aa424129587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "csv_folder = ''\n",
        "SAVEMODEL_NEWFOLDER1 = 'drive/My Drive/데이콘_천체유형/기존모델저장/' + csv_folder\n",
        "initial_rate = 0.01\n",
        "MODEL_SAVE_FOLDER_PATH1 = SAVEMODEL_NEWFOLDER1 +  '/initial_rate=%s/' % initial_rate ## 기존모델저장\n",
        "\n",
        "## model load\n",
        "json_path = MODEL_SAVE_FOLDER_PATH1 + \"model1.json\"\n",
        "weight_path = MODEL_SAVE_FOLDER_PATH1 +\"model1.h5\"\n",
        "\n",
        "from keras.models import model_from_json\n",
        "json_file = open(json_path, \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "## model weight load\n",
        "loaded_model.load_weights(weight_path)\n",
        "\n",
        "## model load and evaluation\n",
        "from keras import optimizers\n",
        "#final_rate = hist.history['lr'][np.argmin(hist.history['val_loss'])]\n",
        "print(\"Loaded model from disk\")\n",
        "load_optimizer = optimizers.Adam(\n",
        "    #lr=final_rate\n",
        "    lr = 0.0003162278\n",
        ")\n",
        "CCE = metrics.sparse_categorical_crossentropy\n",
        "loaded_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=load_optimizer\n",
        "                     ,metrics=[CCE]\n",
        "                     )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKAM00J7zwTN",
        "colab_type": "code",
        "outputId": "189ebe48-d72e-456b-81a3-7963ea664198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        }
      },
      "source": [
        "t = 1\n",
        "for layer in loaded_model.layers:\n",
        "  print('='*25 + '{0}번째'.format(t) + '='*25)\n",
        "  weights = layer.get_weights()\n",
        "  print(  (np.array(weights) ).shape )\n",
        "  t += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================1번째=========================\n",
            "(0,)\n",
            "=========================2번째=========================\n",
            "(2,)\n",
            "=========================3번째=========================\n",
            "(4, 2373)\n",
            "=========================4번째=========================\n",
            "(0,)\n",
            "=========================5번째=========================\n",
            "(0,)\n",
            "=========================6번째=========================\n",
            "(2,)\n",
            "=========================7번째=========================\n",
            "(4, 2355)\n",
            "=========================8번째=========================\n",
            "(0,)\n",
            "=========================9번째=========================\n",
            "(0,)\n",
            "=========================10번째=========================\n",
            "(2,)\n",
            "=========================11번째=========================\n",
            "(4, 1197)\n",
            "=========================12번째=========================\n",
            "(0,)\n",
            "=========================13번째=========================\n",
            "(0,)\n",
            "=========================14번째=========================\n",
            "(2,)\n",
            "=========================15번째=========================\n",
            "(4, 1187)\n",
            "=========================16번째=========================\n",
            "(0,)\n",
            "=========================17번째=========================\n",
            "(0,)\n",
            "=========================18번째=========================\n",
            "(2,)\n",
            "=========================19번째=========================\n",
            "(4, 612)\n",
            "=========================20번째=========================\n",
            "(0,)\n",
            "=========================21번째=========================\n",
            "(0,)\n",
            "=========================22번째=========================\n",
            "(2,)\n",
            "=========================23번째=========================\n",
            "(4, 602)\n",
            "=========================24번째=========================\n",
            "(0,)\n",
            "=========================25번째=========================\n",
            "(0,)\n",
            "=========================26번째=========================\n",
            "(2,)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}