{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1번_딥러닝_ReducelronPleteau_Gelu사용.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoonJaeHoon/Dacon_Behavioral_Data_Analysis/blob/master/1%EB%B2%88_%EB%94%A5%EB%9F%AC%EB%8B%9D_ReducelronPleteau_Gelu%EC%82%AC%EC%9A%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkIsyTZNAJ2P",
        "colab_type": "code",
        "outputId": "3a88dcea-b3d7-4729-db58-28b6b67d25d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoP1zL0pARNV",
        "colab_type": "code",
        "outputId": "92e50689-7f5e-4f12-97bc-11b86d89df99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report #for model evaluation\n",
        "from sklearn.metrics import confusion_matrix #for model evaluation\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit #for data splitting\n",
        "np.random.seed(123) #ensure reproducibility\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('seaborn') # matplotlib 도 종류가 다양하기 때문에 seaborn 스타일로 지정한 거임.\n",
        "sns.set(font_scale=1) # (기본으로) 폰트 크기 2.5로 지정 미리 해놓는거임, 2.5면 꽤 크게 나옴\n",
        "\n",
        "#ignore warnings\n",
        "import warnings\n",
        "#warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd \n",
        "\n",
        "from numpy.random import seed\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "\n",
        "#train = pd.read_csv('drive/My Drive/데이콘_천체유형/train.csv', index_col=0)\n",
        "train = pd.read_csv('drive/My Drive/데이콘_천체유형/이상치_최대한_적게_train.csv', index_col=None)\n",
        "test = pd.read_csv('drive/My Drive/데이콘_천체유형/test.csv', index_col=0)\n",
        "sample_submission = pd.read_csv('drive/My Drive/데이콘_천체유형/sample_submission.csv', index_col=0)\n",
        "\n",
        "\n",
        "print('csv 파일 (train, test, sample)을 불러왔습니다')\n",
        "print('train shape : {0}'.format(train.shape))\n",
        "print('test shape : {0}'.format(test.shape))\n",
        "print('='*50)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "csv 파일 (train, test, sample)을 불러왔습니다\n",
            "train shape : (199898, 23)\n",
            "test shape : (10009, 21)\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4h9A4H3E_XB",
        "colab_type": "text"
      },
      "source": [
        "## petroMag_g 의 maximum 값 하나는 없애도 됨. ( 합리적으로)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKEZBrFkCwzC",
        "colab_type": "code",
        "outputId": "849981fa-bc18-4741-89ce-5d856547cd7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# setting outlier symbol, title, xlabel\n",
        "name = 'petroMag_g'\n",
        "plt.boxplot(train[name], sym=\"bo\")\n",
        "plt.title('Box plot of {0}'.format(name))\n",
        "plt.xticks([1], [name])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# setting outlier symbol, title, xlabel\n",
        "name = 'petroMag_g'\n",
        "plt.boxplot(test[name], sym=\"bo\")\n",
        "plt.title('Box plot of {0}'.format(name))\n",
        "plt.xticks([1], [name])\n",
        "plt.show()\n",
        "\n",
        "Q1 = train.describe().loc['25%',name]\n",
        "Q3 = train.describe().loc['75%',name]\n",
        "minimum = train.describe().loc['min',name]\n",
        "maximum = train.describe().loc['max',name]\n",
        "IQR = Q3-Q1\n",
        "\n",
        "print(maximum - Q3 + (1.5*IQR))\n",
        "print(Q1 - (1.5*IQR) -  minimum)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEPCAYAAABY9lNGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSapkQEiSKv1/86kb8I35pz8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEPCAYAAAC6Kkg/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nb+/ar53IAgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "253.7626148220352\n",
            "475.60843595216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejhKDF9up7Gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######################################################################################################\n",
        "## petroMag_g 이상치 하나 더 없애기\n",
        "name = 'petroMag_g'\n",
        "drop_index = np.argmax(np.array(train[name]))\n",
        "train = train.drop(index=drop_index,axis=0)\n",
        "######################################################################################################\n",
        "\n",
        "train_type_num = train['type_num']\n",
        "needscaling_train = train.drop(['type','fiberID','type_num'],axis=1)\n",
        "needscaling_test = test.drop(['fiberID'], axis=1)\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(needscaling_train)\n",
        "scaled_train = pd.DataFrame(sc.transform(needscaling_train),\n",
        "                      columns=needscaling_train.columns,\n",
        "                      index = needscaling_train.index)\n",
        "\n",
        "scaled_test = pd.DataFrame(sc.transform(needscaling_test),\n",
        "                      columns=needscaling_test.columns,\n",
        "                      index = needscaling_test.index)\n",
        "\n",
        "scaled_train['type_num'] = train_type_num\n",
        "train_x = scaled_train.drop(['type_num'],axis=1)\n",
        "train_y = scaled_train['type_num']\n",
        "test_x = scaled_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in4ouFUuFFLy",
        "colab_type": "text"
      },
      "source": [
        "## 딥러닝 모델 구축 시작 (Elu)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e37totm8FI28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#random seeds for stochastic parts of neural network \n",
        "np.random.seed(123)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(123)\n",
        "\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import  Activation,  Lambda, Flatten, LeakyReLU, ELU, Dense\n",
        "from keras.layers import Input, Concatenate, Reshape, Dropout, BatchNormalization\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.optimizers import Adam, SGD,RMSprop\n",
        "from keras import  backend as K\n",
        "from keras import metrics\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
        "#from swa.keras import SWA # swa optimizer - https://pypi.org/project/keras-swa/\n",
        "import tensorflow as tf\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "\n",
        "################ Gelu #############\n",
        "\n",
        "class Gelu(Activation):\n",
        "    def __init__(self, activation, **kwargs):\n",
        "        super(Gelu, self).__init__(activation, **kwargs)\n",
        "        self.__name__='gelu'\n",
        "        \n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
        "\n",
        "get_custom_objects().update({'gelu': Activation(gelu)})\n",
        "\n",
        "################# tanh 활용 ( 범위 늘림 ) ##############\n",
        "def custom_activation(x):\n",
        "  return (K.tanh(x) * 145) + 155\n",
        "\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "get_custom_objects().update({'custom_activation': Activation(custom_activation)})\n",
        "\n",
        "def create_nn_model():\n",
        "    inp = Input(shape=(20,))\n",
        "    x = Dense(2373)(inp)\n",
        "    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.4)(x)\n",
        "    \n",
        "    x = Dense(2355)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.4)(x)\n",
        "\n",
        "    x = Dense(1197)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.35)(x)\n",
        "\n",
        "    x = Dense(1187)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.3)(x)\n",
        "\n",
        "    x = Dense(612)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.3)(x)\n",
        "\n",
        "    x = Dense(602)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.25)(x)\n",
        "\n",
        "\n",
        "    out = Dense(19, activation='softmax')(x) #scalar_coupling_constant    \n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    return model\n",
        "\n",
        "nn_model = create_nn_model(  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IVJlTVTKYT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_folder = '0218_이상치한개더제거_GELU_RedLR_DO_0.4'\n",
        "rate = ''\n",
        "\n",
        "import os\n",
        "SAVEMODEL_NEWFOLDER0 = 'drive/My Drive/데이콘_천체유형/ModelCheck/' + csv_folder\n",
        "SAVEMODEL_NEWFOLDER1 = 'drive/My Drive/데이콘_천체유형/기존모델저장/' + csv_folder\n",
        "if not os.path.exists(SAVEMODEL_NEWFOLDER1):\n",
        "  os.mkdir(SAVEMODEL_NEWFOLDER1)\n",
        "  print('모델폴더를 새로 생성했습니다.')\n",
        "SUBMISSION_NEWFOLDER = 'drive/My Drive/데이콘_천체유형/파일제출/' + csv_folder\n",
        "if not os.path.exists(SUBMISSION_NEWFOLDER):\n",
        "  os.mkdir(SUBMISSION_NEWFOLDER)\n",
        "  print('제출폴더를 새로 생성했습니다.')\n",
        "\n",
        "################## StratifiedShuffleSplit 를 이용해서 층화분할 #############\n",
        "X_array = train_x.values\n",
        "y_array = train_y.values\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=2, test_size=1/13, random_state=123)\n",
        "index1, index2 = sss.split(X_array, y_array)\n",
        "train_index = index1[0].tolist()\n",
        "val_index = index1[1].tolist()\n",
        "train_input = X_array[train_index]\n",
        "cv_input = X_array[val_index]\n",
        "train_target  = y_array[train_index]\n",
        "cv_target = y_array[val_index]\n",
        "\n",
        "\n",
        "\n",
        "#train_index, val_index = train_test_split(np.arange(len(train_y)),random_state=42, test_size=1/13)\n",
        "#train_input=train_x.iloc[train_index].values\n",
        "#train_target0=train_y[train_index]\n",
        "#train_target=train_target0.values\n",
        "#cv_input=train_x.iloc[val_index].values\n",
        "#cv_target0=train_y[val_index]\n",
        "#cv_target = cv_target0.values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFJe8RqARKMq",
        "colab_type": "text"
      },
      "source": [
        "# Reduce LR on Plateau\n",
        "\n",
        "\n",
        "\n",
        "> patience를 지정해놓고 metric(일반적으로 val_loss)를 보면서 줄어들지 않을 때에 (손실감소곡선이 너무 평탄할때) 일정상수를 initial_learning_rate에 곱해 local minima를 탈출하는 방법\n",
        "\n",
        "# 스케쥴러\n",
        "\n",
        "\n",
        "\n",
        "> 스케쥴러도 위의 Reduce LR과 굉장히 유사함. 하지만 스케쥴러는 위와 달리  (Linear, Cosine) 등 여러가지 종류의 스케쥴러로 **patience를 지정해서 쓰는게 아니라** 일정 패턴을 따라 큰 값에서 작은 값을 왔다갔다하며 그 사이에서 학습되도록 함.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oy650Oa7Fv9G"
      },
      "source": [
        "## initial_rate = 1e-4 (과적합 심하고 Gradient Exploding 문제도 있음)\n",
        "\n",
        "### 1. lr 줄여서 local minima 문제 없애고\n",
        "### 2. Drop Out 비율 더 높게 해주고\n",
        "### 3. Gradient Exploding 문제 해결을 하고 싶다면... clip value 지정해줘야 할 듯 (얘는 아직 적용 안됨)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fea67fb1-33c4-49e8-a4a8-1bfe49e3c252",
        "id": "tDPK5E_dFv9R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epoch =  800\n",
        "pat =  80\n",
        "red_pat =  25\n",
        "batchsize = 256*4\n",
        "initial_rate = 1e-4\n",
        "factor = 1/np.sqrt(10) # red_patience 만큼 기다리다가, 학습률을 *factor 배로 줄여버림 \n",
        "minimumlr = 1e-7\n",
        "\n",
        "import os\n",
        "MODEL_SAVE_FOLDER_PATH0 = SAVEMODEL_NEWFOLDER0 +  '/initial_rate=%s/' % initial_rate ## checkpoint\n",
        "MODEL_SAVE_FOLDER_PATH1 = SAVEMODEL_NEWFOLDER1 +  '/initial_rate=%s/' % initial_rate ## 기존모델저장\n",
        "if not os.path.exists(MODEL_SAVE_FOLDER_PATH1):\n",
        "  os.mkdir(MODEL_SAVE_FOLDER_PATH1)\n",
        "check_path = MODEL_SAVE_FOLDER_PATH0 + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
        "json_path = MODEL_SAVE_FOLDER_PATH1 + \"model1.json\"\n",
        "weight_path = MODEL_SAVE_FOLDER_PATH1 +\"model1.h5\"\n",
        "\n",
        "### model early stopping : 더이상 성능 개선이 되지 않으면 멈춤\n",
        "es = EarlyStopping(monitor= 'val_loss', patience = pat, verbose = 1, mode='min',\n",
        "                    restore_best_weights = True\n",
        "                   )\n",
        "\n",
        "### model check point\n",
        "mc = ModelCheckpoint(filepath=check_path, monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "## ReduceLR on Plateau : val_loss가 안 줄어들 때 lr을 작게 할 수 있음 (local minima 대처방법)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor = factor,   # patience 만큼 기다리다가 0.1이면 학습률을 0.1배로 줄여버림 \n",
        "                        patience = red_pat, mode = 'min', verbose = 1,\n",
        "                        min_lr = minimumlr\n",
        "                        )\n",
        "\n",
        "from keras import optimizers\n",
        "optimizer = optimizers.Adam(\n",
        "    lr=initial_rate,\n",
        ")\n",
        "\n",
        "## compile model\n",
        "CCE = metrics.sparse_categorical_crossentropy\n",
        "\n",
        "nn_model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              #metrics=['accuracy']\n",
        "              #metrics=[metrics.sparse_categorical_accuracy]\n",
        "              metrics=[CCE]\n",
        "              )\n",
        "\n",
        "## fitting model\n",
        "hist = nn_model.fit(  train_input, train_target,validation_data=[cv_input, cv_target],\n",
        "                    batch_size=batchsize,\n",
        "                    epochs=epoch,\n",
        "                    callbacks = [es \n",
        "                                 #,mc\n",
        "                                 ,rlr\n",
        "                                ] )\n",
        "\n",
        "## save model\n",
        "model_json = nn_model.to_json()\n",
        "with open(json_path, \"w\") as json_file : \n",
        "    json_file.write(model_json)\n",
        "## model weight save\n",
        "nn_model.save_weights(weight_path)\n",
        "print(\"모델저장완료\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"Loss와 ACC에 대한 Plot을 그립니다\")\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "loss_ax.legend(loc='lower left')\n",
        "\n",
        "#acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
        "#acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        "#acc_ax.set_ylabel('accuracy')\n",
        "#acc_ax.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## evaluate model\n",
        "print('기존nn모델의 train loss를 출력합니다')\n",
        "loss_and_metric = nn_model.evaluate(train_input, train_target, batch_size=batchsize, verbose=0)\n",
        "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
        "## evaluate model\n",
        "print('기존nn모델의 valid loss를 출력합니다')\n",
        "loss_and_metric = nn_model.evaluate(cv_input, cv_target, batch_size=batchsize, verbose=0)\n",
        "print(\"valid, loss and metric: {}\".format(loss_and_metric))\n",
        "## model weight save ## 기존 모델의 가중치 저장\n",
        "#nn_model.save_weights(weight_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 184521 samples, validate on 15377 samples\n",
            "Epoch 1/800\n",
            "184521/184521 [==============================] - 11s 59us/step - loss: 0.9203 - sparse_categorical_crossentropy: 0.9203 - val_loss: 0.6011 - val_sparse_categorical_crossentropy: 0.6011\n",
            "Epoch 2/800\n",
            "184521/184521 [==============================] - 8s 42us/step - loss: 0.5827 - sparse_categorical_crossentropy: 0.5827 - val_loss: 0.5240 - val_sparse_categorical_crossentropy: 0.5240\n",
            "Epoch 3/800\n",
            "184521/184521 [==============================] - 8s 43us/step - loss: 0.5380 - sparse_categorical_crossentropy: 0.5380 - val_loss: 0.4968 - val_sparse_categorical_crossentropy: 0.4968\n",
            "Epoch 4/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.5137 - sparse_categorical_crossentropy: 0.5137 - val_loss: 0.4764 - val_sparse_categorical_crossentropy: 0.4764\n",
            "Epoch 5/800\n",
            "184521/184521 [==============================] - 8s 43us/step - loss: 0.4974 - sparse_categorical_crossentropy: 0.4974 - val_loss: 0.4892 - val_sparse_categorical_crossentropy: 0.4892\n",
            "Epoch 6/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4848 - sparse_categorical_crossentropy: 0.4848 - val_loss: 0.4595 - val_sparse_categorical_crossentropy: 0.4595\n",
            "Epoch 7/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4743 - sparse_categorical_crossentropy: 0.4743 - val_loss: 0.4459 - val_sparse_categorical_crossentropy: 0.4459\n",
            "Epoch 8/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4681 - sparse_categorical_crossentropy: 0.4681 - val_loss: 0.4426 - val_sparse_categorical_crossentropy: 0.4426\n",
            "Epoch 9/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4644 - sparse_categorical_crossentropy: 0.4644 - val_loss: 0.4318 - val_sparse_categorical_crossentropy: 0.4318\n",
            "Epoch 10/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4564 - sparse_categorical_crossentropy: 0.4564 - val_loss: 0.4289 - val_sparse_categorical_crossentropy: 0.4289\n",
            "Epoch 11/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4504 - sparse_categorical_crossentropy: 0.4504 - val_loss: 0.4235 - val_sparse_categorical_crossentropy: 0.4235\n",
            "Epoch 12/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4468 - sparse_categorical_crossentropy: 0.4468 - val_loss: 0.4264 - val_sparse_categorical_crossentropy: 0.4264\n",
            "Epoch 13/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4422 - sparse_categorical_crossentropy: 0.4422 - val_loss: 0.4252 - val_sparse_categorical_crossentropy: 0.4252\n",
            "Epoch 14/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4395 - sparse_categorical_crossentropy: 0.4395 - val_loss: 0.4157 - val_sparse_categorical_crossentropy: 0.4157\n",
            "Epoch 15/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4361 - sparse_categorical_crossentropy: 0.4361 - val_loss: 0.4191 - val_sparse_categorical_crossentropy: 0.4191\n",
            "Epoch 16/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4345 - sparse_categorical_crossentropy: 0.4345 - val_loss: 0.4225 - val_sparse_categorical_crossentropy: 0.4225\n",
            "Epoch 17/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4319 - sparse_categorical_crossentropy: 0.4319 - val_loss: 0.4051 - val_sparse_categorical_crossentropy: 0.4051\n",
            "Epoch 18/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4272 - sparse_categorical_crossentropy: 0.4272 - val_loss: 0.4094 - val_sparse_categorical_crossentropy: 0.4094\n",
            "Epoch 19/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4250 - sparse_categorical_crossentropy: 0.4250 - val_loss: 0.4089 - val_sparse_categorical_crossentropy: 0.4089\n",
            "Epoch 20/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4225 - sparse_categorical_crossentropy: 0.4225 - val_loss: 0.4006 - val_sparse_categorical_crossentropy: 0.4006\n",
            "Epoch 21/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4224 - sparse_categorical_crossentropy: 0.4224 - val_loss: 0.4014 - val_sparse_categorical_crossentropy: 0.4014\n",
            "Epoch 22/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4200 - sparse_categorical_crossentropy: 0.4200 - val_loss: 0.4054 - val_sparse_categorical_crossentropy: 0.4054\n",
            "Epoch 23/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4191 - sparse_categorical_crossentropy: 0.4191 - val_loss: 0.3968 - val_sparse_categorical_crossentropy: 0.3968\n",
            "Epoch 24/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4172 - sparse_categorical_crossentropy: 0.4172 - val_loss: 0.3950 - val_sparse_categorical_crossentropy: 0.3950\n",
            "Epoch 25/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4150 - sparse_categorical_crossentropy: 0.4150 - val_loss: 0.3930 - val_sparse_categorical_crossentropy: 0.3930\n",
            "Epoch 26/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4126 - sparse_categorical_crossentropy: 0.4126 - val_loss: 0.3934 - val_sparse_categorical_crossentropy: 0.3934\n",
            "Epoch 27/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4111 - sparse_categorical_crossentropy: 0.4111 - val_loss: 0.3911 - val_sparse_categorical_crossentropy: 0.3911\n",
            "Epoch 28/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4122 - sparse_categorical_crossentropy: 0.4122 - val_loss: 0.3905 - val_sparse_categorical_crossentropy: 0.3905\n",
            "Epoch 29/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4093 - sparse_categorical_crossentropy: 0.4093 - val_loss: 0.3966 - val_sparse_categorical_crossentropy: 0.3966\n",
            "Epoch 30/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4073 - sparse_categorical_crossentropy: 0.4073 - val_loss: 0.3885 - val_sparse_categorical_crossentropy: 0.3885\n",
            "Epoch 31/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4080 - sparse_categorical_crossentropy: 0.4080 - val_loss: 0.3861 - val_sparse_categorical_crossentropy: 0.3861\n",
            "Epoch 32/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4062 - sparse_categorical_crossentropy: 0.4062 - val_loss: 0.3905 - val_sparse_categorical_crossentropy: 0.3905\n",
            "Epoch 33/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4030 - sparse_categorical_crossentropy: 0.4030 - val_loss: 0.3856 - val_sparse_categorical_crossentropy: 0.3856\n",
            "Epoch 34/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4034 - sparse_categorical_crossentropy: 0.4034 - val_loss: 0.3864 - val_sparse_categorical_crossentropy: 0.3864\n",
            "Epoch 35/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4016 - sparse_categorical_crossentropy: 0.4016 - val_loss: 0.3861 - val_sparse_categorical_crossentropy: 0.3861\n",
            "Epoch 36/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4025 - sparse_categorical_crossentropy: 0.4025 - val_loss: 0.3891 - val_sparse_categorical_crossentropy: 0.3891\n",
            "Epoch 37/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3990 - sparse_categorical_crossentropy: 0.3990 - val_loss: 0.3867 - val_sparse_categorical_crossentropy: 0.3867\n",
            "Epoch 38/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3996 - sparse_categorical_crossentropy: 0.3996 - val_loss: 0.3865 - val_sparse_categorical_crossentropy: 0.3865\n",
            "Epoch 39/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3989 - sparse_categorical_crossentropy: 0.3989 - val_loss: 0.4002 - val_sparse_categorical_crossentropy: 0.4002\n",
            "Epoch 40/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3983 - sparse_categorical_crossentropy: 0.3983 - val_loss: 0.3838 - val_sparse_categorical_crossentropy: 0.3838\n",
            "Epoch 41/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3965 - sparse_categorical_crossentropy: 0.3965 - val_loss: 0.3823 - val_sparse_categorical_crossentropy: 0.3823\n",
            "Epoch 42/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3963 - sparse_categorical_crossentropy: 0.3963 - val_loss: 0.3834 - val_sparse_categorical_crossentropy: 0.3834\n",
            "Epoch 43/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3958 - sparse_categorical_crossentropy: 0.3958 - val_loss: 0.3840 - val_sparse_categorical_crossentropy: 0.3840\n",
            "Epoch 44/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3966 - sparse_categorical_crossentropy: 0.3966 - val_loss: 0.3803 - val_sparse_categorical_crossentropy: 0.3803\n",
            "Epoch 45/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3950 - sparse_categorical_crossentropy: 0.3950 - val_loss: 0.3797 - val_sparse_categorical_crossentropy: 0.3797\n",
            "Epoch 46/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3925 - sparse_categorical_crossentropy: 0.3925 - val_loss: 0.3787 - val_sparse_categorical_crossentropy: 0.3787\n",
            "Epoch 47/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3940 - sparse_categorical_crossentropy: 0.3940 - val_loss: 0.3777 - val_sparse_categorical_crossentropy: 0.3777\n",
            "Epoch 48/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3912 - sparse_categorical_crossentropy: 0.3912 - val_loss: 0.3896 - val_sparse_categorical_crossentropy: 0.3896\n",
            "Epoch 49/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3920 - sparse_categorical_crossentropy: 0.3920 - val_loss: 0.3832 - val_sparse_categorical_crossentropy: 0.3832\n",
            "Epoch 50/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3916 - sparse_categorical_crossentropy: 0.3916 - val_loss: 0.3814 - val_sparse_categorical_crossentropy: 0.3814\n",
            "Epoch 51/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3904 - sparse_categorical_crossentropy: 0.3904 - val_loss: 0.3923 - val_sparse_categorical_crossentropy: 0.3923\n",
            "Epoch 52/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3902 - sparse_categorical_crossentropy: 0.3902 - val_loss: 0.3807 - val_sparse_categorical_crossentropy: 0.3807\n",
            "Epoch 53/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3898 - sparse_categorical_crossentropy: 0.3898 - val_loss: 0.3780 - val_sparse_categorical_crossentropy: 0.3780\n",
            "Epoch 54/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3888 - sparse_categorical_crossentropy: 0.3888 - val_loss: 0.3921 - val_sparse_categorical_crossentropy: 0.3921\n",
            "Epoch 55/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3894 - sparse_categorical_crossentropy: 0.3894 - val_loss: 0.3806 - val_sparse_categorical_crossentropy: 0.3806\n",
            "Epoch 56/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3867 - sparse_categorical_crossentropy: 0.3867 - val_loss: 0.3764 - val_sparse_categorical_crossentropy: 0.3764\n",
            "Epoch 57/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3867 - sparse_categorical_crossentropy: 0.3867 - val_loss: 0.3771 - val_sparse_categorical_crossentropy: 0.3771\n",
            "Epoch 58/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3875 - sparse_categorical_crossentropy: 0.3875 - val_loss: 0.3739 - val_sparse_categorical_crossentropy: 0.3739\n",
            "Epoch 59/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3861 - sparse_categorical_crossentropy: 0.3861 - val_loss: 0.3725 - val_sparse_categorical_crossentropy: 0.3725\n",
            "Epoch 60/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3854 - sparse_categorical_crossentropy: 0.3854 - val_loss: 0.3889 - val_sparse_categorical_crossentropy: 0.3889\n",
            "Epoch 61/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3854 - sparse_categorical_crossentropy: 0.3854 - val_loss: 0.3830 - val_sparse_categorical_crossentropy: 0.3830\n",
            "Epoch 62/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3854 - sparse_categorical_crossentropy: 0.3854 - val_loss: 0.3863 - val_sparse_categorical_crossentropy: 0.3863\n",
            "Epoch 63/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3824 - sparse_categorical_crossentropy: 0.3824 - val_loss: 0.3774 - val_sparse_categorical_crossentropy: 0.3774\n",
            "Epoch 64/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3848 - sparse_categorical_crossentropy: 0.3848 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 65/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3834 - sparse_categorical_crossentropy: 0.3834 - val_loss: 0.3735 - val_sparse_categorical_crossentropy: 0.3735\n",
            "Epoch 66/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3845 - sparse_categorical_crossentropy: 0.3845 - val_loss: 0.3721 - val_sparse_categorical_crossentropy: 0.3721\n",
            "Epoch 67/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3833 - sparse_categorical_crossentropy: 0.3833 - val_loss: 0.3775 - val_sparse_categorical_crossentropy: 0.3775\n",
            "Epoch 68/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3813 - sparse_categorical_crossentropy: 0.3813 - val_loss: 0.3735 - val_sparse_categorical_crossentropy: 0.3735\n",
            "Epoch 69/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3828 - sparse_categorical_crossentropy: 0.3828 - val_loss: 0.3753 - val_sparse_categorical_crossentropy: 0.3753\n",
            "Epoch 70/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3816 - sparse_categorical_crossentropy: 0.3816 - val_loss: 0.3726 - val_sparse_categorical_crossentropy: 0.3726\n",
            "Epoch 71/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3803 - sparse_categorical_crossentropy: 0.3803 - val_loss: 0.3865 - val_sparse_categorical_crossentropy: 0.3865\n",
            "Epoch 72/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3811 - sparse_categorical_crossentropy: 0.3811 - val_loss: 0.3777 - val_sparse_categorical_crossentropy: 0.3777\n",
            "Epoch 73/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3793 - sparse_categorical_crossentropy: 0.3793 - val_loss: 0.3732 - val_sparse_categorical_crossentropy: 0.3732\n",
            "Epoch 74/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3791 - sparse_categorical_crossentropy: 0.3791 - val_loss: 0.3737 - val_sparse_categorical_crossentropy: 0.3737\n",
            "Epoch 75/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3801 - sparse_categorical_crossentropy: 0.3801 - val_loss: 0.3732 - val_sparse_categorical_crossentropy: 0.3732\n",
            "Epoch 76/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3792 - sparse_categorical_crossentropy: 0.3792 - val_loss: 0.3760 - val_sparse_categorical_crossentropy: 0.3760\n",
            "Epoch 77/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3784 - sparse_categorical_crossentropy: 0.3784 - val_loss: 0.3758 - val_sparse_categorical_crossentropy: 0.3758\n",
            "Epoch 78/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3766 - sparse_categorical_crossentropy: 0.3766 - val_loss: 0.3785 - val_sparse_categorical_crossentropy: 0.3785\n",
            "Epoch 79/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3798 - sparse_categorical_crossentropy: 0.3798 - val_loss: 0.3789 - val_sparse_categorical_crossentropy: 0.3789\n",
            "Epoch 80/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3756 - sparse_categorical_crossentropy: 0.3756 - val_loss: 0.3767 - val_sparse_categorical_crossentropy: 0.3767\n",
            "Epoch 81/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3751 - sparse_categorical_crossentropy: 0.3751 - val_loss: 0.3723 - val_sparse_categorical_crossentropy: 0.3723\n",
            "Epoch 82/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3761 - sparse_categorical_crossentropy: 0.3761 - val_loss: 0.3745 - val_sparse_categorical_crossentropy: 0.3745\n",
            "Epoch 83/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3775 - sparse_categorical_crossentropy: 0.3775 - val_loss: 0.3735 - val_sparse_categorical_crossentropy: 0.3735\n",
            "Epoch 84/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3762 - sparse_categorical_crossentropy: 0.3762 - val_loss: 0.3696 - val_sparse_categorical_crossentropy: 0.3696\n",
            "Epoch 85/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3762 - sparse_categorical_crossentropy: 0.3762 - val_loss: 0.3703 - val_sparse_categorical_crossentropy: 0.3703\n",
            "Epoch 86/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3751 - sparse_categorical_crossentropy: 0.3751 - val_loss: 0.3764 - val_sparse_categorical_crossentropy: 0.3764\n",
            "Epoch 87/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3753 - sparse_categorical_crossentropy: 0.3753 - val_loss: 0.3699 - val_sparse_categorical_crossentropy: 0.3699\n",
            "Epoch 88/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3745 - sparse_categorical_crossentropy: 0.3745 - val_loss: 0.3693 - val_sparse_categorical_crossentropy: 0.3693\n",
            "Epoch 89/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3740 - sparse_categorical_crossentropy: 0.3740 - val_loss: 0.3738 - val_sparse_categorical_crossentropy: 0.3738\n",
            "Epoch 90/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3750 - sparse_categorical_crossentropy: 0.3750 - val_loss: 0.3745 - val_sparse_categorical_crossentropy: 0.3745\n",
            "Epoch 91/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3723 - sparse_categorical_crossentropy: 0.3723 - val_loss: 0.3702 - val_sparse_categorical_crossentropy: 0.3702\n",
            "Epoch 92/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3743 - sparse_categorical_crossentropy: 0.3743 - val_loss: 0.3708 - val_sparse_categorical_crossentropy: 0.3708\n",
            "Epoch 93/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3728 - sparse_categorical_crossentropy: 0.3728 - val_loss: 0.3699 - val_sparse_categorical_crossentropy: 0.3699\n",
            "Epoch 94/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3728 - sparse_categorical_crossentropy: 0.3728 - val_loss: 0.3720 - val_sparse_categorical_crossentropy: 0.3720\n",
            "Epoch 95/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3732 - sparse_categorical_crossentropy: 0.3732 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "Epoch 96/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3714 - sparse_categorical_crossentropy: 0.3714 - val_loss: 0.3681 - val_sparse_categorical_crossentropy: 0.3681\n",
            "Epoch 97/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3745 - sparse_categorical_crossentropy: 0.3745 - val_loss: 0.3725 - val_sparse_categorical_crossentropy: 0.3725\n",
            "Epoch 98/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3716 - sparse_categorical_crossentropy: 0.3716 - val_loss: 0.3687 - val_sparse_categorical_crossentropy: 0.3687\n",
            "Epoch 99/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3721 - sparse_categorical_crossentropy: 0.3721 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 100/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3712 - sparse_categorical_crossentropy: 0.3712 - val_loss: 0.3726 - val_sparse_categorical_crossentropy: 0.3726\n",
            "Epoch 101/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3701 - sparse_categorical_crossentropy: 0.3701 - val_loss: 0.3760 - val_sparse_categorical_crossentropy: 0.3760\n",
            "Epoch 102/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3708 - sparse_categorical_crossentropy: 0.3708 - val_loss: 0.3726 - val_sparse_categorical_crossentropy: 0.3726\n",
            "Epoch 103/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3701 - sparse_categorical_crossentropy: 0.3701 - val_loss: 0.3735 - val_sparse_categorical_crossentropy: 0.3735\n",
            "Epoch 104/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3700 - sparse_categorical_crossentropy: 0.3700 - val_loss: 0.3684 - val_sparse_categorical_crossentropy: 0.3684\n",
            "Epoch 105/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3679 - sparse_categorical_crossentropy: 0.3679 - val_loss: 0.3687 - val_sparse_categorical_crossentropy: 0.3687\n",
            "Epoch 106/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3685 - sparse_categorical_crossentropy: 0.3685 - val_loss: 0.3693 - val_sparse_categorical_crossentropy: 0.3693\n",
            "Epoch 107/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3680 - sparse_categorical_crossentropy: 0.3680 - val_loss: 0.3721 - val_sparse_categorical_crossentropy: 0.3721\n",
            "Epoch 108/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3684 - sparse_categorical_crossentropy: 0.3684 - val_loss: 0.3686 - val_sparse_categorical_crossentropy: 0.3686\n",
            "Epoch 109/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3686 - sparse_categorical_crossentropy: 0.3686 - val_loss: 0.3706 - val_sparse_categorical_crossentropy: 0.3706\n",
            "Epoch 110/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3683 - sparse_categorical_crossentropy: 0.3683 - val_loss: 0.3662 - val_sparse_categorical_crossentropy: 0.3662\n",
            "Epoch 111/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3684 - sparse_categorical_crossentropy: 0.3684 - val_loss: 0.3688 - val_sparse_categorical_crossentropy: 0.3688\n",
            "Epoch 112/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3684 - sparse_categorical_crossentropy: 0.3684 - val_loss: 0.3754 - val_sparse_categorical_crossentropy: 0.3754\n",
            "Epoch 113/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3661 - sparse_categorical_crossentropy: 0.3661 - val_loss: 0.3683 - val_sparse_categorical_crossentropy: 0.3683\n",
            "Epoch 114/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3665 - sparse_categorical_crossentropy: 0.3665 - val_loss: 0.3680 - val_sparse_categorical_crossentropy: 0.3680\n",
            "Epoch 115/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3669 - sparse_categorical_crossentropy: 0.3669 - val_loss: 0.3701 - val_sparse_categorical_crossentropy: 0.3701\n",
            "Epoch 116/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3669 - sparse_categorical_crossentropy: 0.3669 - val_loss: 0.3699 - val_sparse_categorical_crossentropy: 0.3699\n",
            "Epoch 117/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3675 - sparse_categorical_crossentropy: 0.3675 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 118/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.3653 - sparse_categorical_crossentropy: 0.3653 - val_loss: 0.3666 - val_sparse_categorical_crossentropy: 0.3666\n",
            "Epoch 119/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3668 - sparse_categorical_crossentropy: 0.3668 - val_loss: 0.3687 - val_sparse_categorical_crossentropy: 0.3687\n",
            "Epoch 120/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3650 - sparse_categorical_crossentropy: 0.3650 - val_loss: 0.3697 - val_sparse_categorical_crossentropy: 0.3697\n",
            "Epoch 121/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3655 - sparse_categorical_crossentropy: 0.3655 - val_loss: 0.3686 - val_sparse_categorical_crossentropy: 0.3686\n",
            "Epoch 122/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3658 - sparse_categorical_crossentropy: 0.3658 - val_loss: 0.3675 - val_sparse_categorical_crossentropy: 0.3675\n",
            "Epoch 123/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3656 - sparse_categorical_crossentropy: 0.3656 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 124/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3657 - sparse_categorical_crossentropy: 0.3657 - val_loss: 0.3720 - val_sparse_categorical_crossentropy: 0.3720\n",
            "Epoch 125/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3638 - sparse_categorical_crossentropy: 0.3638 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "Epoch 126/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3639 - sparse_categorical_crossentropy: 0.3639 - val_loss: 0.3670 - val_sparse_categorical_crossentropy: 0.3670\n",
            "Epoch 127/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3653 - sparse_categorical_crossentropy: 0.3653 - val_loss: 0.3702 - val_sparse_categorical_crossentropy: 0.3702\n",
            "Epoch 128/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3647 - sparse_categorical_crossentropy: 0.3647 - val_loss: 0.3703 - val_sparse_categorical_crossentropy: 0.3703\n",
            "Epoch 129/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3628 - sparse_categorical_crossentropy: 0.3628 - val_loss: 0.3696 - val_sparse_categorical_crossentropy: 0.3696\n",
            "Epoch 130/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3651 - sparse_categorical_crossentropy: 0.3651 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 131/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3621 - sparse_categorical_crossentropy: 0.3621 - val_loss: 0.3746 - val_sparse_categorical_crossentropy: 0.3746\n",
            "Epoch 132/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3641 - sparse_categorical_crossentropy: 0.3641 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 133/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3622 - sparse_categorical_crossentropy: 0.3622 - val_loss: 0.3727 - val_sparse_categorical_crossentropy: 0.3727\n",
            "Epoch 134/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3635 - sparse_categorical_crossentropy: 0.3635 - val_loss: 0.3690 - val_sparse_categorical_crossentropy: 0.3690\n",
            "Epoch 135/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3632 - sparse_categorical_crossentropy: 0.3632 - val_loss: 0.3710 - val_sparse_categorical_crossentropy: 0.3710\n",
            "\n",
            "Epoch 00135: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.\n",
            "Epoch 136/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3585 - sparse_categorical_crossentropy: 0.3585 - val_loss: 0.3650 - val_sparse_categorical_crossentropy: 0.3650\n",
            "Epoch 137/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3567 - sparse_categorical_crossentropy: 0.3567 - val_loss: 0.3670 - val_sparse_categorical_crossentropy: 0.3670\n",
            "Epoch 138/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3584 - sparse_categorical_crossentropy: 0.3584 - val_loss: 0.3678 - val_sparse_categorical_crossentropy: 0.3678\n",
            "Epoch 139/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3556 - sparse_categorical_crossentropy: 0.3556 - val_loss: 0.3669 - val_sparse_categorical_crossentropy: 0.3669\n",
            "Epoch 140/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3566 - sparse_categorical_crossentropy: 0.3566 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 141/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3567 - sparse_categorical_crossentropy: 0.3567 - val_loss: 0.3643 - val_sparse_categorical_crossentropy: 0.3643\n",
            "Epoch 142/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3549 - sparse_categorical_crossentropy: 0.3549 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 143/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3563 - sparse_categorical_crossentropy: 0.3563 - val_loss: 0.3647 - val_sparse_categorical_crossentropy: 0.3647\n",
            "Epoch 144/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3551 - sparse_categorical_crossentropy: 0.3551 - val_loss: 0.3693 - val_sparse_categorical_crossentropy: 0.3693\n",
            "Epoch 145/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3556 - sparse_categorical_crossentropy: 0.3556 - val_loss: 0.3661 - val_sparse_categorical_crossentropy: 0.3661\n",
            "Epoch 146/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3549 - sparse_categorical_crossentropy: 0.3549 - val_loss: 0.3673 - val_sparse_categorical_crossentropy: 0.3673\n",
            "Epoch 147/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3538 - sparse_categorical_crossentropy: 0.3538 - val_loss: 0.3673 - val_sparse_categorical_crossentropy: 0.3673\n",
            "Epoch 148/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3544 - sparse_categorical_crossentropy: 0.3544 - val_loss: 0.3644 - val_sparse_categorical_crossentropy: 0.3644\n",
            "Epoch 149/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3550 - sparse_categorical_crossentropy: 0.3550 - val_loss: 0.3649 - val_sparse_categorical_crossentropy: 0.3649\n",
            "Epoch 150/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3545 - sparse_categorical_crossentropy: 0.3545 - val_loss: 0.3650 - val_sparse_categorical_crossentropy: 0.3650\n",
            "Epoch 151/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3542 - sparse_categorical_crossentropy: 0.3542 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 152/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3550 - sparse_categorical_crossentropy: 0.3550 - val_loss: 0.3663 - val_sparse_categorical_crossentropy: 0.3663\n",
            "Epoch 153/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3539 - sparse_categorical_crossentropy: 0.3539 - val_loss: 0.3662 - val_sparse_categorical_crossentropy: 0.3662\n",
            "Epoch 154/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3537 - sparse_categorical_crossentropy: 0.3537 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 155/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3525 - sparse_categorical_crossentropy: 0.3525 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 156/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3536 - sparse_categorical_crossentropy: 0.3536 - val_loss: 0.3671 - val_sparse_categorical_crossentropy: 0.3671\n",
            "Epoch 157/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3542 - sparse_categorical_crossentropy: 0.3542 - val_loss: 0.3649 - val_sparse_categorical_crossentropy: 0.3649\n",
            "Epoch 158/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3533 - sparse_categorical_crossentropy: 0.3533 - val_loss: 0.3693 - val_sparse_categorical_crossentropy: 0.3693\n",
            "Epoch 159/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3535 - sparse_categorical_crossentropy: 0.3535 - val_loss: 0.3653 - val_sparse_categorical_crossentropy: 0.3653\n",
            "Epoch 160/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3525 - sparse_categorical_crossentropy: 0.3525 - val_loss: 0.3664 - val_sparse_categorical_crossentropy: 0.3664\n",
            "Epoch 161/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3540 - sparse_categorical_crossentropy: 0.3540 - val_loss: 0.3648 - val_sparse_categorical_crossentropy: 0.3648\n",
            "Epoch 162/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3531 - sparse_categorical_crossentropy: 0.3531 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 163/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3521 - sparse_categorical_crossentropy: 0.3521 - val_loss: 0.3662 - val_sparse_categorical_crossentropy: 0.3662\n",
            "Epoch 164/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3529 - sparse_categorical_crossentropy: 0.3529 - val_loss: 0.3671 - val_sparse_categorical_crossentropy: 0.3671\n",
            "Epoch 165/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3526 - sparse_categorical_crossentropy: 0.3526 - val_loss: 0.3662 - val_sparse_categorical_crossentropy: 0.3662\n",
            "Epoch 166/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3531 - sparse_categorical_crossentropy: 0.3531 - val_loss: 0.3673 - val_sparse_categorical_crossentropy: 0.3673\n",
            "\n",
            "Epoch 00166: ReduceLROnPlateau reducing learning rate to 9.999999259090306e-06.\n",
            "Epoch 167/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3508 - sparse_categorical_crossentropy: 0.3508 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 168/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3508 - sparse_categorical_crossentropy: 0.3508 - val_loss: 0.3646 - val_sparse_categorical_crossentropy: 0.3646\n",
            "Epoch 169/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3502 - sparse_categorical_crossentropy: 0.3502 - val_loss: 0.3650 - val_sparse_categorical_crossentropy: 0.3650\n",
            "Epoch 170/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3507 - sparse_categorical_crossentropy: 0.3507 - val_loss: 0.3665 - val_sparse_categorical_crossentropy: 0.3665\n",
            "Epoch 171/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3500 - sparse_categorical_crossentropy: 0.3500 - val_loss: 0.3650 - val_sparse_categorical_crossentropy: 0.3650\n",
            "Epoch 172/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3507 - sparse_categorical_crossentropy: 0.3507 - val_loss: 0.3652 - val_sparse_categorical_crossentropy: 0.3652\n",
            "Epoch 173/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3511 - sparse_categorical_crossentropy: 0.3511 - val_loss: 0.3649 - val_sparse_categorical_crossentropy: 0.3649\n",
            "Epoch 174/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3488 - sparse_categorical_crossentropy: 0.3488 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 175/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3500 - sparse_categorical_crossentropy: 0.3500 - val_loss: 0.3650 - val_sparse_categorical_crossentropy: 0.3650\n",
            "Epoch 176/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3486 - sparse_categorical_crossentropy: 0.3486 - val_loss: 0.3650 - val_sparse_categorical_crossentropy: 0.3650\n",
            "Epoch 177/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3496 - sparse_categorical_crossentropy: 0.3496 - val_loss: 0.3645 - val_sparse_categorical_crossentropy: 0.3645\n",
            "Epoch 178/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3510 - sparse_categorical_crossentropy: 0.3510 - val_loss: 0.3649 - val_sparse_categorical_crossentropy: 0.3649\n",
            "Epoch 179/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3500 - sparse_categorical_crossentropy: 0.3500 - val_loss: 0.3649 - val_sparse_categorical_crossentropy: 0.3649\n",
            "Epoch 180/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3510 - sparse_categorical_crossentropy: 0.3510 - val_loss: 0.3650 - val_sparse_categorical_crossentropy: 0.3650\n",
            "Epoch 181/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3501 - sparse_categorical_crossentropy: 0.3501 - val_loss: 0.3665 - val_sparse_categorical_crossentropy: 0.3665\n",
            "Epoch 182/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3501 - sparse_categorical_crossentropy: 0.3501 - val_loss: 0.3648 - val_sparse_categorical_crossentropy: 0.3648\n",
            "Epoch 183/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3494 - sparse_categorical_crossentropy: 0.3494 - val_loss: 0.3653 - val_sparse_categorical_crossentropy: 0.3653\n",
            "Epoch 184/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3494 - sparse_categorical_crossentropy: 0.3494 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 185/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3500 - sparse_categorical_crossentropy: 0.3500 - val_loss: 0.3671 - val_sparse_categorical_crossentropy: 0.3671\n",
            "Epoch 186/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3496 - sparse_categorical_crossentropy: 0.3496 - val_loss: 0.3651 - val_sparse_categorical_crossentropy: 0.3651\n",
            "Epoch 187/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3505 - sparse_categorical_crossentropy: 0.3505 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "Epoch 188/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3495 - sparse_categorical_crossentropy: 0.3495 - val_loss: 0.3651 - val_sparse_categorical_crossentropy: 0.3651\n",
            "Epoch 189/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3491 - sparse_categorical_crossentropy: 0.3491 - val_loss: 0.3647 - val_sparse_categorical_crossentropy: 0.3647\n",
            "Epoch 190/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3495 - sparse_categorical_crossentropy: 0.3495 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 191/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3490 - sparse_categorical_crossentropy: 0.3490 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "\n",
            "Epoch 00191: ReduceLROnPlateau reducing learning rate to 3.162277292675049e-06.\n",
            "Epoch 192/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3489 - sparse_categorical_crossentropy: 0.3489 - val_loss: 0.3648 - val_sparse_categorical_crossentropy: 0.3648\n",
            "Epoch 193/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3485 - sparse_categorical_crossentropy: 0.3485 - val_loss: 0.3646 - val_sparse_categorical_crossentropy: 0.3646\n",
            "Epoch 194/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3497 - sparse_categorical_crossentropy: 0.3497 - val_loss: 0.3646 - val_sparse_categorical_crossentropy: 0.3646\n",
            "Epoch 195/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3487 - sparse_categorical_crossentropy: 0.3487 - val_loss: 0.3648 - val_sparse_categorical_crossentropy: 0.3648\n",
            "Epoch 196/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3479 - sparse_categorical_crossentropy: 0.3479 - val_loss: 0.3649 - val_sparse_categorical_crossentropy: 0.3649\n",
            "Epoch 197/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3494 - sparse_categorical_crossentropy: 0.3494 - val_loss: 0.3649 - val_sparse_categorical_crossentropy: 0.3649\n",
            "Epoch 198/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3486 - sparse_categorical_crossentropy: 0.3486 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 199/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3491 - sparse_categorical_crossentropy: 0.3491 - val_loss: 0.3649 - val_sparse_categorical_crossentropy: 0.3649\n",
            "Epoch 200/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3475 - sparse_categorical_crossentropy: 0.3475 - val_loss: 0.3649 - val_sparse_categorical_crossentropy: 0.3649\n",
            "Epoch 201/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3486 - sparse_categorical_crossentropy: 0.3486 - val_loss: 0.3651 - val_sparse_categorical_crossentropy: 0.3651\n",
            "Epoch 202/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3490 - sparse_categorical_crossentropy: 0.3490 - val_loss: 0.3649 - val_sparse_categorical_crossentropy: 0.3649\n",
            "Epoch 203/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3499 - sparse_categorical_crossentropy: 0.3499 - val_loss: 0.3652 - val_sparse_categorical_crossentropy: 0.3652\n",
            "Epoch 204/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3482 - sparse_categorical_crossentropy: 0.3482 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 205/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3485 - sparse_categorical_crossentropy: 0.3485 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 206/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3488 - sparse_categorical_crossentropy: 0.3488 - val_loss: 0.3650 - val_sparse_categorical_crossentropy: 0.3650\n",
            "Epoch 207/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3486 - sparse_categorical_crossentropy: 0.3486 - val_loss: 0.3651 - val_sparse_categorical_crossentropy: 0.3651\n",
            "Epoch 208/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3491 - sparse_categorical_crossentropy: 0.3491 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 209/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3473 - sparse_categorical_crossentropy: 0.3473 - val_loss: 0.3650 - val_sparse_categorical_crossentropy: 0.3650\n",
            "Epoch 210/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3477 - sparse_categorical_crossentropy: 0.3477 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 211/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3475 - sparse_categorical_crossentropy: 0.3475 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 212/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3481 - sparse_categorical_crossentropy: 0.3481 - val_loss: 0.3650 - val_sparse_categorical_crossentropy: 0.3650\n",
            "Epoch 213/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3492 - sparse_categorical_crossentropy: 0.3492 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "Epoch 214/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3487 - sparse_categorical_crossentropy: 0.3487 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 215/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3486 - sparse_categorical_crossentropy: 0.3486 - val_loss: 0.3652 - val_sparse_categorical_crossentropy: 0.3652\n",
            "Epoch 216/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3472 - sparse_categorical_crossentropy: 0.3472 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "\n",
            "Epoch 00216: ReduceLROnPlateau reducing learning rate to 9.999999115286567e-07.\n",
            "Epoch 217/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3484 - sparse_categorical_crossentropy: 0.3484 - val_loss: 0.3651 - val_sparse_categorical_crossentropy: 0.3651\n",
            "Epoch 218/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3475 - sparse_categorical_crossentropy: 0.3475 - val_loss: 0.3652 - val_sparse_categorical_crossentropy: 0.3652\n",
            "Epoch 219/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3473 - sparse_categorical_crossentropy: 0.3473 - val_loss: 0.3653 - val_sparse_categorical_crossentropy: 0.3653\n",
            "Epoch 220/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3476 - sparse_categorical_crossentropy: 0.3476 - val_loss: 0.3650 - val_sparse_categorical_crossentropy: 0.3650\n",
            "Epoch 221/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3482 - sparse_categorical_crossentropy: 0.3482 - val_loss: 0.3650 - val_sparse_categorical_crossentropy: 0.3650\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00221: early stopping\n",
            "모델저장완료\n",
            "==================================================\n",
            "Loss와 ACC에 대한 Plot을 그립니다\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAETCAYAAABnSkJLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "기존nn모델의 train loss를 출력합니다\n",
            "train, loss and metric: [0.3267460208018521, 0.3267460208018521]\n",
            "기존nn모델의 valid loss를 출력합니다\n",
            "valid, loss and metric: [0.36425981141465485, 0.36425981141465485]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5oJjBcLFzzL",
        "colab_type": "code",
        "outputId": "cdb43dd6-aa50-4c98-bf4a-6763b3fa3b8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "savingpath_csv = 'drive/My Drive/데이콘_천체유형/파일제출/' + csv_folder + '/이상치_제거_initial_rate_%s.csv' % initial_rate\n",
        "\n",
        "print('best_n_epoch는 다음과 같습니다')\n",
        "print(np.argmin(hist.history['val_loss']))\n",
        "print('='*50)\n",
        "print('final rate는 다음과 같습니다')\n",
        "print(hist.history['lr'][np.argmin(hist.history['val_loss'])])\n",
        "print('='*25)\n",
        "y_pred = nn_model.predict(test_x)\n",
        "submission = pd.DataFrame(data=y_pred, columns=sample_submission.columns, index=sample_submission.index)\n",
        "submission.to_csv(savingpath_csv, index=True)\n",
        "print('csv 저장완료')\n",
        "\n",
        "print('='*50)\n",
        "best_val_loss = np.min(hist.history['val_loss'])\n",
        "print(\"best_valid_loss: {}\".format(best_val_loss))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final rate는 다음과 같습니다\n",
            "3.1622774e-05\n",
            "=========================\n",
            "csv 저장완료\n",
            "==================================================\n",
            "best_valid_loss: 0.36425981141465485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N9FQiFp70qoN"
      },
      "source": [
        "## initial_rate = 1e-3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "df09d342-af6e-4c36-e4dd-1e68d32081fe",
        "id": "iophgV1W0qoZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epoch =  800\n",
        "pat =  80\n",
        "red_pat =  25\n",
        "batchsize = 256*4\n",
        "initial_rate = 1e-3\n",
        "factor = 1/np.sqrt(10) # red_patience 만큼 기다리다가, 학습률을 *factor 배로 줄여버림 \n",
        "minimumlr = 1e-6\n",
        "\n",
        "import os\n",
        "MODEL_SAVE_FOLDER_PATH0 = SAVEMODEL_NEWFOLDER0 +  '/initial_rate=%s/' % initial_rate ## checkpoint\n",
        "MODEL_SAVE_FOLDER_PATH1 = SAVEMODEL_NEWFOLDER1 +  '/initial_rate=%s/' % initial_rate ## 기존모델저장\n",
        "if not os.path.exists(MODEL_SAVE_FOLDER_PATH1):\n",
        "  os.mkdir(MODEL_SAVE_FOLDER_PATH1)\n",
        "check_path = MODEL_SAVE_FOLDER_PATH0 + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
        "json_path = MODEL_SAVE_FOLDER_PATH1 + \"model1.json\"\n",
        "weight_path = MODEL_SAVE_FOLDER_PATH1 +\"model1.h5\"\n",
        "\n",
        "### model early stopping : 더이상 성능 개선이 되지 않으면 멈춤\n",
        "es = EarlyStopping(monitor= 'val_loss', patience = pat, verbose = 1, mode='min',\n",
        "                    restore_best_weights = True\n",
        "                   ) \n",
        "\n",
        "### model check point\n",
        "mc = ModelCheckpoint(filepath=check_path, monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "## ReduceLR on Plateau : val_loss가 안 줄어들 때 lr을 작게 할 수 있음 (local minima 대처방법)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor = factor,   # patience 만큼 기다리다가 0.1이면 학습률을 0.1배로 줄여버림 \n",
        "                        patience = red_pat, mode = 'min', verbose = 1,\n",
        "                        min_lr = minimumlr\n",
        "                        )\n",
        "\n",
        "from keras import optimizers\n",
        "optimizer = optimizers.Adam(\n",
        "    lr=initial_rate,\n",
        ")\n",
        "\n",
        "## compile model\n",
        "CCE = metrics.sparse_categorical_crossentropy\n",
        "\n",
        "nn_model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              #metrics=['accuracy']\n",
        "              #metrics=[metrics.sparse_categorical_accuracy]\n",
        "              metrics=[CCE]\n",
        "              )\n",
        "\n",
        "## fitting model\n",
        "hist = nn_model.fit(  train_input, train_target,validation_data=[cv_input, cv_target],\n",
        "                    batch_size=batchsize,\n",
        "                    epochs=epoch,\n",
        "                    callbacks = [es \n",
        "                                 #,mc\n",
        "                                 ,rlr\n",
        "                                ] )\n",
        "\n",
        "## save model\n",
        "model_json = nn_model.to_json()\n",
        "with open(json_path, \"w\") as json_file : \n",
        "    json_file.write(model_json)\n",
        "## model weight save\n",
        "nn_model.save_weights(weight_path)\n",
        "print(\"모델저장완료\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"Loss와 ACC에 대한 Plot을 그립니다\")\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "loss_ax.legend(loc='lower left')\n",
        "\n",
        "#acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
        "#acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        "#acc_ax.set_ylabel('accuracy')\n",
        "#acc_ax.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## evaluate model\n",
        "print('기존nn모델의 train loss를 출력합니다')\n",
        "loss_and_metric = nn_model.evaluate(train_input, train_target, batch_size=batchsize, verbose=0)\n",
        "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
        "## evaluate model\n",
        "print('기존nn모델의 valid loss를 출력합니다')\n",
        "loss_and_metric = nn_model.evaluate(cv_input, cv_target, batch_size=batchsize, verbose=0)\n",
        "print(\"valid, loss and metric: {}\".format(loss_and_metric))\n",
        "## model weight save ## 기존 모델의 가중치 저장\n",
        "#nn_model.save_weights(weight_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 184521 samples, validate on 15377 samples\n",
            "Epoch 1/800\n",
            "184521/184521 [==============================] - 11s 57us/step - loss: 0.6159 - sparse_categorical_crossentropy: 0.6159 - val_loss: 0.6626 - val_sparse_categorical_crossentropy: 0.6626\n",
            "Epoch 2/800\n",
            "184521/184521 [==============================] - 8s 42us/step - loss: 0.4991 - sparse_categorical_crossentropy: 0.4991 - val_loss: 0.5971 - val_sparse_categorical_crossentropy: 0.5971\n",
            "Epoch 3/800\n",
            "184521/184521 [==============================] - 8s 42us/step - loss: 0.4719 - sparse_categorical_crossentropy: 0.4719 - val_loss: 0.4651 - val_sparse_categorical_crossentropy: 0.4651\n",
            "Epoch 4/800\n",
            "184521/184521 [==============================] - 8s 43us/step - loss: 0.4611 - sparse_categorical_crossentropy: 0.4611 - val_loss: 0.4596 - val_sparse_categorical_crossentropy: 0.4596\n",
            "Epoch 5/800\n",
            "184521/184521 [==============================] - 8s 42us/step - loss: 0.4493 - sparse_categorical_crossentropy: 0.4493 - val_loss: 0.4468 - val_sparse_categorical_crossentropy: 0.4468\n",
            "Epoch 6/800\n",
            "184521/184521 [==============================] - 8s 43us/step - loss: 0.4443 - sparse_categorical_crossentropy: 0.4443 - val_loss: 0.5184 - val_sparse_categorical_crossentropy: 0.5184\n",
            "Epoch 7/800\n",
            "184521/184521 [==============================] - 8s 43us/step - loss: 0.4372 - sparse_categorical_crossentropy: 0.4372 - val_loss: 0.4372 - val_sparse_categorical_crossentropy: 0.4372\n",
            "Epoch 8/800\n",
            "184521/184521 [==============================] - 8s 43us/step - loss: 0.4359 - sparse_categorical_crossentropy: 0.4359 - val_loss: 0.4617 - val_sparse_categorical_crossentropy: 0.4617\n",
            "Epoch 9/800\n",
            "184521/184521 [==============================] - 8s 43us/step - loss: 0.4339 - sparse_categorical_crossentropy: 0.4339 - val_loss: 0.4138 - val_sparse_categorical_crossentropy: 0.4138\n",
            "Epoch 10/800\n",
            "184521/184521 [==============================] - 8s 43us/step - loss: 0.4253 - sparse_categorical_crossentropy: 0.4253 - val_loss: 0.4231 - val_sparse_categorical_crossentropy: 0.4231\n",
            "Epoch 11/800\n",
            "184521/184521 [==============================] - 8s 43us/step - loss: 0.4242 - sparse_categorical_crossentropy: 0.4242 - val_loss: 0.4163 - val_sparse_categorical_crossentropy: 0.4163\n",
            "Epoch 12/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4200 - sparse_categorical_crossentropy: 0.4200 - val_loss: 0.4065 - val_sparse_categorical_crossentropy: 0.4065\n",
            "Epoch 13/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4196 - sparse_categorical_crossentropy: 0.4196 - val_loss: 0.4019 - val_sparse_categorical_crossentropy: 0.4019\n",
            "Epoch 14/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4184 - sparse_categorical_crossentropy: 0.4184 - val_loss: 0.4128 - val_sparse_categorical_crossentropy: 0.4128\n",
            "Epoch 15/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4156 - sparse_categorical_crossentropy: 0.4156 - val_loss: 0.4128 - val_sparse_categorical_crossentropy: 0.4128\n",
            "Epoch 16/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4143 - sparse_categorical_crossentropy: 0.4143 - val_loss: 0.4275 - val_sparse_categorical_crossentropy: 0.4275\n",
            "Epoch 17/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4114 - sparse_categorical_crossentropy: 0.4114 - val_loss: 0.4404 - val_sparse_categorical_crossentropy: 0.4404\n",
            "Epoch 18/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4085 - sparse_categorical_crossentropy: 0.4085 - val_loss: 0.4144 - val_sparse_categorical_crossentropy: 0.4144\n",
            "Epoch 19/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4090 - sparse_categorical_crossentropy: 0.4090 - val_loss: 0.4123 - val_sparse_categorical_crossentropy: 0.4123\n",
            "Epoch 20/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4072 - sparse_categorical_crossentropy: 0.4072 - val_loss: 0.3974 - val_sparse_categorical_crossentropy: 0.3974\n",
            "Epoch 21/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4055 - sparse_categorical_crossentropy: 0.4055 - val_loss: 0.4065 - val_sparse_categorical_crossentropy: 0.4065\n",
            "Epoch 22/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4043 - sparse_categorical_crossentropy: 0.4043 - val_loss: 0.4027 - val_sparse_categorical_crossentropy: 0.4027\n",
            "Epoch 23/800\n",
            "184521/184521 [==============================] - 8s 45us/step - loss: 0.4036 - sparse_categorical_crossentropy: 0.4036 - val_loss: 0.4002 - val_sparse_categorical_crossentropy: 0.4002\n",
            "Epoch 24/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4031 - sparse_categorical_crossentropy: 0.4031 - val_loss: 0.3915 - val_sparse_categorical_crossentropy: 0.3915\n",
            "Epoch 25/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3998 - sparse_categorical_crossentropy: 0.3998 - val_loss: 0.3883 - val_sparse_categorical_crossentropy: 0.3883\n",
            "Epoch 26/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.4003 - sparse_categorical_crossentropy: 0.4003 - val_loss: 0.3950 - val_sparse_categorical_crossentropy: 0.3950\n",
            "Epoch 27/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3973 - sparse_categorical_crossentropy: 0.3973 - val_loss: 0.3811 - val_sparse_categorical_crossentropy: 0.3811\n",
            "Epoch 28/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3992 - sparse_categorical_crossentropy: 0.3992 - val_loss: 0.3933 - val_sparse_categorical_crossentropy: 0.3933\n",
            "Epoch 29/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3968 - sparse_categorical_crossentropy: 0.3968 - val_loss: 0.3939 - val_sparse_categorical_crossentropy: 0.3939\n",
            "Epoch 30/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3941 - sparse_categorical_crossentropy: 0.3941 - val_loss: 0.3852 - val_sparse_categorical_crossentropy: 0.3852\n",
            "Epoch 31/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3953 - sparse_categorical_crossentropy: 0.3953 - val_loss: 0.4065 - val_sparse_categorical_crossentropy: 0.4065\n",
            "Epoch 32/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3933 - sparse_categorical_crossentropy: 0.3933 - val_loss: 0.3837 - val_sparse_categorical_crossentropy: 0.3837\n",
            "Epoch 33/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3931 - sparse_categorical_crossentropy: 0.3931 - val_loss: 0.3914 - val_sparse_categorical_crossentropy: 0.3914\n",
            "Epoch 34/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3908 - sparse_categorical_crossentropy: 0.3908 - val_loss: 0.3887 - val_sparse_categorical_crossentropy: 0.3887\n",
            "Epoch 35/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3920 - sparse_categorical_crossentropy: 0.3920 - val_loss: 0.3801 - val_sparse_categorical_crossentropy: 0.3801\n",
            "Epoch 36/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3924 - sparse_categorical_crossentropy: 0.3924 - val_loss: 0.3990 - val_sparse_categorical_crossentropy: 0.3990\n",
            "Epoch 37/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3904 - sparse_categorical_crossentropy: 0.3904 - val_loss: 0.3874 - val_sparse_categorical_crossentropy: 0.3874\n",
            "Epoch 38/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3897 - sparse_categorical_crossentropy: 0.3897 - val_loss: 0.3903 - val_sparse_categorical_crossentropy: 0.3903\n",
            "Epoch 39/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3882 - sparse_categorical_crossentropy: 0.3882 - val_loss: 0.3905 - val_sparse_categorical_crossentropy: 0.3905\n",
            "Epoch 40/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3890 - sparse_categorical_crossentropy: 0.3890 - val_loss: 0.3777 - val_sparse_categorical_crossentropy: 0.3777\n",
            "Epoch 41/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3870 - sparse_categorical_crossentropy: 0.3870 - val_loss: 0.3828 - val_sparse_categorical_crossentropy: 0.3828\n",
            "Epoch 42/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3857 - sparse_categorical_crossentropy: 0.3857 - val_loss: 0.3784 - val_sparse_categorical_crossentropy: 0.3784\n",
            "Epoch 43/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3852 - sparse_categorical_crossentropy: 0.3852 - val_loss: 0.3853 - val_sparse_categorical_crossentropy: 0.3853\n",
            "Epoch 44/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3865 - sparse_categorical_crossentropy: 0.3865 - val_loss: 0.3764 - val_sparse_categorical_crossentropy: 0.3764\n",
            "Epoch 45/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3852 - sparse_categorical_crossentropy: 0.3852 - val_loss: 0.3900 - val_sparse_categorical_crossentropy: 0.3900\n",
            "Epoch 46/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3834 - sparse_categorical_crossentropy: 0.3834 - val_loss: 0.3865 - val_sparse_categorical_crossentropy: 0.3865\n",
            "Epoch 47/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3851 - sparse_categorical_crossentropy: 0.3851 - val_loss: 0.3813 - val_sparse_categorical_crossentropy: 0.3813\n",
            "Epoch 48/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3814 - sparse_categorical_crossentropy: 0.3814 - val_loss: 0.3868 - val_sparse_categorical_crossentropy: 0.3868\n",
            "Epoch 49/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3828 - sparse_categorical_crossentropy: 0.3828 - val_loss: 0.3792 - val_sparse_categorical_crossentropy: 0.3792\n",
            "Epoch 50/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3825 - sparse_categorical_crossentropy: 0.3825 - val_loss: 0.3721 - val_sparse_categorical_crossentropy: 0.3721\n",
            "Epoch 51/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3821 - sparse_categorical_crossentropy: 0.3821 - val_loss: 0.3868 - val_sparse_categorical_crossentropy: 0.3868\n",
            "Epoch 52/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3775 - sparse_categorical_crossentropy: 0.3775 - val_loss: 0.3924 - val_sparse_categorical_crossentropy: 0.3924\n",
            "Epoch 53/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3814 - sparse_categorical_crossentropy: 0.3814 - val_loss: 0.3854 - val_sparse_categorical_crossentropy: 0.3854\n",
            "Epoch 54/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3794 - sparse_categorical_crossentropy: 0.3794 - val_loss: 0.3758 - val_sparse_categorical_crossentropy: 0.3758\n",
            "Epoch 55/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3784 - sparse_categorical_crossentropy: 0.3784 - val_loss: 0.3782 - val_sparse_categorical_crossentropy: 0.3782\n",
            "Epoch 56/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3781 - sparse_categorical_crossentropy: 0.3781 - val_loss: 0.3822 - val_sparse_categorical_crossentropy: 0.3822\n",
            "Epoch 57/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3780 - sparse_categorical_crossentropy: 0.3780 - val_loss: 0.3789 - val_sparse_categorical_crossentropy: 0.3789\n",
            "Epoch 58/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3762 - sparse_categorical_crossentropy: 0.3762 - val_loss: 0.3762 - val_sparse_categorical_crossentropy: 0.3762\n",
            "Epoch 59/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3776 - sparse_categorical_crossentropy: 0.3776 - val_loss: 0.3829 - val_sparse_categorical_crossentropy: 0.3829\n",
            "Epoch 60/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3773 - sparse_categorical_crossentropy: 0.3773 - val_loss: 0.4018 - val_sparse_categorical_crossentropy: 0.4018\n",
            "Epoch 61/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3781 - sparse_categorical_crossentropy: 0.3781 - val_loss: 0.3785 - val_sparse_categorical_crossentropy: 0.3785\n",
            "Epoch 62/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3748 - sparse_categorical_crossentropy: 0.3748 - val_loss: 0.4258 - val_sparse_categorical_crossentropy: 0.4258\n",
            "Epoch 63/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3740 - sparse_categorical_crossentropy: 0.3740 - val_loss: 0.3819 - val_sparse_categorical_crossentropy: 0.3819\n",
            "Epoch 64/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3766 - sparse_categorical_crossentropy: 0.3766 - val_loss: 0.3709 - val_sparse_categorical_crossentropy: 0.3709\n",
            "Epoch 65/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3721 - sparse_categorical_crossentropy: 0.3721 - val_loss: 0.3772 - val_sparse_categorical_crossentropy: 0.3772\n",
            "Epoch 66/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3758 - sparse_categorical_crossentropy: 0.3758 - val_loss: 0.3733 - val_sparse_categorical_crossentropy: 0.3733\n",
            "Epoch 67/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3726 - sparse_categorical_crossentropy: 0.3726 - val_loss: 0.3755 - val_sparse_categorical_crossentropy: 0.3755\n",
            "Epoch 68/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3708 - sparse_categorical_crossentropy: 0.3708 - val_loss: 0.3768 - val_sparse_categorical_crossentropy: 0.3768\n",
            "Epoch 69/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3719 - sparse_categorical_crossentropy: 0.3719 - val_loss: 0.3877 - val_sparse_categorical_crossentropy: 0.3877\n",
            "Epoch 70/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3736 - sparse_categorical_crossentropy: 0.3736 - val_loss: 0.3800 - val_sparse_categorical_crossentropy: 0.3800\n",
            "Epoch 71/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3718 - sparse_categorical_crossentropy: 0.3718 - val_loss: 0.4072 - val_sparse_categorical_crossentropy: 0.4072\n",
            "Epoch 72/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3736 - sparse_categorical_crossentropy: 0.3736 - val_loss: 0.3741 - val_sparse_categorical_crossentropy: 0.3741\n",
            "Epoch 73/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3709 - sparse_categorical_crossentropy: 0.3709 - val_loss: 0.3694 - val_sparse_categorical_crossentropy: 0.3694\n",
            "Epoch 74/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3697 - sparse_categorical_crossentropy: 0.3697 - val_loss: 0.3777 - val_sparse_categorical_crossentropy: 0.3777\n",
            "Epoch 75/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3701 - sparse_categorical_crossentropy: 0.3701 - val_loss: 0.3733 - val_sparse_categorical_crossentropy: 0.3733\n",
            "Epoch 76/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3685 - sparse_categorical_crossentropy: 0.3685 - val_loss: 0.3739 - val_sparse_categorical_crossentropy: 0.3739\n",
            "Epoch 77/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3687 - sparse_categorical_crossentropy: 0.3687 - val_loss: 0.3757 - val_sparse_categorical_crossentropy: 0.3757\n",
            "Epoch 78/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3670 - sparse_categorical_crossentropy: 0.3670 - val_loss: 0.3767 - val_sparse_categorical_crossentropy: 0.3767\n",
            "Epoch 79/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3700 - sparse_categorical_crossentropy: 0.3700 - val_loss: 0.3909 - val_sparse_categorical_crossentropy: 0.3909\n",
            "Epoch 80/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3672 - sparse_categorical_crossentropy: 0.3672 - val_loss: 0.3828 - val_sparse_categorical_crossentropy: 0.3828\n",
            "Epoch 81/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3667 - sparse_categorical_crossentropy: 0.3667 - val_loss: 0.3795 - val_sparse_categorical_crossentropy: 0.3795\n",
            "Epoch 82/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3667 - sparse_categorical_crossentropy: 0.3667 - val_loss: 0.3909 - val_sparse_categorical_crossentropy: 0.3909\n",
            "Epoch 83/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3700 - sparse_categorical_crossentropy: 0.3700 - val_loss: 0.3743 - val_sparse_categorical_crossentropy: 0.3743\n",
            "Epoch 84/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3667 - sparse_categorical_crossentropy: 0.3667 - val_loss: 0.3742 - val_sparse_categorical_crossentropy: 0.3742\n",
            "Epoch 85/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3656 - sparse_categorical_crossentropy: 0.3656 - val_loss: 0.3698 - val_sparse_categorical_crossentropy: 0.3698\n",
            "Epoch 86/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3650 - sparse_categorical_crossentropy: 0.3650 - val_loss: 0.3794 - val_sparse_categorical_crossentropy: 0.3794\n",
            "Epoch 87/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3648 - sparse_categorical_crossentropy: 0.3648 - val_loss: 0.3804 - val_sparse_categorical_crossentropy: 0.3804\n",
            "Epoch 88/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3635 - sparse_categorical_crossentropy: 0.3635 - val_loss: 0.3852 - val_sparse_categorical_crossentropy: 0.3852\n",
            "Epoch 89/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3635 - sparse_categorical_crossentropy: 0.3635 - val_loss: 0.3761 - val_sparse_categorical_crossentropy: 0.3761\n",
            "Epoch 90/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3642 - sparse_categorical_crossentropy: 0.3642 - val_loss: 0.3789 - val_sparse_categorical_crossentropy: 0.3789\n",
            "Epoch 91/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3637 - sparse_categorical_crossentropy: 0.3637 - val_loss: 0.3840 - val_sparse_categorical_crossentropy: 0.3840\n",
            "Epoch 92/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3639 - sparse_categorical_crossentropy: 0.3639 - val_loss: 0.3731 - val_sparse_categorical_crossentropy: 0.3731\n",
            "Epoch 93/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3627 - sparse_categorical_crossentropy: 0.3627 - val_loss: 0.3749 - val_sparse_categorical_crossentropy: 0.3749\n",
            "Epoch 94/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3606 - sparse_categorical_crossentropy: 0.3606 - val_loss: 0.3783 - val_sparse_categorical_crossentropy: 0.3783\n",
            "Epoch 95/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3630 - sparse_categorical_crossentropy: 0.3630 - val_loss: 0.3728 - val_sparse_categorical_crossentropy: 0.3728\n",
            "Epoch 96/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3613 - sparse_categorical_crossentropy: 0.3613 - val_loss: 0.3701 - val_sparse_categorical_crossentropy: 0.3701\n",
            "Epoch 97/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3639 - sparse_categorical_crossentropy: 0.3639 - val_loss: 0.3747 - val_sparse_categorical_crossentropy: 0.3747\n",
            "Epoch 98/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3616 - sparse_categorical_crossentropy: 0.3616 - val_loss: 0.3953 - val_sparse_categorical_crossentropy: 0.3953\n",
            "\n",
            "Epoch 00098: ReduceLROnPlateau reducing learning rate to 0.00031622778103685084.\n",
            "Epoch 99/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3520 - sparse_categorical_crossentropy: 0.3520 - val_loss: 0.3675 - val_sparse_categorical_crossentropy: 0.3675\n",
            "Epoch 100/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3477 - sparse_categorical_crossentropy: 0.3477 - val_loss: 0.3652 - val_sparse_categorical_crossentropy: 0.3652\n",
            "Epoch 101/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3460 - sparse_categorical_crossentropy: 0.3460 - val_loss: 0.3653 - val_sparse_categorical_crossentropy: 0.3653\n",
            "Epoch 102/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3470 - sparse_categorical_crossentropy: 0.3470 - val_loss: 0.3670 - val_sparse_categorical_crossentropy: 0.3670\n",
            "Epoch 103/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3472 - sparse_categorical_crossentropy: 0.3472 - val_loss: 0.3680 - val_sparse_categorical_crossentropy: 0.3680\n",
            "Epoch 104/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3465 - sparse_categorical_crossentropy: 0.3465 - val_loss: 0.3652 - val_sparse_categorical_crossentropy: 0.3652\n",
            "Epoch 105/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3440 - sparse_categorical_crossentropy: 0.3440 - val_loss: 0.3674 - val_sparse_categorical_crossentropy: 0.3674\n",
            "Epoch 106/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3447 - sparse_categorical_crossentropy: 0.3447 - val_loss: 0.3695 - val_sparse_categorical_crossentropy: 0.3695\n",
            "Epoch 107/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3441 - sparse_categorical_crossentropy: 0.3441 - val_loss: 0.3693 - val_sparse_categorical_crossentropy: 0.3693\n",
            "Epoch 108/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3449 - sparse_categorical_crossentropy: 0.3449 - val_loss: 0.3695 - val_sparse_categorical_crossentropy: 0.3695\n",
            "Epoch 109/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3442 - sparse_categorical_crossentropy: 0.3442 - val_loss: 0.3671 - val_sparse_categorical_crossentropy: 0.3671\n",
            "Epoch 110/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3444 - sparse_categorical_crossentropy: 0.3444 - val_loss: 0.3645 - val_sparse_categorical_crossentropy: 0.3645\n",
            "Epoch 111/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3440 - sparse_categorical_crossentropy: 0.3440 - val_loss: 0.3691 - val_sparse_categorical_crossentropy: 0.3691\n",
            "Epoch 112/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3437 - sparse_categorical_crossentropy: 0.3437 - val_loss: 0.3676 - val_sparse_categorical_crossentropy: 0.3676\n",
            "Epoch 113/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3415 - sparse_categorical_crossentropy: 0.3415 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 114/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3420 - sparse_categorical_crossentropy: 0.3420 - val_loss: 0.3670 - val_sparse_categorical_crossentropy: 0.3670\n",
            "Epoch 115/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3421 - sparse_categorical_crossentropy: 0.3421 - val_loss: 0.3675 - val_sparse_categorical_crossentropy: 0.3675\n",
            "Epoch 116/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3417 - sparse_categorical_crossentropy: 0.3417 - val_loss: 0.3689 - val_sparse_categorical_crossentropy: 0.3689\n",
            "Epoch 117/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3428 - sparse_categorical_crossentropy: 0.3428 - val_loss: 0.3689 - val_sparse_categorical_crossentropy: 0.3689\n",
            "Epoch 118/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3403 - sparse_categorical_crossentropy: 0.3403 - val_loss: 0.3676 - val_sparse_categorical_crossentropy: 0.3676\n",
            "Epoch 119/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3418 - sparse_categorical_crossentropy: 0.3418 - val_loss: 0.3675 - val_sparse_categorical_crossentropy: 0.3675\n",
            "Epoch 120/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3413 - sparse_categorical_crossentropy: 0.3413 - val_loss: 0.3689 - val_sparse_categorical_crossentropy: 0.3689\n",
            "Epoch 121/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3408 - sparse_categorical_crossentropy: 0.3408 - val_loss: 0.3673 - val_sparse_categorical_crossentropy: 0.3673\n",
            "Epoch 122/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3406 - sparse_categorical_crossentropy: 0.3406 - val_loss: 0.3683 - val_sparse_categorical_crossentropy: 0.3683\n",
            "Epoch 123/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3418 - sparse_categorical_crossentropy: 0.3418 - val_loss: 0.3674 - val_sparse_categorical_crossentropy: 0.3674\n",
            "Epoch 124/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3415 - sparse_categorical_crossentropy: 0.3415 - val_loss: 0.3697 - val_sparse_categorical_crossentropy: 0.3697\n",
            "Epoch 125/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3393 - sparse_categorical_crossentropy: 0.3393 - val_loss: 0.3708 - val_sparse_categorical_crossentropy: 0.3708\n",
            "Epoch 126/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3397 - sparse_categorical_crossentropy: 0.3397 - val_loss: 0.3678 - val_sparse_categorical_crossentropy: 0.3678\n",
            "Epoch 127/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3408 - sparse_categorical_crossentropy: 0.3408 - val_loss: 0.3683 - val_sparse_categorical_crossentropy: 0.3683\n",
            "Epoch 128/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3400 - sparse_categorical_crossentropy: 0.3400 - val_loss: 0.3696 - val_sparse_categorical_crossentropy: 0.3696\n",
            "Epoch 129/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3393 - sparse_categorical_crossentropy: 0.3393 - val_loss: 0.3683 - val_sparse_categorical_crossentropy: 0.3683\n",
            "Epoch 130/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3402 - sparse_categorical_crossentropy: 0.3402 - val_loss: 0.3687 - val_sparse_categorical_crossentropy: 0.3687\n",
            "Epoch 131/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3390 - sparse_categorical_crossentropy: 0.3390 - val_loss: 0.3722 - val_sparse_categorical_crossentropy: 0.3722\n",
            "Epoch 132/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3397 - sparse_categorical_crossentropy: 0.3397 - val_loss: 0.3703 - val_sparse_categorical_crossentropy: 0.3703\n",
            "Epoch 133/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3388 - sparse_categorical_crossentropy: 0.3388 - val_loss: 0.3684 - val_sparse_categorical_crossentropy: 0.3684\n",
            "Epoch 134/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3385 - sparse_categorical_crossentropy: 0.3385 - val_loss: 0.3692 - val_sparse_categorical_crossentropy: 0.3692\n",
            "Epoch 135/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3371 - sparse_categorical_crossentropy: 0.3371 - val_loss: 0.3676 - val_sparse_categorical_crossentropy: 0.3676\n",
            "\n",
            "Epoch 00135: ReduceLROnPlateau reducing learning rate to 0.00010000000639606199.\n",
            "Epoch 136/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3349 - sparse_categorical_crossentropy: 0.3349 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 137/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3335 - sparse_categorical_crossentropy: 0.3335 - val_loss: 0.3662 - val_sparse_categorical_crossentropy: 0.3662\n",
            "Epoch 138/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3349 - sparse_categorical_crossentropy: 0.3349 - val_loss: 0.3671 - val_sparse_categorical_crossentropy: 0.3671\n",
            "Epoch 139/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3329 - sparse_categorical_crossentropy: 0.3329 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 140/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3341 - sparse_categorical_crossentropy: 0.3341 - val_loss: 0.3673 - val_sparse_categorical_crossentropy: 0.3673\n",
            "Epoch 141/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3334 - sparse_categorical_crossentropy: 0.3334 - val_loss: 0.3662 - val_sparse_categorical_crossentropy: 0.3662\n",
            "Epoch 142/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3331 - sparse_categorical_crossentropy: 0.3331 - val_loss: 0.3669 - val_sparse_categorical_crossentropy: 0.3669\n",
            "Epoch 143/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3331 - sparse_categorical_crossentropy: 0.3331 - val_loss: 0.3662 - val_sparse_categorical_crossentropy: 0.3662\n",
            "Epoch 144/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3327 - sparse_categorical_crossentropy: 0.3327 - val_loss: 0.3676 - val_sparse_categorical_crossentropy: 0.3676\n",
            "Epoch 145/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3331 - sparse_categorical_crossentropy: 0.3331 - val_loss: 0.3674 - val_sparse_categorical_crossentropy: 0.3674\n",
            "Epoch 146/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3331 - sparse_categorical_crossentropy: 0.3331 - val_loss: 0.3677 - val_sparse_categorical_crossentropy: 0.3677\n",
            "Epoch 147/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3318 - sparse_categorical_crossentropy: 0.3318 - val_loss: 0.3677 - val_sparse_categorical_crossentropy: 0.3677\n",
            "Epoch 148/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3329 - sparse_categorical_crossentropy: 0.3329 - val_loss: 0.3662 - val_sparse_categorical_crossentropy: 0.3662\n",
            "Epoch 149/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3322 - sparse_categorical_crossentropy: 0.3322 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "Epoch 150/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3320 - sparse_categorical_crossentropy: 0.3320 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 151/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3310 - sparse_categorical_crossentropy: 0.3310 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 152/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3331 - sparse_categorical_crossentropy: 0.3331 - val_loss: 0.3674 - val_sparse_categorical_crossentropy: 0.3674\n",
            "Epoch 153/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3309 - sparse_categorical_crossentropy: 0.3309 - val_loss: 0.3681 - val_sparse_categorical_crossentropy: 0.3681\n",
            "Epoch 154/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3315 - sparse_categorical_crossentropy: 0.3315 - val_loss: 0.3682 - val_sparse_categorical_crossentropy: 0.3682\n",
            "Epoch 155/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3306 - sparse_categorical_crossentropy: 0.3306 - val_loss: 0.3675 - val_sparse_categorical_crossentropy: 0.3675\n",
            "Epoch 156/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3316 - sparse_categorical_crossentropy: 0.3316 - val_loss: 0.3676 - val_sparse_categorical_crossentropy: 0.3676\n",
            "Epoch 157/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3318 - sparse_categorical_crossentropy: 0.3318 - val_loss: 0.3673 - val_sparse_categorical_crossentropy: 0.3673\n",
            "Epoch 158/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3319 - sparse_categorical_crossentropy: 0.3319 - val_loss: 0.3683 - val_sparse_categorical_crossentropy: 0.3683\n",
            "Epoch 159/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3318 - sparse_categorical_crossentropy: 0.3318 - val_loss: 0.3681 - val_sparse_categorical_crossentropy: 0.3681\n",
            "Epoch 160/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3318 - sparse_categorical_crossentropy: 0.3318 - val_loss: 0.3677 - val_sparse_categorical_crossentropy: 0.3677\n",
            "\n",
            "Epoch 00160: ReduceLROnPlateau reducing learning rate to 3.1622778103685084e-05.\n",
            "Epoch 161/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3299 - sparse_categorical_crossentropy: 0.3299 - val_loss: 0.3667 - val_sparse_categorical_crossentropy: 0.3667\n",
            "Epoch 162/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3297 - sparse_categorical_crossentropy: 0.3297 - val_loss: 0.3675 - val_sparse_categorical_crossentropy: 0.3675\n",
            "Epoch 163/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3288 - sparse_categorical_crossentropy: 0.3288 - val_loss: 0.3669 - val_sparse_categorical_crossentropy: 0.3669\n",
            "Epoch 164/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3287 - sparse_categorical_crossentropy: 0.3287 - val_loss: 0.3670 - val_sparse_categorical_crossentropy: 0.3670\n",
            "Epoch 165/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3295 - sparse_categorical_crossentropy: 0.3295 - val_loss: 0.3673 - val_sparse_categorical_crossentropy: 0.3673\n",
            "Epoch 166/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3299 - sparse_categorical_crossentropy: 0.3299 - val_loss: 0.3670 - val_sparse_categorical_crossentropy: 0.3670\n",
            "Epoch 167/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3293 - sparse_categorical_crossentropy: 0.3293 - val_loss: 0.3679 - val_sparse_categorical_crossentropy: 0.3679\n",
            "Epoch 168/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3288 - sparse_categorical_crossentropy: 0.3288 - val_loss: 0.3669 - val_sparse_categorical_crossentropy: 0.3669\n",
            "Epoch 169/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3290 - sparse_categorical_crossentropy: 0.3290 - val_loss: 0.3670 - val_sparse_categorical_crossentropy: 0.3670\n",
            "Epoch 170/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3298 - sparse_categorical_crossentropy: 0.3298 - val_loss: 0.3685 - val_sparse_categorical_crossentropy: 0.3685\n",
            "Epoch 171/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3291 - sparse_categorical_crossentropy: 0.3291 - val_loss: 0.3672 - val_sparse_categorical_crossentropy: 0.3672\n",
            "Epoch 172/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3296 - sparse_categorical_crossentropy: 0.3296 - val_loss: 0.3673 - val_sparse_categorical_crossentropy: 0.3673\n",
            "Epoch 173/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3302 - sparse_categorical_crossentropy: 0.3302 - val_loss: 0.3674 - val_sparse_categorical_crossentropy: 0.3674\n",
            "Epoch 174/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3269 - sparse_categorical_crossentropy: 0.3269 - val_loss: 0.3673 - val_sparse_categorical_crossentropy: 0.3673\n",
            "Epoch 175/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3287 - sparse_categorical_crossentropy: 0.3287 - val_loss: 0.3668 - val_sparse_categorical_crossentropy: 0.3668\n",
            "Epoch 176/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3286 - sparse_categorical_crossentropy: 0.3286 - val_loss: 0.3672 - val_sparse_categorical_crossentropy: 0.3672\n",
            "Epoch 177/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3287 - sparse_categorical_crossentropy: 0.3287 - val_loss: 0.3669 - val_sparse_categorical_crossentropy: 0.3669\n",
            "Epoch 178/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3301 - sparse_categorical_crossentropy: 0.3301 - val_loss: 0.3667 - val_sparse_categorical_crossentropy: 0.3667\n",
            "Epoch 179/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3287 - sparse_categorical_crossentropy: 0.3287 - val_loss: 0.3666 - val_sparse_categorical_crossentropy: 0.3666\n",
            "Epoch 180/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3296 - sparse_categorical_crossentropy: 0.3296 - val_loss: 0.3675 - val_sparse_categorical_crossentropy: 0.3675\n",
            "Epoch 181/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3295 - sparse_categorical_crossentropy: 0.3295 - val_loss: 0.3682 - val_sparse_categorical_crossentropy: 0.3682\n",
            "Epoch 182/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3294 - sparse_categorical_crossentropy: 0.3294 - val_loss: 0.3676 - val_sparse_categorical_crossentropy: 0.3676\n",
            "Epoch 183/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3282 - sparse_categorical_crossentropy: 0.3282 - val_loss: 0.3673 - val_sparse_categorical_crossentropy: 0.3673\n",
            "Epoch 184/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3287 - sparse_categorical_crossentropy: 0.3287 - val_loss: 0.3681 - val_sparse_categorical_crossentropy: 0.3681\n",
            "Epoch 185/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3287 - sparse_categorical_crossentropy: 0.3287 - val_loss: 0.3676 - val_sparse_categorical_crossentropy: 0.3676\n",
            "\n",
            "Epoch 00185: ReduceLROnPlateau reducing learning rate to 1.0000000409520217e-05.\n",
            "Epoch 186/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3287 - sparse_categorical_crossentropy: 0.3287 - val_loss: 0.3671 - val_sparse_categorical_crossentropy: 0.3671\n",
            "Epoch 187/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3284 - sparse_categorical_crossentropy: 0.3284 - val_loss: 0.3676 - val_sparse_categorical_crossentropy: 0.3676\n",
            "Epoch 188/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3286 - sparse_categorical_crossentropy: 0.3286 - val_loss: 0.3673 - val_sparse_categorical_crossentropy: 0.3673\n",
            "Epoch 189/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3281 - sparse_categorical_crossentropy: 0.3281 - val_loss: 0.3669 - val_sparse_categorical_crossentropy: 0.3669\n",
            "Epoch 190/800\n",
            "184521/184521 [==============================] - 8s 44us/step - loss: 0.3292 - sparse_categorical_crossentropy: 0.3292 - val_loss: 0.3671 - val_sparse_categorical_crossentropy: 0.3671\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00190: early stopping\n",
            "모델저장완료\n",
            "==================================================\n",
            "Loss와 ACC에 대한 Plot을 그립니다\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAETCAYAAAC4BDhUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeXxcZb348c85Z9ZM9jTpku7bQ9kK\ntGwVJJS17lcFRFlckPLDq1YveF0upah49SqKLEpQlFau6EURlF0QENnL0gJtH7qnTZul2ZfZ5/z+\nOJN0mmabpJmZ0O/79cork2fO8j1Jm2+e3bBtGyGEEGI8MrMdgBBCCDFSksSEEEKMW5LEhBBCjFuS\nxIQQQoxbksSEEEKMW5LEhBBCjFuubAcghBDivU8p9RPgE8BM4Bit9dv9HGMBtwDnAzbwQ631rwe7\nrtTEhBBCZMIDwPuBnYMc8xlgLjAPOBVYpZSaOdhFD9eaWAFwCbAJiGY5FiGEGA/cdXV1J6xaterx\np59+urPPe61a69bBTtZa/wtAKTXYYRcBv9JaJ4BGpdQDwAXAjwc64XBNYpcAv8h2EEIIMZ5MmjSJ\ntra2n/Tz1g3AqkNwi+kcWFOrAaYNdsLhmsQ2AbS3B4nHE2mfXFISoKWl65AHdahIfKOX6zFKfKOT\n6/FB7sVoWSaFhX6uuOKKi6+++uqX+rw9aC1sLB2uSSwKEI8niMXST2LAiM/LFIlv9HI9RolvdHI9\nPsjNGM8666zdWusdY3T5GmAG8Gry6741s4McrklMCCFE7rkP+KJS6n6gDPgYcPpgJ8joRCGEEGNO\nKXWLUmo3MBV4Uin1TrL8EaXU4uRhvwO2AZuBl4Dvaq23D3ZdqYkJIYQYc1rrrwBf6af8Aymv48D/\nS+e6UhMTQggxbkkSE0IIMW5JEhNCCDFuSRJLU+f6dYQbG7MdhhBCCCSJpa3u13ey56FHsh2GEEII\nJImlzbAs4sFgtsMQQgiBJLG0mV4viXA422EIIYRAkljaDK+XeEiSmBBC5AJJYmkyPR4SoVC2wxBC\nCEEGV+xQSs0HVuOsh9UEXKa13tzPcRcC1wEGzs6eZ2ut65VSq4CrgT3JQ5/XWn8pE7GnMrxeEpFI\npm8rhBCiH5lcduoO4Hat9T1KqUuAamBp6gHJ9bNWAUu11nVKqSIgte1ujdb6mkwF3B/T4yHe2Z7N\nEIQQQiRlJIkppSqAE4BzkkX3Arcppcq11qmTrr4G/ERrXQegtW47BPcuBopTy6qrqydVVVWN6HqG\nx0tM+sSEECInZKomNg2oTS7uiNY6rpTakyxPTWJHAtuVUv8E8oH7gRu11nby/U8ppc4F6oDrtdYv\nDuPeK4DrUwuqq6upqqqipCSQ9oO0FuXTuj1MeXlB2udmksQ3erkeo8Q3OrkeH4yPGLMt11axt4Bj\ncWpsHuAxnE3S1uA0R96otY4qpc4BHlRKLdBaNw1xzZuBu1MLli9fvhi4r6WlK+1N5yK2QSIcprGx\nI63zMqm8vEDiG6Vcj1HiG51cjw9yL0aXyxzRH/5jLVNJbBdQqZSykrUwC5iSLE9VA/xJax0Gwkqp\nB4GTcPrC6noO0lr/XSm1CzgaeHawG2utWzl46+ypI30Qw+MlLqMThRAiJ2RkiL3WugF4E7g4WXQx\n8Eaf/jCA3wPnKqUMpZQbOAtYB6CUquw5SCl1HDAT0GMc+kFMrxc7FsOOxzN9ayGEEH1ksjnxKmC1\nUmol0AJcBs6unsBKrfVa4A/AYmADkAAeB+5Knv8DpdQiIA5EgEtTa2eZYng8ACQiESy/P9O3F0II\nkSJjSUxrvQk4uZ/y1F09E8DXkx99j7t8TAMcJtPjBcAOh0GSmBBCZJWs2JEm0+skMZnwLIQQ2SdJ\nLE2G12lOtGURYCGEyDpJYmnqaU5MRCSJCSFEtkkSS5ORbE60pTlRCCGyTpJYmnprYtKcKIQQWSdJ\nLE29Q+wliQkhRNZJEkuT2ducKElMCCGyTZJYmnqH2IelT0wIIbJNkliaepoTpSYmhBDZJ0ksTYbb\nDYYhQ+yFECIHSBJLk2EYmF6vNCcKIUQOkCQ2ApbXKyt2CCFEDpAkNgKmzyvNiUIIkQMkiaWpfvMa\nsGxsaU4UQoiskySWpkj3XmzLlpqYEELkAEliaTJMC8NtydqJQgiRAySJpckwLAyPJctOCSFEDsjY\nzs7vGYYFLlOSmBBCpEEpNR9YDZQBTcBlWuvNfY6pAH4LTAPcwNPAV7TWsYGuKzWxNPXUxKQ5UQgh\n0nIHcLvWej5wO1DdzzHfBjZqrY8FjgUWAR8f7KJSE0uTYVgYUhMTQhymHn300UkrVqyY2ae4VWvd\nOtA5yRrWCcA5yaJ7gduUUuVa68aUQ22gQCllAl7AA9QOFo9h23aajzAyw6lKJo+7ELgOMHAe6Gyt\ndb1SygJuAc5Plv9Qa/3rEYZzGvDcCM8VQojD1tKlS6mtPSiv3KC1XjXQOUqpRcAarfVRKWUbgEu0\n1q+nlJUCfwaOBALAbVrrbw4WTyZrYj1VyXuUUpfgVCWXph6glFoMrAKWaq3rlFJFQE+V5zPAXGAe\nTiJ8Qyn1pNZ6x0gDamnpIhZLpHVOnb6L+Ct76Xp2C/Oq78KwrJHefsyUlxfQ2NiR7TAGlOvxQe7H\nKPGNTq7HB7kXo8tlUlIS4Nprr71gxYoVa/u8PWAtLE0XAOuBs4AC4FGl1Ce11n8aMK5DdONBpVGV\n/BrwE611HYDWui3lvYuAX2mtE0CjUuoBnAf+8RD3LgaKU8uqq6snVVVVjehZDNPCdhsAJCIRLL9/\nRNcRQojxaNmyZXXLli3bkeZpu4BKpZSltY4nW9amJMtTfRn4fPL3fJtS6kHgTCC7SQxnpEmt1joO\nkHyIPcny1CR2JLBdKfVPIB+4H7hRa20D04GdKcfWJM8fygrg+tSC6upqqqqqKCkJpP0gLTu9xFxO\nEivNd+MpLUj7GplQXp6bcfXI9fgg92OU+EYn1+OD8RHjcGitG5RSbwIXA/ckP7/RpxIDsB2ny+gV\npZQHOBsnDwwo1wZ2WDgjUs7B6dB7DCdZrRnFNW8G7k4tWL58+WLgvpE0J0ZjNiSTWOOeJjzxXPsW\n5l4zRF+5Hh/kfowS3+jkenyQezH2NCeOwlXAaqXUSqAFuAxAKfUIsFJrvRan0nGHUuotnHzwNPCr\nQeMaTURpGG5Vsgb4k9Y6DISTVcmTcJJYDTADeDV5bN+aWb+SI2b6ttdOHemDGIYFpjMYxo4POHVB\nCCFECq31JuDkfso/kPJ6K/u7nYYlI/PEtNYNQE9VEgauSv4eOFcpZSil3Dide+uS790HfFEpZSql\nyoGPMUg76VgxDAs7+V2zY5LEhBAimzI52fkq4MtKqXdxOu+uAqcqmRyVCPAHoAHYgJP03gHuSr73\nO2AbsBl4Cfiu1np75sJPMixsI9kEGY9n/PZCCCH2y1iHzjCrkgng68mPvsfFgf83ljEOh2FYYPQ0\nJ0oSE0KIbJJlp9JkmBZ2T5+YNCcKIURWSRJLl2EBTnOi1MSEECK7JImlyUjpE5MkJoQQ2SVJLE3O\n6MTkepMyxF4IIbJKkliaZGCHEELkDkli6TKs3u+aJDEhhMguSWJpMgwLLGfZKRmdKIQQ2SVJLE2G\naWGYySQmNTEhhMgqSWLpSmlOlBU7hBAiuySJpclZAFhqYkIIkQskiaXpgCQmfWJCCJFVksTSdMDA\nDqmJCSFEVkkSS5f0iQkhRM6QJJYmwzAxDANMU5oThRAiyySJpckwLeezZUpzohBCZJkksXQZThLD\nlCQmhBDZJkksTYbRUxOzsGUBYCGEyCpJYmnqSWJYpgzsEEKILJMkli4jpU8sJklMCCGySZJYmvbX\nxKQ5UQghss2VqRsppeYDq4EyoAm4TGu9uc8xq4CrgT3Joue11l9Kvnc3cDawL/nefVrrG8c+8gP1\njk40DRnYIYQQWZaxJAbcAdyutb5HKXUJUA0s7ee4NVrrawa4xg+11reNWYTDkNonJklMCCGyKyPN\niUqpCuAE4N5k0b3ACUqp8kzc/5BK6RNDJjsLIURWZaomNg2o1VrHAbTWcaXUnmR5Y59jP6WUOheo\nA67XWr+Y8t7XlVLLga3At7TWG4e6sVKqGChOLauurp5UVVU1ogfprYlJc6IQQmRdJpsTh+MO4Eat\ndVQpdQ7woFJqgda6CfgOsFdrnVBKXQY8ppSa3ZMYB7ECuD61oLq6mqqqKkpKAmkHmEj42Q1YbguX\nCeXlBWlfIxNyNa4euR4f5H6MEt/o5Hp8MD5izLZMJbFdQKVSykrWwixgSrK8l9a6LuX135VSu4Cj\ngWe11rUp761RSv0MmArsHOLeNwN3pxYsX758MXBfS0sXsVgirQexbRuABBAJRWhs7Ejr/EwoLy/I\nybh65Hp8kPsxSnyjk+vxQe7F6HKZI/rDf6xlJIlprRuUUm8CFwP3JD+/obU+oClRKVXZk6yUUscB\nMwHdz3vnAXGgliForVuB1j7FU0f6LIZhgGE6zYnSJyaEEFmVyebEq4DVSqmVQAtwGYBS6hFgpdZ6\nLfADpdQinAQVAS5NqZ2tVkpNxKkEtQMf0VpnJYuYhkXCMiAsfWJCCJFNGUtiWutNwMn9lH8g5fXl\ng5x/9hiFljbDdMnADiGEyAGyYscIGKazu7Os2CGEENklSWwEDMMCQ2piQgiRbbk2xH5cMEzLmfMs\nSUwIIYZlOEsPJo+7ELgOMAAbOFtrXT/QdaUmNgJmT5+YjE4UQojh6ll6cD5wO87SgwdQSi0GVgHn\naK2PBk4D2ga7qNTERsAwLDCR5kQhxGHn0UcfnbRixYqZfYpbk9OZ+pWy9OA5yaJ7gduUUuV9plp9\nDfhJz6h0rfWgCQzA6Jm8e5g5DXgu20EIIcR4s3TpUmprD5qie4PWetVA5ySnTq3RWh+VUrYBuERr\n/XpK2RvAw8D7gXzgfpxVnAZMVId1TWwkK3YANG1fTedT2wi/upv51XeNQWSjk2sz/fvK9fgg92OU\n+EYn1+OD3IuxZ8WOa6+99oIVK1as7fP2gLWwNFnAsTg1Ng/wGFADrBkwrkN048OKYbic3sR4HNu2\nnVU8hBDiMLBs2bK6ZcuW7UjztGEtPYiTsP6ktQ4DYaXUg8BJDJLEZGDHCJim5YybAUikX5MTQojD\nida6AehZehAGWHoQ+D1wrlLKUEq5gbOAdYNdW5LYCBiGhZ38zskIRSGEGJargC8rpd4Fvpz8GqXU\nI8lRiQB/ABqADThJ7x1g0D4baU4cAcO0MHqSmIxQFEKIIQ1z6cEE8PXkx7BITWwEDNOFbSYHy0gS\nE0KIrJEkNgLOPDEnifVdP7Fj7auEd/ftqxRCCDEWJImNgGmmJrEDa2IN96yh9R9PZiMsIYQ47EgS\nGwHDtMBIJrHYgUksEQmTCEeyEZYQQhx2JImNgGG4sJND7FNrYrZtY0ci2LFoliITQojDiySxEXBG\nJybnh6X0idlRJ3nZEamJCSFEJkgSGwHDtLCNg/vEepJXIio1MSGEyARJYiNgGpazwhcHJrFEJOyU\nSRITQoiMkCQ2AkZyPzE4cMWOnpqYNCcKIURmDHvFDqXUmcAOrfV2pdRk4IdAAvhWz94vh4uBVuxI\n9CQxqYkJIURGpLPs1C+A85Kvb0p+DgJ3Ah8Z6uThbE2tlFoFXA3sSRY9r7X+UvK9POC3wCIgBlyj\ntX4ojfgPGWeyc3J4Yr99YlITE0KITEgniVVqrWuUUi6cZDYDiLA/4QylZ2vqe5RSl+BsTb20n+PW\naK2v6af8GqBdaz1XKTUPeE4pNVdr3ZnGMxwShukC6+DmxN6aWERqYkIIkQnp9Im1K6UmAmcAG1KS\nh3uoE1O2pr43WXQvcIJSqjyN+1+Ek/hI1uDWAsuGce9ipdTM1I9nnnlmUhr3PYiZUhPrb3SizBMT\nQojMSKcmdivwKs5umyuSZe8DNg3j3GlArdY6DpDcFG1PsrzvfjKfUkqdC9QB12utX0yWTwd2phxX\nkzx/KCuA61MLqqurqaqqoqQkMIzTD9a819Wb/gsCbsrLC5wvvE6hHY3uL8uSbN9/KLkeH+R+jBLf\n6OR6fDA+Ysy2YScxrfWPlFJ/AeJa663J4lrgikMYzx3AjVrrqFLqHOBBpdQCrXXTKK55M3B3asHy\n5csXA/e1tHQRi6W/qaXPm99bE2tr7oTkFuJtTe2A08TYUN+GYWZn8GeubWveV67HB7kfo8Q3Orke\nH+RejC6XOeI//MdSWvuJaa3f7XmdHK2Y0Fo/O4xTh7U1deooR63135VSu4CjgWdxal4z2F9zmw48\nPYyYW4HWPsVThxHzgNzeQox+B3aE97+ORjG83tHcRgghxBCGXVVQSj2rlHpf8vV/4uzA+Xul1LeH\nOne4W1MrpSpTXh8HzAR0sug+YHnyvXnAicBjw43/UHJ7C3u/c/0NsQeZKyaEEJmQTk3saOCl5Osv\nAmcCHcDzwA+Gcf5VwGql1EqgBbgMnK2pgZVa67XAD5RSi4A4zsjHS1NqZz8G7lZKbUm+f6XWOit1\nbcvlw3B7gAP3E0tNXIlopGdRDyGEEGMknSRmArZSag5gaK03ACilSoZz8jC3pr58kPO7gAvSiHdM\nWV6nw3XAmphMeBZCiDGXThL7F3AbMBn4C0Ayoe0bg7hyXk8S62+ys/NakpgQQoy1dIbPfRZngMR6\nYFWy7Ajg54c2pPHB8hYBA9fEZNUOIYQYe+kMsW8Cvt2n7OFDHtE44fIWgHHgtiu2NCcKIURGpbMA\nsBv4L+BSnOHxe4Df4czrOuyqHZarAEwDO9Z/7cuWmpgQQoy5dPrE/gc4CWeU4U6cOVvXAYXA1w59\naLnNcheACYlosLfMjkQwXC7sWExqYkIIkQHpJLELgIUpq2dopdTrwDoOyyTmrNqRiIZ6y+xIBDM/\nn3hrKwkZ2CGEEGMunYEdRprl72lOTezAJJYIh7EC+YA0JwohRCakUxO7D/ibUuoG9i8B9V/J8sOO\n5S4AC+xoylJTkQhWYaHzWpoThRBizKWTxL6Bk7RuxxnYUYuz9NT3xiCunGdaHgzTJJGSxBLRCJ5k\nTSwhy04JIcSYGzSJKaX6blr5TPLDAOxk2WnAPw51YOOCZR0wOtGORDADzirPUhMTQoixN1RN7K4B\nynsSWE8ym33IIhpHDJeLRCylJhaJYPn9YBiSxIQQIgMGTWJa61mZCmQ8Ml0eEpEQiXgYw/Q4Q+y9\nHgyPR1axF0KIDMjOro3vEabHDwmbcGcNdiwGto3p8WK43Qes5CGEEGJsSBIbBdPthwSEOnf01rwM\njwfT7ZHmRCGEyABJYqNg+vzQDaH27b2jEQ2PB8PjlnliQgiRAZLERqHw1CUkWoKE3t5KvLsdANPt\nwXC5SUQizkcoNMRVhBBCjFQ688REHwUnnsy+B/9E7NVmQkfuAJLNiR6nObHhf39HtGkf0675z+wG\nKoQQWaaUmg+sBsqAJuAyrfXmAY5VwBvAL7TW1wx2XamJjYJhmpR9+OPYzVE6X3sFANPrwXC7saNR\nwrt3Ed3XmOUohRAiJ9wB3K61no+zaEZ1fwcppazkew8M56KSxEap8MSTwTIIbdoBgNEzOjESIdbS\nTKI7OPgFhBBiHHn00UcnKaVm9vkoHuwcpVQFcAJwb7LoXuAEpVR5P4d/E3gIeHc48Ri2bQ991HvP\nacBz2Q5CCCHGm6VLl1JbW9u3+Aat9aqBzlFKLQLWaK2PSinbAFyitX49pWwhcCtwJs5WX/lDNSdm\nrE9stO2hSqm7gbOBfcnD7tNa3ziamFpauojFEmmfV15eQGNjR+/Xe1bfRudzawGYccONND/8Nzpf\nX+vMHQPm3l6N6fWOJtRRxZdrcj0+yP0YJb7RyfX4IPdidLlMSkoCXHvttResWLFibZ+3W0d7/eTG\ny3cCn9Nax500MIy4RnvjNPS0h96jlLoEp82z79qMQ7WH/lBrfdvYhpm+vHnH9CYx05PsE0smMIBE\nsDujSUwIIcbKsmXL6pYtW7YjzdN2AZVKKSuZoCycheR3pRwzGZgDPJJMYMWAoZQq1FpfOdCFM5LE\nUtpDz0kW3QvcppQq11r3HfnQ0x6an/wY7b2Lcb4ZvaqrqydVVVWN9tK98uYt6H3dM08sVSIYhOKS\nQ3Y/IYQYT7TWDUqpN4GLgXuSn99I/f2vta4BJvR8rZRaRQ41J04DarXWcYBkJt6TLO99iGR76Hns\nbw/t6+tKqeXAVuBbWuuNw7j3CuD61ILq6mqqqqooKQmM6GHAqer3sCfkU5PnJdEdZsLkIkKFAdpS\nji30GhSkHJ8J5Rm+X7pyPT7I/RglvtHJ9fhgfMSYhquA1UqplUALcBmAUuoRYKXWum8T5bDkzDyx\nYbSHfgfYq7VOKKUuAx5TSs3uSYyDuBm4O7Vg+fLli4H7DlWfGIBnxjRCG7ewc8vzRGMHHt+0t4lQ\naebatnOtLb2vXI8Pcj9GiW90cj0+yL0Ye/rERkprvQk4uZ/yDwxw/KphxTXiiNIz6vZQrXXvcBit\n9Rql1M+AqcDOwW6stW7l4E7HqaN+oj4KF59GZF8d7fuex2/NdQotC+JxpzlRCCHEIZeReWJa6wag\npz0UBmgP1VpP0FrP1FrPxKlB/aqnQ08pVdlzrFLqPCCOs7t0Tig+o4rpK1dix8NEQrsB8EycBDgD\nO4QQQhx6mWxOHG176Gql1EQgAbQDH9Fax4Y4J6M8/gr8RYrOyBvO11MqieypJRGU9ROFEGIsZCyJ\njbY9VGt99thEdmjllx1Hh/kqAN4pU+gE4mnWxBKRCE0P3E/pBz6ElT/qAZpCCPGeJctOHWK+wrlY\nHj8ArrIJGF5f2n1i3RveoeWJx+h6e/1YhCiEEO8ZksQOMcMw8ZXMAiDmasby+0mE0ktioR3bnPNb\nRj0JXggh3tMkiY2BkkUfwH3cZDqN14mbQWIdLWmdH9q+HYBYa3rnCSHE4UaS2BjwFE9k5pd+QLm6\nGMNjEmreSqhjx7DOtW2b0PZkTUySmBBCDEqS2BgxDIO8IoW3dCZEoHPfa8M6L9pQT6LbGQgSa5Ek\nJoQQg5EkNsasvADELILtm7ETQ88I6KmFeafPkJqYEEIMQZLYGDP9fojY2IkIoY7tQx4f2rYNw+sl\n78ijiLW1YSfSXxZLCCEOF5LExpjl82OHoximh+62TUMeH9qxHd+MmbhLSyEeJ97RnoEohRBifJIk\nNsbMvDzsSARfYA7B1o10Na8fsFnRTiQI796Fd8ZMXCXO1i0yzL5/9Wt+y97qX2Q7DCFElkkSG2Om\n35n47DfmEd8SpGnnAzRu/z9s2z7o2Gh9HXYkgnfqNFzJ/cekX6x/4d27CdfuznYYQogskyQ2xkyf\nk8TaHnmG8CPbCbCQUPsWupoPXo0jvMtZ1N83fXpKTUySWH8SwaDsDiCEyJ39xN6rempiXW+/BUD0\nrRa8p0yjpfZxoqEGvIGp+IvmYxgWoV01YFl4Jk8B0wTTlJrYABKhIPFu2R1AiMOd1MTGWE8SIx7H\nzAvQ8fJLFFeci9tbSkfjK+zbfh973rmVYNtmwrtq8E6pxHC5MEwTV1GR1MQGEO8OYofD2LGc2shA\nCJFhksTGmNWTxICJl12OHQ4RXPcuk9QVTFv4LcpnfwrT8tG4/f8I7dyOd9r03uNdxSVSE+uHnUhg\nh53tbaRJUYjDmySxMWb68wDwVE4lf9GJeKZU0vHKy4CzWLC/aD4V8y7HihWS6OgkXtBJLOIMq5ck\n1r/UxCVNikIc3iSJjTEz4CSxwLELMQyDwDHHEtyymURo/0aZwbc2Ym1y9g2L+Buof/cuYuEWPFOn\nEtmzh4Y/3ivNZilSd8qWXbOFOLxJEhtjroJCJl91NaXLnL0/A0cfA/E43Zs2AhBrbWXP7bfQ/syz\nmHl5TF5yJXYiSsOWeyg65wyKl55N698fp/nxR7P5GDkldadsqYkJcXiTJJYBBYtPctZQBHxz52F4\nPHS98zYA3dpZxWPqN77FnJ/egq90JuVzPkM81kXjjj9QdsHHcE+ZTHDzu1mLP9ek7pSdkCQmxGFN\nkliGmW43eeoIupNJLKg3Yfr9+OfOw3A5Mx68gUrK51xMPNJK7ds/JZ7fQXDHJkKdO7MZes5I7RMb\nSRKT9SiFeO+QJJYFeUcdQ7Shnkh9Hd16E/558zHMA38UvvwZVMy9lMKKJQRmH43dGaX+rd9Q+9db\naX7qsRHfu+vt9bQ+84/RPkJWpfaDxdPsE4vU1bHl6isJ7951qMMSQmSBJLEsyD/hBAyXi4Z71hCt\nr8Ovjuj3OG/+NIorz6ZwwWkA+GJz6fr7G+y79w80PPc7wp27hrW9S6qWJ/9O04MPjPoZsim1Tyzd\nmli4dhd2LEZ4T+2hDksIkQUZW7FDKTUfWA2UAU3AZVrrzQMcq4A3gF9ora9JluUBvwUWATHgGq31\nQ5mI/VBzl5ZRsuyDNP/tQQDyjlgw6PHeqdMASGzogGACXCatf3qGoLkFI+DG459E4aTTyStSQ947\nuq+ReEc7iUgE0+MZ/cNkQU9NzPB40h7YEWtrAyDe0XHI4xJCZF4ma2J3ALdrrecDtwPV/R2klLKS\n7/WtLlwDtGut5wIfBn6tlMofw3jHVOmyD+IuL8f0+w+Y4NwfV1ERVkEhHWtfBdNk6oprMSIm8Yc6\nCHiOIRGPsG/7nwh37ca2E/0uLgxOX1Bs3z4AYs1Nh/yZxtrum/6Htn/9k3gwCJaFq7Ao7ZpYvNXZ\nFUC2uBHivSEjSUwpVQGcANybLLoXOEEpVd7P4d8EHgL6Dse7iGTiS9bg1gLLhnHvYqXUzNSPZ555\nZtIIH+WQMT0epnx5BVOu/vJB/WH98U6bBraNf9588o5YQOXXriHRGaR9zb+omHMpLnchDVt+x651\n/83Gl24mHusmHu0g2L6lN6nF29t655tFm/pPYvFgcMD3sine3UX3xg10b9xAIhjE8udh5uWlPU+s\nZ/K41MSEeG/IVHPiNKBWax0H0FrHlVJ7kuWNPQcppRYC5wFnAtf1ucZ0IHV4Xk3y/KGsAK5PLaiu\nrqaqqoqSkkC6z9GrvLxgxOfuv0j/fWH96Zw/h+4N7zDxfSc79y5fTF78Kt696WfktbagTryCvdue\nxHLlsa/2ZZq2/Y5ouJ14LN2WNiYAACAASURBVEj5tCVMUx+hY9/+fiBfpLPfZ9h25x/Z98KLnPib\nXw0ruY5Uut+/js31ABgdbbi9btz5eXiLC0lEw2ldq6G7EwBXJDjkeYfkZzyGJL7RyfX4YHzEmG05\ns4q9UsoN3Al8LpnkDtWlbwbuTi1Yvnz5YuC+lpYuYrH0h1uXlxfQ2Jjhv+SnznKG4M89qvfeiVkK\nw+Vi99PPUTHxM+RP+jAAs8vmsvXNNc4K+cWTaNz1Ap3tzXjqK3sv17KzFqufZ2jdvI1oSyu16zXe\nyqlj8igj+f61620AdNc1EHd5sD0+4i4PkaaWtK7V3eA0p3bvG/y8rPyM+9FTc+6ZftEjV+IbiMQ3\nerkWo8tljuoP/7GSqSS2C6hUSlnJBGUBU5LlPSYDc4BHkgmsGDCUUoVa6ytxal4z2F9zmw48PdSN\ntdatQN/tkcfmt/MYCiw8jtk3/RwrsP8fkenzkXfkUXS+8TrlF30awzBo+OO9eOIRKi/4OqbL2VXa\nbojSWf8a3XqDc6LfIlS3o9/7ROqdGk/w3XfHLImNRLShAXCaA10lJZh+P6Y/L+0+sVhbT59Y7vxy\nGMzeX1cDBlOuujrboQiRkzKSxLTWDUqpN4GLgXuSn9/QWjemHFMDTOj5Wim1CsjvGZ0I3AcsB9Yq\npeYBJyavc1gwDOOABNYj/4RFdK1fR3hXDa6SEtqefgrD5aLoU5fSufZV55dgPI7v6NlE440YAQ9m\nsZfgns3Uvv0zXN4SSqd9GJe3lEhbLfHkL/ng5ncpPnPpmD1PqGYnpt+Pp7xiWMdH6uucF4kEkb17\n8c+fj5mXl9boxEQ0QqKrC4BY5/hIYqHt2zDd43MUqRCZkMnmxKuA1UqplUALcBmAUuoRYKXWeu0Q\n5/8YuFsptQWIA1dqrcfHb6IxFFh4HBgGbc89i7tsAnYshh2LEW2op/P1tViBAL6Zs+jetBFP5VSM\nSSauCWV063ew2kvofOY1wmfU4imrIFSzw7mo26Br0zpikQ5idc0kurrwTJmCq6j4kMQc6+5m909+\nhGfiJKZ/Z+Wwzok21INlQTxOorsLy5+HlZeHHQ5hx+MYljXkNeKtzvB6V2kpsZYW7ERiWP1+8e4u\n2l94geIzl/beJ9rYSHD7VgpPOmVY8Y+EHYsRa27G9PnG7B5CjHcZS2Ja603Ayf2Uf2CA41f1+boL\nuGBMghvHXAWFFFUtpe3ppzD9flwlJcRaWgjt3Elw61b884+g6P1nOLW17dsoOPkU3GUT6GzvJv56\nG/FtHSSao1gXBMizjiJCLb5j5xJ6bTM7fvyfJLY7NR3f/LlMuPIT+ApmYVoH/1KNB4PUfHclpR/6\nCEXvO33QmPc+/CiJ7m5C27cR3rXLGXk5hEh9Pf45cwm+qwF6mxPBWYbKyh96tkUsObzeO3UaseZm\n4p2duAoLhzyv46UXafzD/+IuLyd/4XEAND38N9r/9U/yj12I6fMPcYWRiTY1gW2TCAZJhMOYXu+Y\n3EeI8UxW7HgPqLjoYvIWHEUiGGTCv30Sw+2ma92bxJqb8M+ZQ958hZnn/MJ3TyjHVVYGiQRd694k\n76ijMcIGvGxjBZ1EUH7OhQAktnfjXjQRc26A0LatNG75P/ZuurPfNRxD27cRbWyk4XerCW7b1lse\n77NpZSIcZs+Df3OW2nK5aHvu2SGfL97ZSaKri7wFR/aWmX4/VvKZhmpSjNTX0/zIQ73D63vm5Q13\nrlioxnnejrWv9Jb1LMgc2bt3WNcYiei+3tb23knaQoxXSqn5SqkXlVLvJj/P6+eY65RS7yil1iul\nXlNKnTfUdSWJvQcYLheTr/53Jn3xKgpOOZXAzBl0vPYqAL45zsLCgWMXAuCeMAF3WZlzom0z4WMf\nd2pq77xNaOcOrOJifLPn4Js9h5LzP8CM5T+g6MQzIGZT5K8CoGHzahq2/C8d+96gbdsL1G++m31r\n7wPAKixk7x23EQt20vnWOrZ+9UsEt23tjbX9pReJdXQw4ZMXkn/CItpfeoFEJDLo80UanMEm3mnT\nMZP9gqbf35uYhxrc0frU39l3/5/ofH1t8jpOzW+4gzvCNTUAdL35BolohFhbG9FkH114z55Bz02E\nwyPeCy7a2ND7uqevUohxbDgLXrwCnKi1Phb4PPBHpdSgTR2SxN4jLL+fwpNPwTBNArNnQzyO4Xbj\nm+7UOgoWnQiAZ9IUXKXO+Bn3xIl4Z84i/4TFEI/T9eYbeComYpgm0799HeWfvBDTtCg6psq5yb4E\nk49YTtHkpUSCdex76l7qf3An4d11xPa2YRS68J4/nVhzMzX33kD9H++CRIL255/rjTO0fSvuokJ8\ns+dQdPoZJLq76Xpr3aDPFk2OmHRXTMRd6iRgMznZGYbeGLNnu5uOta+CZeGZOBkYXhKzYzHCtbvx\nTp9BIhik++23CW7ZPw8/snfgJJaIRNi56jrq71l9QHnH2ld6mzYHE21MrYlJEhO54dFHH53UdwEJ\npdSgHebDXfBCa/241rrnP/R6wMBZqnBAxkBLFL3HnQY8N+RRQgghDrB06VJqaw9aQPuGvuMYUiml\nFgFrtNZHpZRtAC7RWr8+wDmXA1/VWp8wWDw5M9k5G8bVZOc0+FrrWX/Nf1Jy3vmUX/Cpfo+JtbZi\nFRb2js6r/981tD39DyZ88kJKzz94rE3tz39KpL6eRCRMvLWVojOX0vb0P8A0ncEkTU1M+MSFFJ39\nfrq2rKfupjsxi/xYSwqJPlqP76PzoNIidMcmrGOLqLz8a5iWh123/hi7PsLsn9yKaZrsueMXdK59\nhbKPfZyyD30E27bZ8V/fwgoEmP7t62j4/e9o/cdTVK74Op7KaWy/9muUf/oSSpae3e9zdqx9lb13\n3E7pBz9M88N/wzdnLtO+8S02L/8CpR/+KBM++m/9ntfzM27713PU330XM7//33S8+gpND/4Fw+PB\nN3sOVn4B4Z3bmfXfPz7o/Eh9HTtvWIkVyCfW0szUa/4TvzqCXT/4HqHt2/BOm86M67876M9x53ev\nxyospHvjBkrOPZ/yT+wf15Tr/wYlvtHLtRh7Jjtfe+21F6xYsaLvaPJD2lSglDoD+B5wzpBxHcob\ni9wQmDWTwtPfT+GSgUcJuooPrP0XLDqRtqf/MeAEZ9/sOXS9tR4A96RJtD39DwyXi5Jzz6f5EWcz\nAd/MmVjufAoXLCFxSQhXeSmx4lb2PXcfrtoSyo7/MDsT1+GZUkLL7kex7QTGVDfxzS3Uv1SNK1xO\n59pXcE2YQNMD9+OZNAkrkE+0vo6yz3/RiTulOdFVXIxVXExoy2YYIIl1600YXi+lH/gQbf98FndZ\nGYZlYQYCxIcxVyxcsxPD68VdMZHSD36Y0PZtdK1f17sHXOdrrx4wctCOxdh75y/pfON1DLebyq9+\njZrv30DnujeJd3US2r4Nz5RKwrt3Ee/q6p37Z8diB63KEd3XiG/OXFx7ansXLhYi25YtW1a3bNmy\nHWmeNpwFLwBQSp2KM5/4o1prPdSFpU/sPch0uZh0+efxVlYOfXBS3hELmP5fq8g7+ph+3/fNmuUc\nd/SxVH75axguF/knLKbwtPf3HuOdMaP3dXHVUvKPOo7iyioKTzqVrjdfp3vTRgCmnPQhoqFGYuEm\nKt7/GQC6n9tEy30PY0z0Yn08H2Oyj713/pL636/B8HsJTdhJc81DdNvOqiMJTwjDMJxdsvUmEokY\n3e9qIimDIQCCeiP+ufMwvV6mXvtNJiRrplZBQW+fWHj3Lvb+uppEKERf4V01eKdOwzBNDNNk0hXL\nKTrjTApPfR+eyVPAtvdPxAaCWzbT+fprFFWdycwbbsQ7dRr+I46k45WXqL/7N3imTqPi05eAbfeO\ncOxcv44tX/13Ote92XudeFcXie5uPOXluIqKpU9MjGta6wagZ8EL6GfBCwCl1InAH4FPDtTM2JfU\nxEQv38yZA77nn6coOHUJZR/8MJ6JE5l+3Q1OTSgQwDOlEjsew8rrf121otPPoPUfT9L80N8wXC4q\njjmd9reasTyF5FcsxDN1GpHNu3BVTKDs8x/DyLeIXr6P5t8+THRvHdbxRcRizURaa7FmFeP/3AJa\nW5/AO3E6/vnz6Xj5JXY98QPCD9TgKiykaPlZ+BKzaH7wb0T27KHotDMA8E6Z0huTq6CwN4m1PPV3\nOl56EVdhEeUXOkmu9c117P7Dnwlu2UzxWftbNKy8PCZeejkAdiwKQGRPLb7pTgLvevstsCzKP3FB\n7/yx/IXH0f32etwTyqn8ytewCgow3G669SY8kyZT96s7sMMh9t3/JwLHHIthmr2DOtzl5VhFRQcM\n8hBinBrOghe/APxAdcr6uZdqrd8a6KKSxMSwmF4vk79wZe/XqbW8SZ+/AjsaHfBc77RpeGfOIrxj\nO96ZszBdLkqmntv7fum559O5/k0mXnL5/knLE6HgGyfS+PAfKF56LoFJ++eIRYL11L/7W/ZuugPD\n7fwTjjyxC2IxYi3NNP/mr9iNEaz8QsovupjiM89yjumuw3T5cHmKMQM+InV12PG40/TnctHy5BO4\nJ00iqDfR8fJLuMrKKP3QRyg5p/+pKp6KiWBZNP3tr3Rv3Ej5BRfR9dZ6/PPmHzABuuDkU4g2NlC8\n9CzcpaUA+ObMpfO1V+l49WUMy8WEj3+ydxpAweKTCCfnprknVOAqKia4pd/9Y4UYN4az4IXW+sR0\nrytJTIyab+asIY8pOv39NOzY3u/qHIVL3kfhkvcdVO4pmkTlp1ccXO6fSMXcS+nc9xph917MwgCJ\n9i7MGXkY5R7ia1sxp/vJ//giCo84nXiii7aaf9LV9AaG4SJQupCwbw+xun3U/2U1ic5OJl7+Ofbd\n/yca1tyN4XIx9YJP4Ft63qDrFhouFyXnnEfwXU37i88Tb28jUrubCRdcdMBxVl5ebw2vR546gqZN\nG/FMqWTSFVfinTqN9hdfYN9f/oxv9hyaHnoQ77TpeCorcRUXk+js7LffbDwK1ewksns3BaecOqbb\n/YjDw/j/HyHGhYKTTqHliccIDNDnli5voBJvwKkN7l1wBx0vv4TvpNnkLTgK/1lziRY20br3Sfa8\ncwt2IgKYFFScQizUTGfTawSWHE372y/Q/tg/MTwufAsVFZWfhZBBYO7RTKws6x0ZZts28UgrLm/J\nQXGUfvSDmNbH2Xf//bQ89ggAgaOPHTL+4rPPdZaxWnQiptsNQMVnLqX25pvYcd13sMMhJn/xKgzT\nxCoqApxVO3onqueYRDhMx8sv0b1pAxM+/kkYYB+saFMTtT/9CfHODtpe+BeTPv/F3tqpECMhSUxk\nhOX3M+vGH43JtYvPOgdXcTFl51yImfzL3s88fIWzaa97DpevlPzS43F5i7Ftm1ikBZenBN8Fs6j/\n7W8xpnup33qnczHDItbYBLFygiGLvJKjaN71MF1Nb1BceQ6FFaf23renWdPtr6D8gxc5y1LZNp6U\nvrfBvh+Fpyw5oCzviAVM+dJX2HP7LRScugT/vPnA/pGksdaWg5KYnXCmiPTUaCL19dT9uppoY6Oz\n83V3N57Jk5l81dW9Czi3PPV37HCY4rPOOWg9xkQ0QvsLzxOprcVwu8hfdCL+2XMAZ2mxrnfepvDU\nJbhKy4g2NuIqKSGydw97bv05sZZmMAxCO3dSduMqujdtp/WpJwnt3EHhqUvwVk6l+bFHsOMxyv7t\nEzQ//Dd2rvovSs//AOHdu3EVF1P64Y9i+cdmLcr+2LbtLAYdDjlrjxaXHPR+ZE8t2DZWQQFWYRGG\nYYz4fpG6OjBNPBXD271hoJjjbW1Y+fm9NXPbtkl0dWEGAqOKbzw6rCc75/o8sbvuquayyz6PO/mX\n+nCVlxfw3HMv88c//p7rr/9+WufeeOMqjjhiAZ/4xEVDHzxCuTL/xbZtmh/6K74jZ5EoDOPyFNHd\nuoGu5vW9x1juAuLRDlzeMmLhJvInLMaXPxMMi5bax7DjERLxIHklx1DoPwU7lkhrVGh/Yq2tzuCP\n5Ir5oZ07qPneKiou/SzFZ1QBUOxJsOUP99P2wr+wAvlMveYbhHfuYO+dd4BhULDoRBKhIKbPR/vL\nL+EqLGLq168l1tHBrv/+nvNLubiYiZd+lrwFR9L+/L8IbtlM96YNxNvaMP1+7GgUOxZzdj9wuQjv\n3AGA4fHgKikhWl+P6fNhJxJYgXwmfeGLGJbF7p/+uLeP1MwL4Js5k+6NG8C2nSXSrvoS+ccdT6S+\njrpf30lo+zZntGhnJ67iYio+cxn5xx2PHYvRvWkD4ZoaAsedgB2L0v7C8/hmzSLviCOdbWp8PlxF\nRbT8/XHCu3dh5gVIdHf3DtrxzZlD2Uf/7aAtf8qKfWz69Ro6Xn7JSb5JrgkTsPILIJHAKiwiWrf3\ngDUsTb+fvCOOJHDssQS3bSURDOKbPgPD5SYe7CbW1IThdmH68+h6+y0SoSAFxy/CVVJCaPs2Ol51\n1t/MO/pYXCXFYNuQSDg7KrhceKdMBcsk2tBAfkkBYcuLVViEFQhgx2J0vraWrg1vE29rc1afmTwF\nV3Ex0bo6ovsaMf1+pxl6SiWx1hZiTU24SkudNVNLSrAjEbzTppN//KDzh/uVsinm6cC/0r7AGJEk\nlsNJ7LTTFvPEE/8kL7m8UqpYLIZrgP6R0cR3OCWxgcSjXUwoL2T39nW01j6Jv/gISirPobnmIbqa\n3wJ6aj9uKuZdTqh9K217n8a0/OQVL8CbPwOXtwSXpwTLPfqdcBPRCDXf/y6Rur2UnrcMLIv2p58i\n1tVF3pFHEXxXYxUUEmtpxjt1KlOu/gru8v2r+QS3bqH25z/DcLux8vOJd3Yy6XNfoPFP/0dk9y7M\nQIBEVxeuklK8M2dSctY5+NUR2OEwbf98hu6NG7BjMfzqCPKPX0Tzow8Rb28ncOxCIntqiXcHqfjU\nxb21mOCWzRg1W4kWlxM48ihMn4/ovkYSwRDuiooDan92PE60sRF3RQWhHdupX/1bIrW78UyaTLS5\nCbvvupqG4fzi78NwufDNnefsaJAXwCoowE7E6Vq/DjsaxSoowFVUhFVYhKuomPje3XRt30Hg+BMI\nLDgSMz+feFsbwS2be+8Za2vDKiykYNFiTH8esWSfZ+frrxPvaHcWoQ7k709yhoGruBg7GiPe1elM\n7fD76XrnbWcZOI+HkrPPBcui48UXSMSiGIbpPJNpYIcjvYtSG14fxKLY8fgBz2n6/QSOWYhv1ixi\nbW1E9tQSa23FVVKCf85cos3NhHfVENlTi1VUhHtCBbGWZqL79mGHnSkk+YtPGtEmq5LEckvOJ7Gb\nbvoRf/nLfcyZMxfDMLn11mpuueUmLMuipmYn3d3d3H3377nhhv+ipmYn0WiEysppfOtbK5kzp5LH\nH3+a22//OXfd9Tv27t3DFVdcykc+8nFeeul5QqEQ3/zmShYmtxVJlZrEuru7ufnmH7Nx4zsAnH/+\nB/nMZ5zh5b/5zZ08+eTjeDxeDANuuaUat9vN979/PTt2bMOyXEyfPoPvfe+HWfn+jVZPjLZtH9A8\nYydiREON2Ni43IVY7nxs2ybcsZ3OptcJtm9J9sEBGOQVH4mvYCaJeJhw124S8SBubxkYFoZhEShb\niMc/0bm2bYOdwDAtoqEmuprXU1BxCpbLTzwYpO5Xd9C13llnsvDooyi58NN4p1TS9c7b7Ln9FvKP\nO4GJl3+u3y1bwrW72f3TnxBva2XSF5dTePKpJKJRmh78C+FdNZR+8MPkzVcHnTfa71+67FiMlice\nc6YfTJ5C3hEL8E6fTscrLwPOdI3Qtq2Ed+/CN2cuiVCQaF09+YtP7LdvLdbaQtu/niPW3EysrZVY\nW5vTFOe2KLvw0+Qfd/yIYozs3Ytn8mQMl8vZqcFOYHq8+5v3Uva4S4TD2PHYAe8PJNbWBnYCq6iY\n8gn51O+sJ9beTqK7Czsexzd7Tm8falox2zaJUAjT4xnW3nv9kSSWW4ZMYp1N6+hqfrPf99xui2g0\n3u97wxEoPY78soVDB9mnJnbjjavYtm0rt912J/5kv0FrayvFyT6TO+/8BfF4nJUrv31QErvggo/w\nox/9jPe973SeeOJR/vKX+/jlL39z0D1Tk9gvfnELzc1NfOc7q+ju7mL58s/zpS99laOOOpoLL/wo\nDz74GF6vj+7uLjweL88//xwPPvhnfvrT2wBob2+nsJ/9usZTEkuXbSeIBhuIR9sJde6kc99rvUnN\n8hRjufOJhZoASCQiYMfxFy+gZMrZNO96iHD3XgrLT6Zj36skYt24vKWUz74It6+8t9/DcLuZOHXC\nAfElIhFMz+A7QEebmgjqTRScumTM+01y/Wec6/FB7sWYq0lMBnaMM1VVZ/UmMIDHHnuIJ554jFgs\nSjAYYlpyr6y+/P483pfcrPKoo47htttuHvJea9e+wle/eg2GYRAI5HP22eeydu0rnHTSKVRWTuN7\n37uek046hSVLTicvL8DcufPYsWM7N930I44/fhFLlpx2aB56HDEME0/eJGAS/qL5FE2uIhELYpgu\nLNeBzcLxWJDOxldoq3uOYOtGMEw8/sm01T2L5S6ibMa/0VL7OE07/8ok9QUMwxhw88+hEhiAu6wM\ndz9TGYQYzySJDSC/bOGAtaVs/oWUl7c/ga1b9wYPPPBnfvnL31BSUsITTzzGX/96f7/neTz7myBM\n0yQeH9keVwCWZVFd/Vveemsdr7++li984RJuuulW5s6dxz33/B9r177KSy89z5133s7q1X/Aexjv\nSGyabkxP/80/lstP0eQz8BXOoa3uOQorTsWbP4Nw53bcvgqn1hZpoW3vM8SjXYekf02I9xqZaZjD\n8vICdHV1Dvh+R0cHgUA+RUVFRCIRHn74r4f0/osXn8TDDz+Ibdt0d3fx1FNPcOKJJ9Pd3UVrayvH\nH7+IL3xhObNnz2Hbtq00NNRjmhbvf38VX/nKf9Da2kLHMHdPPpx5A1OpmHMxvoKZGIaBr2A2ltup\ncfkL5wIQ6tg62CWEOGxJTSyHfepTn+ErX7kKr9fHrbcevAnqKacs4YknHuXiiz9OUVExxx13PBs2\nvHPI7v/Zz17Bz372P1x2mTNS8bzzPsAppyyhoaGe73znG0QiYRKJBPPnH8EZZ5zJ66+v5Y47nP6w\nRCLOJZd8lgkTyge7hRiC2z8Z0xUg2L6FQOnQk6iFONxkbGCHUmo+sBpnl84m4DKt9eY+x3wO+BrO\nGGYL+JXW+pbke6uAq4GerXSf11p/aYTh5PzoxNGQ+EYvl2Lct+MBQu2bqTzmP5wh2eRWfP2R+EYv\n12LM1YEdmWxOvAO4XWs9H7gdOLhqAX8GFmqtjwOWAP+hlEr983ON1vq45MdIE5gQ44q/cC6JeJBI\n10G76Qpx2MtIc6JSqgI4gf27dN4L3KaUKk/dT0ZrndqBkge4gVFVFZVSxcABO0BWV1dPqqqqGs1l\nhcgYf+EcDMtL696nqZh76WG3rJAQg8lUn9g0oFZrHQdI7uy5J1ned1O0jwD/DcwBvtVnH5lPKaXO\nBeqA67XWLw7j3iuA61MLqqurqaqq6qkaj0j5AAuc5gqJb/RyJ8YCzNgHqdl4P2ZUM6HS2a0id+Lr\nn8Q3euMhxmzLuYEdWuu/An9VSk0HHlBKPZLcovoO4EatdVQpdQ7woFJqgda6aYhL3gzcnVqwfPny\nxcB90ieWHbkeH+RejLb3KLyBV6nZ+BfaW9soLC6iYfd64tEOTMuLN386+RMWHzQXLVty7fvXV67H\nB7kXY0qfWE7JVBLbBVQqpaxkLcwCpiTL+6W1rlFKvQJ8yPlS16W893el1C7gaODZwW6stW4F+u7t\nPnWEzyFEVhiGQdnMT9Bc81daah+npdZZAcTlKSEe66Jt7zN0Nb9F+eyLiEfaMUw3bt8EQp01JOIh\nfAUzwTCx4xFcniIMM+f+fhViRDLyL1lr3aCUehO4GLgn+fmN1P4wgGTNamPy9QTgTOD+5NeVWuva\n5OvjgJmAzkT8QuQCl6eQ8jmfIdyxnZKyIrojpb39Y6HOnTRu/QN7N/5iWNey3EW4fWUUTT6zd1+2\nVLZtEws34/KWSh+cyGmZ/HPsKmC1Umol0AJcBqCUegRYqbVeC1yZ7POKAgZwm9b6ieT5P1BKLQLi\nQAS4NLV2JsThwDAMfIWzyS8uIJjS1OTLn8HE+Z8j2LYJj38yth0nGmrEG6jEtPIId9WAYWIYbuKR\nVqLhZsKdO6jffDeB0oXEo+0kYkHAxuUtIxqsJxpqIK/kaEoqz6Vj36tgJ3D7J+LxT8TlLQXbprt1\nA+HOXZiuPPKKFySX3BIic2QB4PdYn9i///uVXHXVlRx99OIDyntWsn/44aeyFNl+ufz965HrMR6K\n+OKxbpp2Pki4Yzsu3wQsVwBIEA01YbkCuPMm09X0Os7fkyQ/p/x/MSyw4xiWFzseAcOkpPIcLHcR\nRHfStOcNXJ5CvAWz8BXMIhHtIty1C1/BHLyBqUSCdbi8Jbh9FRmv7eX6zxdyL8ZcnScmDeNCHKYs\nVx4Vcy4+aLuZVN7AVELtWyicdDpubxnR0D4iwXrikVYSiTC+gjn4CmaTiAdp2vHn/9/e3UdHVZ8J\nHP/eO2+ZZPJGAgRCSCDADxSrDdRFoOix7Slw0K20W9SKrXr2WDkrttstXbfKEVTEWmwLFrDEHnuW\nrp7K9rAWheqx5SzWw8Li1vrWn2h5CxpMyHsmk8m87B/3JgxhMuElzMwlz+ccDpnf3Dvz3MyTPPnd\n+c19aK7bBYBhuvEXTiUW6aKz8U06GvbZ4146m946PQ5PPm5vMb78SgpGzsJ0p6+zs3A+KWIDaHvj\nT7S+/t9J76v3uukJn/8FdAvnzqNgkKuJP/tsLW1trSxf/j0AWltbuPXWr7Jt2w7effdttmzZRDjc\nTTQa5fbb7+SLX/zyOcWwd+8bPP30U8RiMYqKivn+9/+NceMqOHr0MI8+uopQKEQsFmXBghu49dal\n7Nmzmy1bNmGaLqLRa4lpHAAADblJREFUCN/97gpqamYO/kQi66WaBQVKriJQcqrvnDe3LOkpQ5c7\n13q/ruMIhsvHmPJKmpqt7s7xWITu4HFM04fHP5pQ24dEwi14/WX0hBoIdRwhEm6mrX4P7Q37KalY\nRG7xZUN/oOKSJEUsS82fv4i77/4my5bdh9vt5tVXdzFnzjz8fj9Tpkxl48ZaXC4XTU0nueuupVx9\n9TVJe3cl09zcxCOPrGTDhl8wYcJEduzYzqpVD7Bly6/47W+3MXfuPJYuvQOweoIB1NY+zYoVP2T6\n9M8QjUYJhbou2rELZzIMk5z8CQC43DlYb21bs7KcQGXfdv7CyX1f+wIVBEprAAgH62mqe5nGw9vI\nD86msOzzRMItdLUeJL90JqY7h3gsYjcUlcUmwiJFbAAFs+cMOFtKx7nqsrIyqqqq2bv3T8ydey0v\nv7yD5cv/GYCWlmYee2w1dXVHcbnctLW1cvToEaZPv+KsHvvdd9+hunoKEyZMBGDhwhtZt+5xgsFO\nrrrqs2zcuJ5QKERNzcy+2daMGTNZv/5JrrvuembNms3EiZMuzoGLYcubW8boSbfTVLeL9k/foOPk\nAeLRbgA6Th7Al1tOsOV9XJ58fHkVuLwFGIbVpdjtLcIw3UTCLcRjEUxXDnkjrgTDoLvjCG5fCZ6c\nkacVv2CrxjQ95ORPzMjxiqEhRSyLLVy4iJ07dzBmTDmdnR1ceaXVSn3durXMmTOPNWuewDAMbr55\nMeFw95A853XXfYHp0z/Dvn172br1WV566UVWrnyY5cu/x0cffciBA/t58MF/ZcmSb3DjjTcNyXMK\n0csw3ZSMX0Sg5LO0N/wPbm8RvkAlTcdeoqvtIwKlM4lGOugOHifWqonHezusJy5QM4EYrZ/sJk4c\nercxXLg8+eQEqgh+6qXxuPU+XcHouZjuXOKxMP6CKfR0NxJq+5BYNEQsGu7rzA0GGAa5hYr8Udf0\nXYxZZJYUsSx27bXXs2HDkzz//FYWLFjU91dke3s7Y8aMwTAM9u/fy/HjA35mPKnLL7+CtWtXc+TI\nYSorq9i5cweTJytyc/OoqzvG2LHlLFx4A+PGVbBmzWoAjh49THX1JKqrJ9HVFeT999+TIiYuGl9e\nOb68xX23x152L8RjGKbrjG3j8RjRcCvxeAS3txjDdNMTaqC9YT+G4cJfNJVIdzM9oQai4VaCLe/R\n2RQmf+QsopFO2k6cWmjX+sluAEx3Hi5PANP0YrpysFZmxolFQ7R8/BrBlvfx5o3D4ysht2gqLk8+\nsWiIUPshero+JRrpxO0bgddfhtdfBoZJJNxMV+tBDMPAmzeOaLiNcFc9PaFGAqU15BaqM44t3HWC\n9oZ9uL2FBEpq6A4eJ9rdgmF6MEyPNSsNVJ716dVYNAzEMV2XTqNaKWJZLCcnxz6V+Dt+85tTDS/v\nueefWLfucZ555hdMm3YZ1dWTUzzKmYqLi3nggdWsWvVDotEoRUXFrFz5MAB/+MOrvPLKLjweN4Zh\ncN991sKSTZue6jt9GQgEuP/+lUN3oEIMwjAMa0l/0vtM3L7i08Y8OSMZUbHw1EDCe3KxWA9FBQZt\nHW7i8TgFo2ZhegIYGHS1HcTtLcQXmJC0MMTjcYLNb9Nav4fOpreIR7tprtuJYbjtWaE1IzRMH/HY\nWZwdMVyYrhwaD/2N0sqb6Gz6C+GuTzAMNx8TIRJutx870ldg+3N7i3F5C4h0N1sxxGPEieFyB3B5\n8oj2dGK6fLi9xXS1HcTrH83oKXcMHptDyOfELrHPiYHENxSyPUaJ78IMVXw9XQ10tX1ANBK0FrDk\nT8SXW45huon2dFgzra4TgIHpzsNfYL3/1h38GLe3CE9OKbFoNyf0M0TCzRimh9yiacTjcXJzc+mJ\n5REonUE03EKwVePLq8DrLyMejxCP9RDuqqej8U17FjoC0/SAYYJhEA23EYsEMT15xCJBekIn8RdM\nJH/UbLz+Ued8rPI5MSGEuMR4/CPx+JN3L3d5Avg9k/AXnLkIKvHUofXxhFtob/xfCkbOwu2zOkcl\nFlqX2483d8yZz59TSl7x9KE4FMeSIiaEEBnmySllxLj5mQ7DkWR5jRBCCMeSIiaEEMKxpIgJIYRw\nLCliQgghHEuKmBBCCMeS1YlCCCEuOqXUFOBXQAlwErhda32w3zYuYD0wH+uT42u11rWpHldmYkII\nIdJhM/BzrfUU4OfA00m2+QYwCZgMXAM8pJSqSvWgw3Um5gFwuc6/hrvd2V3/Jb4Ll+0xSnwXJtvj\ng+yKsff35WuvvTZu2bJlVf3ubtFatwy0r1JqFFADfMkeeg54Sik1UmvdkLDpEmCL1joGNCiltgP/\nADwx0GMP1yI2FaCg4Pw7yNqXX8laEt+Fy/YYJb4Lk+3xQXbGWFtb+1yS4VXAQyl2qwCOa62jAFrr\nqFLqY3s8sYiNB44k3D5qbzOg4VrEttr//5Xezn1CCCFS8dTX19cUFhb+Hujod9+As7CLbbgWsXZg\nU6aDEEIIJykrK/vj5s2bz2fXY0C5Usplz8JcwFh7PNFRoBLYb9/uPzM7Q/accBVCCHFJ0lp/CvwZ\nuMUeugX4v37vhwG8APyjUspUSo0EvgJsS/XYUsSEEEKkw7eBe5VSHwD32rdRSr2slJppb/PvwN+A\ng8BeYLXW+lCqBx2u/cSEEEJcAmQmJoQQwrGkiAkhhHAsKWJCCCEcS4qYEEIIx5IiJoQQwrGkiAkh\nhHAsKWJCCCEcS4qYEEIIxxqu1048L2fT1C3N8ZRgfcK9Gghjfcr9bq11g1IqDrwNxOzNl2qt385A\njIeBkP0P4Ada698rpWZh9RPyA4eB2+xL06Qztipge8JQEVCgtR4xUNxpiuvHwFeBKuAKrfU79viA\n+ZfO3EwWX6pctPdJWz6m+P4dZoDXNJ35OMD3r4oBcnGw2Ic7KWLnprep21al1G1YSX99BuOJAz/S\nWu8GUEo9AawF7rLvn6217n+16Uz4Wu8vEgCllInVSeBbWuvXlVIPYMV9ZzqD0lofBq5KiOunnP4z\ncVrcabQd+Bmwp994qvxLZ24mi2+wXIT05eNA3z9I8ppmIB/PiO8schEyl49ZTU4nnqWEpm69vXSe\nA2rsi1RmhNa6qfeXhm0v1hWgs90MIKS1ft2+vRn4egbjQSnlxeoq+8tMxgGgtX5da33a1b1T5V+6\nczNZfNmUi8niG0Ra83Gw+LIpF51AitjZO6OpG9Db1C3j7L8m7wFeTBjerZT6s1LqMaWUL0OhAfxa\nKfUXpdRGpVQR/doraK0bAVMpNSJjEcKNWK/vmwlj/ePOpFT5l1W5OUAuQnbkY7LXNNvyMVkuQnbl\nY9aQInbp2IDVqO4p+/Z4rfVMYB5wGfBghuL6vNb6SuBzgJEQX7a5k9P/8nVK3Nmofy5CduSjU17T\n/rkIzok97aSInb2+pm4AKZq6pZ39RvFkYInWOgbQe7pCa90G1AJzMhFbQhzdwEY7jt7GdwAopUqB\nmNa6KRMxKqXKgWuBX/eODRB3JqXKv6zJzWS5CNmRjyle06zJx2S5CFmZj1lDithZOoembmmllFqD\ndU7/K3aCo5QqVkr57a/dwNewYk93bHlKqUL7awO42Y7jAOBXSs21N/02VjO8TPkm8JLW+iSkjDtj\nUuVftuRmsly0xzOej4O8ptmUj6flImRnPmYT6Sd2DpRSU7GWMRcDzVjLmHUG47kceAf4AOiyhw8B\nP8JanRYHPMAbwHfSvVJRKTUR+E/AZf97D1iutf5EKTXbjjGHU0uaT6QzvoQ4P7Dj2jVY3GmKZz2w\nGCgDGoGTWuvLU+VfOnMzWXxYCyHOyEWt9U1KqWtIYz4OEN8NpHhN05mPA72+9n2n5aI9ltF8zHZS\nxIQQQjiWnE4UQgjhWFLEhBBCOJYUMSGEEI4lRUwIIYRjSRETQgjhWHIBYCEcyr7y+SHAo7WOZDgc\nITJCZmJCCCEcS4qYEEIIx5IPOwsxhJRSY7EugDsP6yK4P9Far1dKPQRMB6LAQqymkXdord+y95sG\nbMLqKXUcuF9r/aJ9nx94BOtyTUVYzSW/BIzGOp34LeBhINd+vkfTcaxCZAOZiQkxROwWJL8D3gLK\ngS8A31FKfdne5O+xrsk3AvgPYLtSyqOU8tj7vQKMAu7Faruh7P1+jHVNwtn2vis41SEZYC6g7Odb\naRdEIYYFmYkJMUSUUn8HvKC1Hp8wdj8wBatf1Xyt9Sx73MSacfU2X3wBGNt75Xel1HOABlYDncCs\n3llbwmNXYc3EKrTWdfbYPuBJrfXzF+s4hcgmsjpRiKFTCYxVSrUkjLmw2tAfIaE1itY6ppSqw2qZ\nAnAssXWJvX05UIp1UdqPUjxvfcLXQSBw3kcghMNIERNi6BzDunL75P532O+JVSTcNoFxWB2YASqU\nUmZCIRuPdUX4RiAEVGOdphRCJJAiJsTQ2Qe0K6V+AKwHwsA0wG/fP0MptRh4EVgOdAN7sTr1BoEV\nSql1WA0PbwA+Z8/Yfgk8qZRaCpwArgb6t64XYliShR1CDBGtdRRYhLXC8BDWLKoWKLQ3+S9gCVa/\nr6XAYq11j9Y6jFW0Ftj7bMTqB/ZXe79/wVqRuB9oAh5HfnaFAGRhhxBpYZ9OnKS1vi3TsQhxKZG/\n5oQQQjiWFDEhhBCOJacThRBCOJbMxIQQQjiWFDEhhBCOJUVMCCGEY0kRE0II4VhSxIQQQjjW/wPO\n0ZL01NikCwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "기존nn모델의 train loss를 출력합니다\n",
            "train, loss and metric: [0.3128682939152156, 0.3128682939152156]\n",
            "기존nn모델의 valid loss를 출력합니다\n",
            "valid, loss and metric: [0.3644961395712228, 0.3644961395712228]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ea239f45-016b-4197-e221-f0f11cdacf39",
        "id": "BphPz2vS0qpS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "savingpath_csv = 'drive/My Drive/데이콘_천체유형/파일제출/' + csv_folder + '/이상치_제거_initial_rate_%s.csv' % initial_rate\n",
        "\n",
        "print('final rate는 다음과 같습니다')\n",
        "print(hist.history['lr'][np.argmin(hist.history['val_loss'])])\n",
        "print('='*25)\n",
        "y_pred = nn_model.predict(test_x)\n",
        "submission = pd.DataFrame(data=y_pred, columns=sample_submission.columns, index=sample_submission.index)\n",
        "submission.to_csv(savingpath_csv, index=True)\n",
        "print('csv 저장완료')\n",
        "\n",
        "print('='*50)\n",
        "best_val_loss = np.min(hist.history['val_loss'])\n",
        "print(\"best_valid_loss: {}\".format(best_val_loss))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final rate는 다음과 같습니다\n",
            "0.0003162278\n",
            "=========================\n",
            "csv 저장완료\n",
            "==================================================\n",
            "best_valid_loss: 0.3644961395712228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7UR0QNdWEF9",
        "colab_type": "text"
      },
      "source": [
        "# 저장된 모델 로드해오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1W33dlgNXSTo",
        "outputId": "9084d431-0d8c-4358-bfdd-77be9e884eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "initial_rate = 1e-2\n",
        "SAVEMODEL_NEWFOLDER1 = 'drive/My Drive/데이콘_천체유형/기존모델저장/' + csv_folder\n",
        "MODEL_SAVE_FOLDER_PATH1 = SAVEMODEL_NEWFOLDER1 +  '/initial_rate=%s/' % initial_rate ## 기존모델저장\n",
        "\n",
        "## model load\n",
        "json_path = MODEL_SAVE_FOLDER_PATH1 + \"model1.json\"\n",
        "weight_path = MODEL_SAVE_FOLDER_PATH1 +\"model1.h5\"\n",
        "\n",
        "from keras.models import model_from_json\n",
        "json_file = open(json_path, \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "## model weight load\n",
        "loaded_model.load_weights(weight_path)\n",
        "\n",
        "## model load and evaluation\n",
        "from keras import optimizers\n",
        "final_rate = hist.history['lr'][np.argmin(hist.history['val_loss'])]\n",
        "print(\"Loaded model from disk\")\n",
        "load_optimizer = optimizers.Adam(\n",
        "    lr=final_rate,\n",
        ")\n",
        "CCE = metrics.sparse_categorical_crossentropy\n",
        "loaded_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=load_optimizer\n",
        "                     ,metrics=[CCE]\n",
        "                     )\n",
        "batchsize=1024\n",
        "train_score = loaded_model.evaluate(train_input,train_target,verbose=0, batch_size=batchsize)\n",
        "print('Load한 Check모델의 train loss를 출력합니다')\n",
        "print(\"train, loss and metric: {}\".format(train_score))\n",
        "cv_score = loaded_model.evaluate(cv_input,cv_target,verbose=0, batch_size=batchsize)\n",
        "print('Load한 Check모델의 val loss를 출력합니다')\n",
        "print(\"valid, loss and metric: {}\".format(cv_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n",
            "Load한 Check모델의 train loss를 출력합니다\n",
            "train, loss and metric: [0.3077159990481829, 0.3077159990481829]\n",
            "Load한 Check모델의 val loss를 출력합니다\n",
            "valid, loss and metric: [0.3495782041837298, 0.3495782041837298]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}